{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711af527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import joblib\n",
    "import numpy as np\n",
    "from dtaidistance import dtw\n",
    "\n",
    "# Import the Douglas-Peucker function from the simplification library\n",
    "from simplification.cutil import simplify_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the data for one type of trip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/fix_noise.parquet\")\n",
    "df[\"time_stamp\"] = pd.to_datetime(df[\"time_stamp\"])\n",
    "df = df.sort_values(by=[\"time_stamp\", \"trip_id\"])\n",
    "\n",
    "df = df[df[\"start_port\"] == \"BREMERHAVEN\"]\n",
    "df.value_counts(\"trip_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering with DBSCAN and DTW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We comperess the trajectories to reduce the number of points per trajectory.\n",
    "As later we will use DTW to calculate the distance between trajectories, this will speed up the process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Data into Trajectories\n",
    "trajectories = {trip_id: group for trip_id, group in df.groupby('trip_id')}\n",
    "print(f\"Found {len(trajectories)} unique trajectories.\")\n",
    "\n",
    "# Compress Trajectories using the 'simplification' library\n",
    "print(\"\\nCompressing trajectories...\")\n",
    "compressed_trajectories = {}\n",
    "for trip_id, traj_df in trajectories.items():\n",
    "    points = traj_df[['latitude', 'longitude']].values\n",
    "    compressed_points = simplify_coords(points, epsilon=0.000001) # sensitivity parameter\n",
    "    compressed_trajectories[trip_id] = compressed_points\n",
    "print(\"Trajectory compression complete.\")\n",
    "joblib.dump(compressed_trajectories, \"data/compressed_trajectories.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compressed_trajectories = joblib.load(\"data/compressed_trajectories.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5865e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Clustering with Standard DTW ---\n",
    "print(\"\\n--- Part 2: Clustering with Standard DTW ---\")\n",
    "trajectories_list = list(compressed_trajectories.values()) # Convert the dictionary of compressed trajectories to a list\n",
    "\n",
    "# Use the dtaidistance library to compute the distance matrix using standard DTW\n",
    "print(\"Calculating pairwise distances using standard DTW...\")\n",
    "distance_matrix = dtw.distance_matrix(trajectories_list, use_c=True, show_progress=True)\n",
    "print(\"Distance matrix calculation complete.\")\n",
    "joblib.dump(distance_matrix, \"data/distance_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da6c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = joblib.load(\"data/distance_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DBSCAN Clustering\n",
    "# NOTE: The 'eps' parameter is highly sensitive. You will likely need to tune this value.\n",
    "\n",
    "print(\"Running DBSCAN clustering...\")\n",
    "dbscan = DBSCAN(eps=5.0, min_samples=5, metric='precomputed')\n",
    "cluster_labels = dbscan.fit_predict(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Identify the \"normal\" cluster (the largest one)\n",
    "trip_ids = list(compressed_trajectories.keys())\n",
    "normal_cluster_id_series = pd.Series(cluster_labels[cluster_labels!=-1]).mode()\n",
    "if not normal_cluster_id_series.empty:\n",
    "    normal_cluster_id = normal_cluster_id_series[0]\n",
    "    normal_trip_ids = [trip_id for trip_id, label in zip(trip_ids, cluster_labels) if label == normal_cluster_id]\n",
    "    print(f\"\\nIdentified normal traffic pattern as Cluster ID: {normal_cluster_id}\")\n",
    "else:\n",
    "    normal_trip_ids = []\n",
    "    print(\"\\nNo significant clusters found. Cannot proceed.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting the clusters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import visualize_trajectory_clusters_interactive_fancy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = visualize_trajectory_clusters_interactive_fancy(compressed_trajectories, trip_ids, cluster_labels)\n",
    "fig.write_html(\"data/trajectories_clusters.html\", include_plotlyjs='cdn')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Anomaly Detection with PyTorch BiGRU\n",
    "\n",
    "Here we are going to use a BiGRU model to detect anomalies in the trajectories.\n",
    "Is will detect exact measurement that is not fitting the normal pattern."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Creates sequences from time-series data for training recurrent neural networks.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): The input time-series data.\n",
    "        seq_length (int): The length of each input sequence.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two numpy arrays:\n",
    "               - xs (np.array): Input sequences.\n",
    "               - ys (np.array): Target values (the next step after the sequence).\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i : i + seq_length])\n",
    "        ys.append(data[i + seq_length])\n",
    "    return np.array(xs), np.array(ys)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BiGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiGRUModel, self).__init__()\n",
    "        # Bidirectional GRU layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        # Fully connected layer to map GRU output to desired output size\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size) # hidden_size * 2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through GRU layer\n",
    "        out, _ = self.gru(x)\n",
    "        # Get the output from the last time step and pass through the fully connected layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def load_model(self, path):\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(path))\n",
    "            self.eval()  # Set the model to evaluation mode after loading\n",
    "            print(f\"Model loaded successfully from {path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model file not found at {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading the model: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10 # Using 10 epochs for demonstration\n",
    "features = ['latitude', 'longitude', 'speed_over_ground', 'course_over_ground']\n",
    "SEQ_LENGTH = 20 # Define sequence length and create sequences for training\n",
    "BATCH_SIZE = 32 # Batch size for DataLoader\n",
    "model = BiGRUModel(input_size=len(features), hidden_size=128, num_layers=1, output_size=len(features))\n",
    "scaler = MinMaxScaler()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if normal_trip_ids:\n",
    "    print(\"Preparing training data from normal cluster...\")\n",
    "    # Filter for normal trips and sort by time for sequence creation\n",
    "    normal_df = df[df['trip_id'].isin(normal_trip_ids)].sort_values(by='time_stamp')\n",
    "    training_data = normal_df[features].values\n",
    "\n",
    "    # Scale features to a range between 0 and 1\n",
    "    training_data_scaled = scaler.fit_transform(training_data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if normal_trip_ids:\n",
    "    X_train, y_train = create_sequences(training_data_scaled, SEQ_LENGTH)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "    # Create DataLoader for batching and shuffling\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(\"Building and training the BiGRU model...\")\n",
    "    # Instantiate the model, define loss function and optimizer\n",
    "    loss_function = nn.MSELoss()  # Mean Squared Error Loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Adam optimizer\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for sequences, labels in train_loader:\n",
    "            outputs = model(sequences)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad() # Clear gradients\n",
    "            loss.backward()       # Backpropagation\n",
    "            optimizer.step()      # Update weights\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss.item():.4f}\")\n",
    "    print(\"Model training complete.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\nDetecting anomalies in a test trajectory...\")\n",
    "# Select a test trip ID, prioritizing a non-normal one if available\n",
    "test_trip_id = next((tid for tid, label in zip(trip_ids, cluster_labels) if label != normal_cluster_id), trip_ids[-1])\n",
    "test_df = trajectories[test_trip_id]\n",
    "test_data = test_df[features].values\n",
    "\n",
    "# Create a fake anomaly for demonstration purposes: sudden increase in Speed Over Ground (SOG)\n",
    "test_data[20:25, 2] = 50\n",
    "\n",
    "# Scale the test data using the same scaler fitted on training data\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "X_test, y_test_actual = create_sequences(test_data_scaled, SEQ_LENGTH)\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad(): # Disable gradient calculation during inference\n",
    "    y_test_pred_scaled_tensor = model(X_test_tensor)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual values to their original scale\n",
    "y_test_pred_real = scaler.inverse_transform(y_test_pred_scaled_tensor.numpy())\n",
    "y_test_actual_real = scaler.inverse_transform(y_test_actual)\n",
    "\n",
    "# Calculate deviations\n",
    "# Position deviation in meters (approx. 1 degree latitude/longitude ~ 111 km)\n",
    "position_deviation = np.linalg.norm(y_test_actual_real[:, :2] - y_test_pred_real[:, :2], axis=1) * 111000\n",
    "# Speed Over Ground (SOG) deviation in knots\n",
    "sog_deviation = np.abs(y_test_actual_real[:, 2] - y_test_pred_real[:, 2])\n",
    "\n",
    "# Define anomaly thresholds\n",
    "POS_THRESHOLD_M = 5000 # 5000 meters\n",
    "SOG_THRESHOLD_KN = 5   # 5 knots\n",
    "\n",
    "print(f\"\\n--- Anomaly Report for TripID {test_trip_id} ---\")\n",
    "# Report anomalies based on thresholds\n",
    "for i in range(len(position_deviation)):\n",
    "    if position_deviation[i] > POS_THRESHOLD_M or sog_deviation[i] > SOG_THRESHOLD_KN:\n",
    "        print(f\"Time Step {i+SEQ_LENGTH}: **ANOMALY DETECTED** \"\n",
    "              f\"(Pos Dev: {position_deviation[i]:.0f}m, SOG Dev: {sog_deviation[i]:.1f}kn)\")\n",
    "    else:\n",
    "        print(f\"Time Step {i+SEQ_LENGTH}: Normal\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"data/bigru_model.pth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = BiGRUModel(input_size=len(features), hidden_size=128, num_layers=1, output_size=len(features))\n",
    "loaded_model.load_model(\"data/bigru_model.pth\")\n",
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## For testing if model works correctly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import visualize_trip_interactive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected Trip ID 1130240 for testing.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "test_trip_pool = [tid for tid, label in zip(trip_ids, cluster_labels) if label != normal_cluster_id]\n",
    "if not test_trip_pool:\n",
    "    print(\"Warning: No trips outside the normal cluster were found. Testing on a 'normal' trip for demonstration.\")\n",
    "    test_trip_pool = normal_trip_ids\n",
    "\n",
    "test_trip_id = random.choice(test_trip_pool)\n",
    "print(f\"Randomly selected Trip ID {test_trip_id} for testing.\")\n",
    "test_df = df[df['trip_id'] == test_trip_id].sort_values(by='time_stamp')\n",
    "\n",
    "# Step 4\n",
    "test_data = test_df[features].values\n",
    "test_data_scaled = scaler.transform(test_data) # Use the SAME scaler from training\n",
    "X_test, y_test_actual = create_sequences(test_data_scaled, SEQ_LENGTH)\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred_scaled_tensor = model(X_test_tensor)\n",
    "\n",
    "y_test_pred_real = scaler.inverse_transform(y_test_pred_scaled_tensor.numpy())\n",
    "y_test_actual_real = scaler.inverse_transform(y_test_actual)\n",
    "\n",
    "\n",
    "# Step 4d: Detect Anomalies based on deviation\n",
    "position_deviation = np.linalg.norm(y_test_actual_real[:, :2] - y_test_pred_real[:, :2], axis=1) * 111000\n",
    "sog_deviation = np.abs(y_test_actual_real[:, 2] - y_test_pred_real[:, 2])\n",
    "anomaly_indices = np.where((position_deviation > 3000) | (sog_deviation > 1))[0] + SEQ_LENGTH # Threshold of 1km"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T18:42:54.786425940Z",
     "start_time": "2025-06-10T18:42:54.720421730Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Visualizing Trip ID: 1130240 (Interactive Map) ---\n",
      "Return interactive map for Trip ID 1130240.\n"
     ]
    }
   ],
   "source": [
    "fig = visualize_trip_interactive(test_df, test_trip_id)\n",
    "fig.write_html(f\"data/test_trip{test_trip_id}.html\", include_plotlyjs='cdn')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T18:42:55.743857288Z",
     "start_time": "2025-06-10T18:42:55.715601219Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
