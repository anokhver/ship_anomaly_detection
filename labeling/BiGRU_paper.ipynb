{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:46:56.540411185Z",
     "start_time": "2025-06-15T18:46:56.499111153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell: Imports and Setup for BiGRU\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm # For progress bars in notebook\n",
    "\n",
    "# Assuming 'utils.py' contains visualize_trip_interactive for plotting\n",
    "# If not available, you would need to implement or remove the visualization calls.\n",
    "from utils import visualize_trip_interactive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/cleaned/bremerhaven_anomalies_partlabeled.parquet')\n",
    "df[\"time_stamp\"] = pd.to_datetime(df[\"time_stamp\"])\n",
    "df = df.sort_values(by=[\"time_stamp\", \"trip_id\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:46:58.341386622Z",
     "start_time": "2025-06-15T18:46:57.638994308Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Get trips that have any anomalies\n",
    "df = df[df['is_anomaly'].notna()]\n",
    "trips_with_anomalies = df.groupby('trip_id')['is_anomaly'].any()\n",
    "anomaly_trip_ids = trips_with_anomalies[trips_with_anomalies].index\n",
    "normal_trip_ids = trips_with_anomalies[~trips_with_anomalies].index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:46:58.680602246Z",
     "start_time": "2025-06-15T18:46:58.637073740Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "1222     False\n1320     False\n1321     False\n1322     False\n1324     False\n         ...  \n25452    False\n25451    False\n25454    False\n25455    False\n25456    False\nName: is_anomaly, Length: 26368, dtype: object"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_anomaly']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:48:09.478328472Z",
     "start_time": "2025-06-15T18:48:09.455760952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trips with anomalies: 54\n",
      "Trips without anomalies: 0\n",
      "Total points in anomaly trips: 26368\n",
      "Total points in normal trips: 0\n"
     ]
    }
   ],
   "source": [
    "    # Split dataframes\n",
    "df_anomaly_trips = df[df['trip_id'].isin(anomaly_trip_ids)]\n",
    "df_normal_trips = df[df['trip_id'].isin(normal_trip_ids)]\n",
    "\n",
    "# Print results\n",
    "print(f\"Trips with anomalies: {len(anomaly_trip_ids)}\")\n",
    "print(f\"Trips without anomalies: {len(normal_trip_ids)}\")\n",
    "print(f\"Total points in anomaly trips: {len(df_anomaly_trips)}\")\n",
    "print(f\"Total points in normal trips: {len(df_normal_trips)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:13:02.068405998Z",
     "start_time": "2025-06-15T18:13:02.053570050Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Cell: Helper function to create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Creates sequences from time-series data for training recurrent neural networks.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): The input time-series data.\n",
    "        seq_length (int): The length of each input sequence.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two numpy arrays:\n",
    "               - xs (np.array): Input sequences.\n",
    "               - ys (np.array): Target values (the next step after the sequence).\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i : i + seq_length])\n",
    "        ys.append(data[i + seq_length])\n",
    "    return np.array(xs), np.array(ys)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:13:02.833651715Z",
     "start_time": "2025-06-15T18:13:02.819808870Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n",
    "# Cell: BiGRU Model Definition\n",
    "class BiGRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional GRU (BiGRU) model for time series prediction.\n",
    "    Predicts the next step in a sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiGRUModel, self).__init__()\n",
    "        # Bidirectional GRU layer: processes sequence in both forward and backward directions\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        # Fully connected layer to map GRU output to desired output size\n",
    "        # hidden_size * 2 because of bidirectional output (concatenates forward and backward)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through GRU layer\n",
    "        # out: output features from the last layer of the GRU for each timestep\n",
    "        # _: hidden state for each element in the batch (not directly used here for prediction)\n",
    "        out, _ = self.gru(x)\n",
    "        # Get the output from the last time step of the sequence and pass through the fully connected layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Loads the saved state dictionary into the model.\n",
    "\n",
    "        Args:\n",
    "            path (str): The file path to the saved model state dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(path))\n",
    "            self.eval()  # Set the model to evaluation mode after loading\n",
    "            print(f\"Model loaded successfully from {path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model file not found at {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading the model: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:09:14.133223783Z",
     "start_time": "2025-06-15T18:09:14.123344958Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell: Model Initialization and Hyperparameters\n",
    "NUM_EPOCHS = 10 # Number of training epochs\n",
    "features = ['latitude', 'longitude', 'speed_over_ground', 'course_over_ground'] # Features used for training\n",
    "SEQ_LENGTH = 20 # Length of each input sequence for the BiGRU model\n",
    "BATCH_SIZE = 32 # Batch size for DataLoader during training\n",
    "\n",
    "# Initialize the BiGRU model\n",
    "# input_size: number of features for each time step\n",
    "# hidden_size: number of features in the hidden state\n",
    "# num_layers: number of recurrent layers\n",
    "# output_size: number of features to predict (same as input_size for next-step prediction)\n",
    "model = BiGRUModel(input_size=len(features), hidden_size=128, num_layers=1, output_size=len(features))\n",
    "\n",
    "# Initialize the MinMaxScaler for feature scaling\n",
    "scaler = MinMaxScaler()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:09:19.484949152Z",
     "start_time": "2025-06-15T18:09:19.470055109Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Cell: Prepare Training Data (Assuming 'normal_df' is your pre-loaded dataset of normal trajectories)\n",
    "# Example: If your normal data is in a DataFrame named `normal_trajectory_data_df`\n",
    "# replace `normal_df` with your actual DataFrame name.\n",
    "# It should contain 'time_stamp' and the 'features' columns.\n",
    "# For demonstration, let's create a placeholder `normal_df` if it doesn't exist.\n",
    "try:\n",
    "    # This assumes 'df' is loaded from your previous steps and\n",
    "    # 'normal_trip_ids' is defined from a prior clustering or filtering process.\n",
    "    # If you are loading a dataset that *only* contains normal trajectories,\n",
    "    # you would replace this with your direct dataset loading.\n",
    "    if 'df' in locals() and 'normal_trip_ids' in locals() and normal_trip_ids:\n",
    "        print(\"Using normal trips identified from previous clustering for training.\")\n",
    "        normal_df = df[df['trip_id'].isin(normal_trip_ids)].sort_values(by='time_stamp')\n",
    "    else:\n",
    "        print(\"Assuming 'normal_df' needs to be loaded or defined from a dedicated normal dataset.\")\n",
    "        # Placeholder for loading your pre-filtered normal trajectory data\n",
    "        # IMPORTANT: Replace this with your actual data loading for normal trajectories\n",
    "        # Example: normal_df = pd.read_parquet(\"path/to/your/normal_trajectories.parquet\")\n",
    "        # Ensure it has 'latitude', 'longitude', 'speed_over_ground', 'course_over_ground'\n",
    "        # and 'time_stamp' columns.\n",
    "\n",
    "        # Creating a dummy normal_df for demonstration if not already present\n",
    "        # In a real scenario, this would be your actual normal data.\n",
    "        if 'df' in locals() and not df.empty:\n",
    "            print(\"Using a subset of the main DataFrame as 'normal_df' for demonstration.\")\n",
    "            normal_df = df[df['trip_id'] == df['trip_id'].iloc[0]].sort_values(by='time_stamp').copy()\n",
    "        else:\n",
    "            print(\"Cannot create dummy normal_df. Please load your normal trajectory dataset.\")\n",
    "            normal_df = pd.DataFrame() # Empty DataFrame to prevent errors\n",
    "\n",
    "\n",
    "    if not normal_df.empty:\n",
    "        # Extract features and scale them\n",
    "        training_data = normal_df[features].values\n",
    "        print(f\"Original training data shape: {training_data.shape}\")\n",
    "        training_data_scaled = scaler.fit_transform(training_data)\n",
    "        print(f\"Scaled training data shape: {training_data_scaled.shape}\")\n",
    "    else:\n",
    "        print(\"Normal trajectory data is empty. Cannot prepare training data.\")\n",
    "        training_data_scaled = None # Set to None to indicate no data\n",
    "except NameError:\n",
    "    print(\"Dependencies (e.g., 'df' or 'normal_trip_ids') not found. Please ensure your normal data is loaded into 'normal_df'.\")\n",
    "    training_data_scaled = None\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Cell: Train the BiGRU Model\n",
    "if training_data_scaled is not None and len(training_data_scaled) > SEQ_LENGTH:\n",
    "    X_train, y_train = create_sequences(training_data_scaled, SEQ_LENGTH)\n",
    "    print(f\"Created {len(X_train)} sequences for training.\")\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "    # Create DataLoader for batching and shuffling\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(\"Building and training the BiGRU model...\")\n",
    "    # Define loss function (Mean Squared Error for regression) and optimizer (Adam)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in tqdm(range(NUM_EPOCHS), desc=\"Training BiGRU\"):\n",
    "        for sequences, labels in train_loader:\n",
    "            outputs = model(sequences)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad() # Clear gradients before backward pass\n",
    "            loss.backward()       # Perform backpropagation\n",
    "            optimizer.step()      # Update model weights\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss.item():.4f}\")\n",
    "    print(\"Model training complete.\")\n",
    "else:\n",
    "    print(\"Model training skipped: Not enough normal data or data not prepared correctly.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Cell: Save the trained model\n",
    "if 'model' in locals() and model is not None:\n",
    "    torch.save(model.state_dict(), \"data/bigru_model.pth\")\n",
    "    print(\"Model saved to data/bigru_model.pth\")\n",
    "else:\n",
    "    print(\"Model not trained or not found. Skipping save.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell: Load the trained model\n",
    "# This cell can be run independently to load a previously saved model\n",
    "loaded_model = BiGRUModel(input_size=len(features), hidden_size=128, num_layers=1, output_size=len(features))\n",
    "loaded_model.load_model(\"data/bigru_model.pth\")\n",
    "# Assign the loaded model back to 'model' variable if you want to continue using it\n",
    "model = loaded_model\n",
    "\n",
    "\n",
    "# Cell: Anomaly Detection for a Test Trajectory\n",
    "print(\"\\n--- Performing Anomaly Detection on a Test Trajectory ---\")\n",
    "\n",
    "# For this section, you would typically load a *new* test trajectory\n",
    "# which might or might not contain anomalies.\n",
    "# For demonstration, we'll use a portion of the 'normal_df' or a dummy test trip.\n",
    "# IMPORTANT: Replace this with loading your actual test trajectory data.\n",
    "# Example: test_df = pd.read_parquet(\"path/to/your/test_trajectory_with_anomalies.parquet\")\n",
    "# Ensure it has 'time_stamp' and the 'features' columns.\n",
    "\n",
    "if 'normal_df' in locals() and not normal_df.empty:\n",
    "    # Using a portion of the normal data as a 'test trip' for demonstration\n",
    "    # You would replace this with loading your actual test data.\n",
    "    test_df = normal_df.head(100).copy() # Take first 100 points as a test trip\n",
    "    test_trip_id = \"demonstration_normal_trip\" # Assign a dummy ID\n",
    "\n",
    "    # Optional: Introduce a *fake* anomaly for demonstration purposes\n",
    "    # This simulates a sudden change in Speed Over Ground (SOG)\n",
    "    # Ensure indices are within bounds of test_data\n",
    "    test_data = test_df[features].values\n",
    "    if len(test_data) > 25:\n",
    "        print(\"Injecting a fake anomaly (sudden SOG increase) for demonstration.\")\n",
    "        test_data[20:25, 2] = 50 # SOG value (knots)\n",
    "    else:\n",
    "        print(\"Test data too short to inject fake anomaly.\")\n",
    "\n",
    "    # Scale the test data using the *same scaler* fitted on training data\n",
    "    test_data_scaled = scaler.transform(test_data)\n",
    "    X_test, y_test_actual = create_sequences(test_data_scaled, SEQ_LENGTH)\n",
    "\n",
    "    if len(X_test) > 0:\n",
    "        X_test_tensor = torch.from_numpy(X_test).float()\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        with torch.no_grad(): # Disable gradient calculation during inference\n",
    "            y_test_pred_scaled_tensor = model(X_test_tensor)\n",
    "\n",
    "        # Inverse transform the scaled predictions and actual values to their original scale\n",
    "        y_test_pred_real = scaler.inverse_transform(y_test_pred_scaled_tensor.numpy())\n",
    "        y_test_actual_real = scaler.inverse_transform(y_test_actual)\n",
    "\n",
    "        # Calculate deviations between actual and predicted values\n",
    "        # Position deviation in meters (approx. 1 degree latitude/longitude ~ 111 km)\n",
    "        position_deviation = np.linalg.norm(y_test_actual_real[:, :2] - y_test_pred_real[:, :2], axis=1) * 111000\n",
    "        # Speed Over Ground (SOG) deviation in knots\n",
    "        sog_deviation = np.abs(y_test_actual_real[:, 2] - y_test_pred_real[:, 2])\n",
    "\n",
    "        # Define anomaly thresholds\n",
    "        POS_THRESHOLD_M = 5000 # 5000 meters deviation\n",
    "        SOG_THRESHOLD_KN = 5   # 5 knots deviation\n",
    "\n",
    "        print(f\"\\n--- Anomaly Report for TripID {test_trip_id} ---\")\n",
    "        # Report anomalies based on defined thresholds\n",
    "        anomaly_indices = []\n",
    "        for i in range(len(position_deviation)):\n",
    "            if position_deviation[i] > POS_THRESHOLD_M or sog_deviation[i] > SOG_THRESHOLD_KN:\n",
    "                anomaly_indices.append(i + SEQ_LENGTH) # Add original index offset by SEQ_LENGTH\n",
    "                print(f\"Time Step {i+SEQ_LENGTH}: **ANOMALY DETECTED** \"\n",
    "                      f\"(Pos Dev: {position_deviation[i]:.0f}m, SOG Dev: {sog_deviation[i]:.1f}kn)\")\n",
    "            else:\n",
    "                print(f\"Time Step {i+SEQ_LENGTH}: Normal\")\n",
    "        print(f\"Total anomalies detected: {len(anomaly_indices)}\")\n",
    "\n",
    "        # Visualize the test trip with detected anomalies\n",
    "        fig = visualize_trip_interactive(test_df, test_trip_id, anomaly_indices=anomaly_indices)\n",
    "        fig.write_html(f\"data/test_trip_{test_trip_id}_bigru_anomalies.html\", include_plotlyjs='cdn')\n",
    "        print(f\"Interactive trip visualization with anomalies saved to data/test_trip_{test_trip_id}_bigru_anomalies.html\")\n",
    "    else:\n",
    "        print(\"Not enough test data to create sequences for anomaly detection.\")\n",
    "\n",
    "else:\n",
    "    print(\"Test trajectory data (or normal_df) is empty. Skipping anomaly detection demonstration.\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
