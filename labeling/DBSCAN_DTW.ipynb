{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "711af527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T17:52:29.123264078Z",
     "start_time": "2025-06-15T17:52:27.752409453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup (Ensure you have dtaidistance, simplification, scikit-learn, and torch installed)\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import joblib\n",
    "import numpy as np\n",
    "# Import dtw.fast for optimized DTW calculation\n",
    "from dtaidistance import dtw\n",
    "from simplification.cutil import simplify_coords\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm.notebook import tqdm # For progress bars in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the data for one type of trip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# --- Configuration for Anomaly Detection (from original notebook) ---\n",
    "NUM_EPOCHS = 10\n",
    "features = ['latitude', 'longitude']\n",
    "SEQ_LENGTH = 20\n",
    "BATCH_SIZE = 32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T17:52:30.451829077Z",
     "start_time": "2025-06-15T17:52:30.442262964Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3095a4b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T17:52:33.062422572Z",
     "start_time": "2025-06-15T17:52:32.399020524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_id\n",
      "2035389    700\n",
      "2035337    698\n",
      "365365     698\n",
      "1288934    695\n",
      "1288936    695\n",
      "          ... \n",
      "119446     170\n",
      "688639     167\n",
      "1173817    156\n",
      "37617      146\n",
      "119576     137\n",
      "Name: count, Length: 703, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../data/cleaned/bremerhaven_anomalies_partlabeled.parquet\")\n",
    "df[\"time_stamp\"] = pd.to_datetime(df[\"time_stamp\"])\n",
    "df = df.sort_values(by=[\"time_stamp\", \"trip_id\"])\n",
    "\n",
    "df = df[df[\"start_port\"] == \"BREMERHAVEN\"]\n",
    "print(df.value_counts(\"trip_id\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering with DBSCAN and DTW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We comperess the trajectories to reduce the number of points per trajectory.\n",
    "As later we will use DTW to calculate the distance between trajectories, this will speed up the process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ed94aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T17:52:51.288840717Z",
     "start_time": "2025-06-15T17:52:50.983382619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 703 unique trajectories.\n",
      "\n",
      "Compressing trajectories...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Compressing trajectories:   0%|          | 0/703 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0353d21a8cb7418dbd5e643a4a251957"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory compression complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": "['data/compressed_trajectories.pkl']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clustering with DBSCAN and DTW - Trajectory Compression\n",
    "# We compress the trajectories to reduce the number of points per trajectory.\n",
    "# As later we will use DTW to calculate the distance between trajectories, this will speed up the process.\n",
    "# Note: A more advanced Adaptive Douglas-Peucker (AF-DP) algorithm as described\n",
    "# in the paper would involve dynamic epsilon selection and feature point retention\n",
    "# based on movement characteristics (e.g., changes in speed/course).\n",
    "# For this update, we keep `simplify_coords` but acknowledge the adaptive potential.\n",
    "\n",
    "# Group Data into Trajectories\n",
    "trajectories = {trip_id: group for trip_id, group in df.groupby('trip_id')}\n",
    "print(f\"Found {len(trajectories)} unique trajectories.\")\n",
    "\n",
    "# Compress Trajectories using the 'simplification' library\n",
    "print(\"\\nCompressing trajectories...\")\n",
    "compressed_trajectories = {}\n",
    "# Using a general epsilon. An AF-DP implementation would dynamically set this\n",
    "# based on trajectory characteristics for better feature preservation.\n",
    "DEFAULT_DP_EPSILON = 0.0000001\n",
    "for trip_id, traj_df in tqdm(trajectories.items(), desc=\"Compressing trajectories\"):\n",
    "    points = traj_df[['latitude', 'longitude']].values\n",
    "    # Here, you would integrate the adaptive threshold logic from the paper\n",
    "    # to dynamically determine 'epsilon' for each trajectory, and also\n",
    "    # identify and retain \"feature points\" (e.g., sharp turns).\n",
    "    # For simplicity, we use a fixed epsilon for now.\n",
    "    compressed_points = simplify_coords(points, epsilon=DEFAULT_DP_EPSILON)\n",
    "    compressed_trajectories[trip_id] = compressed_points\n",
    "\n",
    "print(\"Trajectory compression complete.\")\n",
    "joblib.dump(compressed_trajectories, \"data/compressed_trajectories.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "compressed_trajectories = joblib.load(\"data/compressed_trajectories.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T17:52:54.074934914Z",
     "start_time": "2025-06-15T17:52:54.032922004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5865e75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T17:53:24.163200589Z",
     "start_time": "2025-06-15T17:53:24.120727686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Pairwise Distances using DTW ---\n",
      "Calculating pairwise distances using DTW...\n",
      "DTW distance matrix calculation complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": "['data/dtw_distance_matrix.pkl']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Cell 5: Calculate Distance Matrix with Fast-DTW\n",
    "# This section replaces the standard DTW distance calculation with Fast-DTW.\n",
    "# Fast-DTW offers significant speed improvements while maintaining accuracy.\n",
    "print(\"\\n--- Calculating Pairwise Distances using DTW ---\")\n",
    "trajectories_list = list(compressed_trajectories.values())\n",
    "\n",
    "# Use dtw.distance_matrix for the optimized DTW algorithm (Corrected call)\n",
    "# The 'compact' parameter returns a condensed distance matrix, saving memory\n",
    "print(\"Calculating pairwise distances using DTW...\")\n",
    "distance_matrix = dtw.distance_matrix(trajectories_list, use_c=True, show_progress=True) # Corrected function call\n",
    "print(\"DTW distance matrix calculation complete.\")\n",
    "joblib.dump(distance_matrix, \"data/dtw_distance_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DTW distance matrix.\n"
     ]
    }
   ],
   "source": [
    "distance_matrix = joblib.load(\"data/dtw_distance_matrix.pkl\")\n",
    "print(\"Loaded DTW distance matrix.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T17:55:22.215394427Z",
     "start_time": "2025-06-15T17:55:22.173437028Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Adaptive DBSCAN Parameter Selection ---\n",
      "Trying 9 epsilon candidates and 11 min_samples candidates.\n",
      "Epsilon candidates: [45.04 45.32 63.64 63.82 64.02 78.09 78.27 90.17 90.37]\n",
      "Min_samples candidates: [3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 7: Adaptive DBSCAN Parameter Selection and Clustering\n",
    "# This section implements the adaptive parameter selection using Silhouette Score\n",
    "# as described in Section 2.5.2 of the paper.\n",
    "\n",
    "print(\"\\n--- Performing Adaptive DBSCAN Parameter Selection ---\")\n",
    "\n",
    "# Step 1: Generate candidate eps values\n",
    "# The paper mentions using KANN and similarity distribution.\n",
    "# A practical approach is to explore a range of percentiles of the distances.\n",
    "# We'll use a range of percentiles from the distance matrix for epsilon candidates.\n",
    "# Exclude diagonal (distance to self is 0) and flatten the upper triangle.\n",
    "flat_distances = distance_matrix[np.triu_indices(distance_matrix.shape[0], k=1)].flatten()\n",
    "\n",
    "# Ensure there are enough unique distances to compute percentiles\n",
    "if len(np.unique(flat_distances)) > 10:\n",
    "    eps_candidates = np.percentile(flat_distances, np.arange(5, 50, 5)) # From 5th to 45th percentile\n",
    "    eps_candidates = np.unique(eps_candidates) # Ensure unique values\n",
    "else:\n",
    "    print(\"Warning: Not enough unique distances for percentile-based eps candidates. Using a default range.\")\n",
    "    eps_candidates = np.linspace(flat_distances.min() + 1e-6, flat_distances.max() / 2, 10) # Fallback range\n",
    "    eps_candidates = eps_candidates[eps_candidates > 0] # Ensure eps > 0\n",
    "\n",
    "# Step 2: Generate MinPts (min_samples) candidates\n",
    "# The paper suggests \"5 to eps list length\". Let's use a reasonable range.\n",
    "min_samples_candidates = list(range(3, 25, 2)) # e.g., 3, 5, 7, ..., 23\n",
    "\n",
    "best_score = -1\n",
    "best_params = {'eps': None, 'min_samples': None}\n",
    "best_cluster_labels = None\n",
    "results = [] # To store all tried combinations and their scores\n",
    "\n",
    "print(f\"Trying {len(eps_candidates)} epsilon candidates and {len(min_samples_candidates)} min_samples candidates.\")\n",
    "print(f\"Epsilon candidates: {np.round(eps_candidates, 2)}\")\n",
    "print(f\"Min_samples candidates: {min_samples_candidates}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T17:55:25.004451973Z",
     "start_time": "2025-06-15T17:55:24.963025301Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Searching eps:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fc8a00eb88a440c8c949f85102f46c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Perform grid search\n",
    "for eps in tqdm(eps_candidates, desc=\"Searching eps\"):\n",
    "    for min_samples in min_samples_candidates:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "        cluster_labels = dbscan.fit_predict(distance_matrix)\n",
    "\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "        # Only calculate silhouette score if there are more than 1 cluster (excluding noise)\n",
    "        # and not all points are noise.\n",
    "        if 1 < n_clusters <= len(np.unique(cluster_labels[cluster_labels != -1])):\n",
    "            # Filter out noise points for silhouette score calculation\n",
    "            core_samples_mask = np.zeros_like(cluster_labels, dtype=bool)\n",
    "            core_samples_mask[dbscan.core_sample_indices_] = True\n",
    "\n",
    "            # Get indices of points that are part of a cluster (not noise)\n",
    "            clustered_points_indices = np.where(cluster_labels != -1)[0]\n",
    "            if len(clustered_points_indices) > 1:\n",
    "                # Need distance matrix subset for silhouette calculation\n",
    "                # Using distance_matrix (symmetric, so row/col indexing works for subset)\n",
    "                sub_distance_matrix = distance_matrix[clustered_points_indices[:, None], clustered_points_indices]\n",
    "                current_score = silhouette_score(sub_distance_matrix, cluster_labels[clustered_points_indices], metric='precomputed')\n",
    "            else:\n",
    "                current_score = -1 # Cannot calculate score for 0 or 1 clustered point\n",
    "        else:\n",
    "            current_score = -1 # Invalid number of clusters for silhouette score\n",
    "\n",
    "        results.append({\n",
    "            'eps': eps,\n",
    "            'min_samples': min_samples,\n",
    "            'num_clusters': n_clusters,\n",
    "            'num_noise': n_noise,\n",
    "            'silhouette_score': current_score\n",
    "        })\n",
    "\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_params['eps'] = eps\n",
    "            best_params['min_samples'] = min_samples\n",
    "            best_cluster_labels = cluster_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T17:55:37.803220464Z",
     "start_time": "2025-06-15T17:55:36.963332752Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Best DBSCAN Parameters Found ---\n",
      "Optimal eps: 90.1681\n",
      "Optimal min_samples: 15\n",
      "Best Silhouette Score: 0.5700\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Best DBSCAN Parameters Found ---\")\n",
    "if best_params['eps'] is not None:\n",
    "    print(f\"Optimal eps: {best_params['eps']:.4f}\")\n",
    "    print(f\"Optimal min_samples: {best_params['min_samples']}\")\n",
    "    print(f\"Best Silhouette Score: {best_score:.4f}\")\n",
    "else:\n",
    "    print(\"Could not find optimal parameters. Defaulting to a fixed setting (eps=5.0, min_samples=5).\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T17:55:39.091559567Z",
     "start_time": "2025-06-15T17:55:39.088121432Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# eps = best_params.get('eps', 5.0)  # Fallback to default if not found\n",
    "# min_samples = best_params.get('min_samples', 5)  # Fallback to default if not found\n",
    "\n",
    "eps = 0.8\n",
    "min_samples = 5  # Default value if no optimal found\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "cluster_labels = dbscan.fit_predict(distance_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:43:36.201908513Z",
     "start_time": "2025-06-15T18:43:36.161159615Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified normal traffic pattern as Cluster ID: 2\n",
      "Number of normal trips: 40\n",
      "\n",
      "Cluster distribution:\n",
      "  Noise points (-1): 166 trajectories\n",
      "  Cluster 0: 31 trajectories\n",
      "  Cluster 1: 36 trajectories\n",
      "  Cluster 2: 40 trajectories\n",
      "  Cluster 3: 16 trajectories\n",
      "  Cluster 4: 19 trajectories\n",
      "  Cluster 5: 15 trajectories\n",
      "  Cluster 6: 6 trajectories\n",
      "  Cluster 7: 30 trajectories\n",
      "  Cluster 8: 15 trajectories\n",
      "  Cluster 9: 6 trajectories\n",
      "  Cluster 10: 28 trajectories\n",
      "  Cluster 11: 27 trajectories\n",
      "  Cluster 12: 27 trajectories\n",
      "  Cluster 13: 34 trajectories\n",
      "  Cluster 14: 30 trajectories\n",
      "  Cluster 15: 17 trajectories\n",
      "  Cluster 16: 8 trajectories\n",
      "  Cluster 17: 21 trajectories\n",
      "  Cluster 18: 30 trajectories\n",
      "  Cluster 19: 16 trajectories\n",
      "  Cluster 20: 24 trajectories\n",
      "  Cluster 21: 21 trajectories\n",
      "  Cluster 22: 7 trajectories\n",
      "  Cluster 23: 8 trajectories\n",
      "  Cluster 24: 7 trajectories\n",
      "  Cluster 25: 7 trajectories\n",
      "  Cluster 26: 6 trajectories\n",
      "  Cluster 27: 5 trajectories\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identify the \"normal\" cluster (the largest one, excluding noise)\n",
    "trip_ids = list(compressed_trajectories.keys())\n",
    "# Filter out noise (-1) before finding the mode\n",
    "cluster_labels_no_noise = cluster_labels[cluster_labels != -1]\n",
    "normal_cluster_id_series = pd.Series(cluster_labels_no_noise).mode()\n",
    "\n",
    "if not normal_cluster_id_series.empty:\n",
    "    normal_cluster_id = normal_cluster_id_series[0]\n",
    "    normal_trip_ids = [trip_id for trip_id, label in zip(trip_ids, cluster_labels) if label == normal_cluster_id]\n",
    "    print(f\"\\nIdentified normal traffic pattern as Cluster ID: {normal_cluster_id}\")\n",
    "    print(f\"Number of normal trips: {len(normal_trip_ids)}\")\n",
    "else:\n",
    "    normal_trip_ids = []\n",
    "    print(\"\\nNo significant non-noise clusters found. Cannot proceed with normal cluster identification.\")\n",
    "\n",
    "# Print overall cluster distribution\n",
    "unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "print(\"\\nCluster distribution:\")\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    if label == -1:\n",
    "        print(f\"  Noise points (-1): {count} trajectories\")\n",
    "    else:\n",
    "        print(f\"  Cluster {label}: {count} trajectories\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:43:36.990136945Z",
     "start_time": "2025-06-15T18:43:36.986391111Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimized Trajectory Clusters Visualization ---\n",
      "--- Visualization complete. Use the interactive map to explore clusters.\n",
      "Interactive cluster visualization saved to data/trajectories_clusters_improved.html\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Plotting the clusters (Keep as is, will use new cluster_labels)\n",
    "from utils import visualize_trajectory_clusters_interactive_fancy\n",
    "\n",
    "fig = visualize_trajectory_clusters_interactive_fancy(compressed_trajectories, trip_ids, cluster_labels)\n",
    "fig.write_html(\"data/trajectories_clusters_improved.html\", include_plotlyjs='cdn')\n",
    "print(\"Interactive cluster visualization saved to data/trajectories_clusters_improved.html\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T18:43:38.136965471Z",
     "start_time": "2025-06-15T18:43:36.990686388Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
