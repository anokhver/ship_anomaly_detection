{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Maritime Data Preparation Pipeline\n",
    "\n",
    "This notebook contains a universal data preparation pipeline suitable for various ML algorithms including:\n",
    "- Classification algorithms (Logistic Regression, Random Forest, SVM, etc.)\n",
    "- Deep learning models (LSTM, Neural Networks, etc.)\n",
    "- Anomaly detection algorithms (One-Class SVM, Isolation Forest, etc.)\n",
    "- Time series models\n",
    "\n",
    "The pipeline focuses on feature engineering and data quality without algorithm-specific assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T18:40:36.288537201Z",
     "start_time": "2025-06-17T18:40:36.247667501Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T18:40:36.862657017Z",
     "start_time": "2025-06-17T18:40:36.852728638Z"
    }
   },
   "outputs": [],
   "source": [
    "# ───────────────────────────── Configuration ────────────────────────────── #\n",
    "\n",
    "# Data cleaning parameters\n",
    "DROP_TRIPS = [10257]  # List of trip IDs to exclude from analysis\n",
    "\n",
    "# Feature groups for different use cases\n",
    "CORE_FEATURES = [\n",
    "    \"speed_over_ground\", \"course_over_ground\", \"draught\",     # Basic vessel state\n",
    "    \"latitude\", \"longitude\", \"x_km\", \"y_km\",     # Spatial features\n",
    "    \"zone\", \"route_id\",     # Zone and route information\n",
    "    \"trip_duration_hours\", \"trip_distance_km\"     # Trip context\n",
    "]\n",
    "\n",
    "ENGINEERED_FEATURES = [\n",
    "    \"speed_change\", \"course_change\", \"draught_change\",     # Change-based features (deltas)\n",
    "    \"dist_to_route_center\", \"deviation_from_avg_route\",    # Trajectory features\n",
    "    \"time_since_trip_start\", \"progress_along_trip\",        # Temporal patterns\n",
    "    \"speed_rolling_mean\", \"speed_rolling_std\"              # Statistical features\n",
    "]\n",
    "\n",
    "# Geographic zones [lat_max, lat_min, lon_max, lon_min]\n",
    "GEOGRAPHIC_ZONES = [\n",
    "    [53.8, 53.5, 8.6, 8.14],   # Zone 0\n",
    "    [53.66, 53.0, 11.0, 9.5]   # Zone 1\n",
    "]\n",
    "\n",
    "# Constants\n",
    "EARTH_RADIUS_KM = 6371.0\n",
    "ROLLING_WINDOW = 5  # For rolling statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T18:40:37.549157688Z",
     "start_time": "2025-06-17T18:40:37.534590297Z"
    }
   },
   "outputs": [],
   "source": [
    "def haversine_distance(lat1: np.ndarray, lon1: np.ndarray, \n",
    "                      lat2: np.ndarray, lon2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate haversine distance between points in kilometers.\n",
    "    Vectorized implementation for efficiency.\n",
    "    \"\"\"\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = (np.sin(dlat / 2) ** 2 + \n",
    "         np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2)\n",
    "    return 2 * EARTH_RADIUS_KM * np.arcsin(np.sqrt(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "\n",
    "def lat_lon_to_km(df: pd.DataFrame,\n",
    "                  reference_lat: Optional[float] = None,\n",
    "                  reference_lon: Optional[float] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert latitude/longitude to local Cartesian coordinates in kilometers.\n",
    "    Uses the centroid as reference point if not specified.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if reference_lat is None:\n",
    "        reference_lat = df['latitude'].mean()\n",
    "    if reference_lon is None:\n",
    "        reference_lon = df['longitude'].mean()\n",
    "\n",
    "    # Conversion factors (approximate, valid for small areas)\n",
    "    km_per_deg_lon = 111.320 * np.cos(np.radians(reference_lat))\n",
    "    km_per_deg_lat = 110.574\n",
    "\n",
    "    df['x_km'] = (df['longitude'] - reference_lon) * km_per_deg_lon\n",
    "    df['y_km'] = (df['latitude'] - reference_lat) * km_per_deg_lat\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-17T18:40:37.905211678Z",
     "start_time": "2025-06-17T18:40:37.894242438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def assign_geographic_zone(lat: float, lon: float):\n",
    "    \"\"\"\n",
    "    Assign geographic zone based on lat/lon coordinates.\n",
    "    Returns zone index or -1 if outside all defined zones.\n",
    "    \"\"\"\n",
    "    for i, (lat_max, lat_min, lon_max, lon_min) in enumerate(GEOGRAPHIC_ZONES):\n",
    "        if lat_min <= lat <= lat_max and lon_min <= lon <= lon_max:\n",
    "            return i\n",
    "    return -1  # Outside all defined zones"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-17T18:40:38.572282715Z",
     "start_time": "2025-06-17T18:40:38.565514743Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T18:42:22.350492101Z",
     "start_time": "2025-06-17T18:42:22.305785261Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_clean_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load maritime data and perform basic cleaning operations.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the parquet file\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned DataFrame with basic preprocessing applied\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    print(f\"Loaded {len(df):,} rows\")\n",
    "    \n",
    "    # Remove specified problematic trips\n",
    "    if DROP_TRIPS:\n",
    "        n_dropped = len(df[df.trip_id.isin(DROP_TRIPS)])\n",
    "        df = df[~df.trip_id.isin(DROP_TRIPS)].reset_index(drop=True)\n",
    "        print(f\"Dropped {n_dropped:,} rows from trips {DROP_TRIPS}\")\n",
    "    \n",
    "    # Parse datetime columns\n",
    "    datetime_cols = [\"start_time\", \"end_time\", \"time_stamp\"]\n",
    "    for col in datetime_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    # Create standardized target variable\n",
    "    if 'is_anomaly' in df.columns:\n",
    "        df['target'] = df['is_anomaly'].astype('boolean')\n",
    "    \n",
    "    # Create route identifier\n",
    "    if 'start_port' in df.columns:\n",
    "        df['route_id'] = df['start_port']\n",
    "    \n",
    "    # Sort by trip and time for consistent processing\n",
    "    df = df.sort_values(['trip_id', 'time_stamp']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Cleaned data: {len(df):,} rows, {df.trip_id.nunique()} trips\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T18:42:23.258954222Z",
     "start_time": "2025-06-17T18:42:23.252826783Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_spatial_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add spatial features including zone assignment and coordinate conversion.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Assign geographic zones\n",
    "    print(\"Assigning geographic zones...\")\n",
    "    df['zone'] = df.progress_apply(\n",
    "        lambda row: assign_geographic_zone(row['latitude'], row['longitude']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Convert to local Cartesian coordinates\n",
    "    print(\"Converting to Cartesian coordinates...\")\n",
    "    df = lat_lon_to_km(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_change_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add features based on changes between consecutive points within each trip.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"Computing change-based features...\")\n",
    "    \n",
    "    # Calculate differences within each trip\n",
    "    for feature, new_name in [\n",
    "        ('speed_over_ground', 'speed_change'),\n",
    "        ('course_over_ground', 'course_change'),\n",
    "        ('draught', 'draught_change')\n",
    "    ]:\n",
    "        if feature in df.columns:\n",
    "            df[new_name] = df.groupby('trip_id')[feature].diff().abs().fillna(0)\n",
    "    \n",
    "    # Handle course change wraparound (0-360 degrees)\n",
    "    if 'course_change' in df.columns:\n",
    "        df['course_change'] = np.minimum(df['course_change'], 360 - df['course_change'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_trip_context_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add features that provide context about the trip.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"Computing trip context features...\")\n",
    "    \n",
    "    # Time since trip start\n",
    "    if 'time_stamp' in df.columns:\n",
    "        df['time_since_trip_start'] = (\n",
    "            df.groupby('trip_id')['time_stamp']\n",
    "            .transform(lambda x: (x - x.min()).dt.total_seconds() / 3600)  # Hours\n",
    "        )\n",
    "    \n",
    "    # Progress along trip (0 to 1)\n",
    "    trip_stats = df.groupby('trip_id').agg({\n",
    "        'time_stamp': ['min', 'max', 'count']\n",
    "    }).round(2)\n",
    "    trip_stats.columns = ['trip_start', 'trip_end', 'trip_points']\n",
    "    trip_stats['trip_duration_hours'] = (\n",
    "        (trip_stats['trip_end'] - trip_stats['trip_start']).dt.total_seconds() / 3600\n",
    "    )\n",
    "    \n",
    "    # Merge trip statistics back to main dataframe\n",
    "    df = df.merge(trip_stats[['trip_duration_hours']], left_on='trip_id', right_index=True)\n",
    "    \n",
    "    # Calculate cumulative distance within trip\n",
    "    def calc_cumulative_distance(group):\n",
    "        group = group.sort_values('time_stamp')\n",
    "        if len(group) < 2:\n",
    "            group['cumulative_distance'] = 0\n",
    "            group['progress_along_trip'] = 0\n",
    "            return group\n",
    "        \n",
    "        distances = haversine_distance(\n",
    "            group['latitude'].iloc[1:].values,\n",
    "            group['longitude'].iloc[1:].values,\n",
    "            group['latitude'].iloc[:-1].values,\n",
    "            group['longitude'].iloc[:-1].values\n",
    "        )\n",
    "        \n",
    "        group['cumulative_distance'] = np.concatenate([[0], np.cumsum(distances)])\n",
    "        total_distance = group['cumulative_distance'].iloc[-1]\n",
    "        \n",
    "        if total_distance > 0:\n",
    "            group['progress_along_trip'] = group['cumulative_distance'] / total_distance\n",
    "        else:\n",
    "            group['progress_along_trip'] = 0\n",
    "        \n",
    "        return group\n",
    "    \n",
    "    tqdm.pandas(desc=\"Computing trip distances\")\n",
    "    df = df.groupby('trip_id').progress_apply(calc_cumulative_distance).reset_index(drop=True)\n",
    "    \n",
    "    # Add trip distance to trip stats\n",
    "    trip_distances = df.groupby('trip_id')['cumulative_distance'].max()\n",
    "    df = df.merge(trip_distances.rename('trip_distance_km'), left_on='trip_id', right_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_rolling_features(df: pd.DataFrame, window: int = ROLLING_WINDOW) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add rolling statistical features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(f\"Computing rolling features (window={window})...\")\n",
    "    \n",
    "    # Rolling statistics for speed\n",
    "    if 'speed_over_ground' in df.columns:\n",
    "        df['speed_rolling_mean'] = (\n",
    "            df.groupby('trip_id')['speed_over_ground']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .mean()\n",
    "            .reset_index(0, drop=True)\n",
    "        )\n",
    "        \n",
    "        df['speed_rolling_std'] = (\n",
    "            df.groupby('trip_id')['speed_over_ground']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .std()\n",
    "            .reset_index(0, drop=True)\n",
    "            .fillna(0)\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Route-Specific Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T18:42:23.697704848Z",
     "start_time": "2025-06-17T18:42:23.674445855Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_route_reference(df_route: pd.DataFrame, n_points: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute reference trajectory for a route by averaging all trips.\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    \n",
    "    for trip_id, trip_data in df_route.groupby('trip_id'):\n",
    "        trip_data = trip_data.sort_values('time_stamp')\n",
    "        \n",
    "        if len(trip_data) < 2:\n",
    "            continue\n",
    "            \n",
    "        lat, lon = trip_data['latitude'].values, trip_data['longitude'].values\n",
    "        \n",
    "        # Calculate cumulative distance\n",
    "        distances = haversine_distance(lat[1:], lon[1:], lat[:-1], lon[:-1])\n",
    "        cum_dist = np.concatenate([[0], np.cumsum(distances)])\n",
    "        \n",
    "        if cum_dist[-1] <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Normalize to 0-1\n",
    "        normalized_dist = cum_dist / cum_dist[-1]\n",
    "        \n",
    "        # Interpolate to fixed number of points\n",
    "        target_positions = np.linspace(0, 1, n_points)\n",
    "        interp_lat = np.interp(target_positions, normalized_dist, lat)\n",
    "        interp_lon = np.interp(target_positions, normalized_dist, lon)\n",
    "        \n",
    "        trajectories.append(np.column_stack([interp_lat, interp_lon]))\n",
    "    \n",
    "    if not trajectories:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Average all trajectories\n",
    "    return np.mean(trajectories, axis=0)\n",
    "\n",
    "\n",
    "def add_route_deviation_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add features measuring deviation from typical route patterns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"Computing route deviation features...\")\n",
    "    \n",
    "    route_features = []\n",
    "    \n",
    "    for route_id in tqdm(df['route_id'].unique(), desc=\"Processing routes\"):\n",
    "        route_data = df[df['route_id'] == route_id].copy()\n",
    "        \n",
    "        # Compute reference trajectory\n",
    "        reference_traj = compute_route_reference(route_data)\n",
    "        \n",
    "        if reference_traj.size == 0:\n",
    "            route_data['deviation_from_avg_route'] = 0.0\n",
    "            route_data['dist_to_route_center'] = 0.0\n",
    "        else:\n",
    "            # Calculate route center\n",
    "            route_center_lat = route_data['latitude'].mean()\n",
    "            route_center_lon = route_data['longitude'].mean()\n",
    "            \n",
    "            # Distance to route center\n",
    "            route_data['dist_to_route_center'] = haversine_distance(\n",
    "                route_data['latitude'].values,\n",
    "                route_data['longitude'].values,\n",
    "                np.full(len(route_data), route_center_lat),\n",
    "                np.full(len(route_data), route_center_lon)\n",
    "            )\n",
    "            \n",
    "            # Deviation from average route\n",
    "            deviations = []\n",
    "            for _, trip in route_data.groupby('trip_id'):\n",
    "                trip = trip.sort_values('time_stamp')\n",
    "                \n",
    "                if len(trip) < 2:\n",
    "                    deviations.extend([0.0] * len(trip))\n",
    "                    continue\n",
    "                \n",
    "                # Calculate progress along trip\n",
    "                lat, lon = trip['latitude'].values, trip['longitude'].values\n",
    "                distances = haversine_distance(lat[1:], lon[1:], lat[:-1], lon[:-1])\n",
    "                cum_dist = np.concatenate([[0], np.cumsum(distances)])\n",
    "                \n",
    "                if cum_dist[-1] > 0:\n",
    "                    progress = cum_dist / cum_dist[-1]\n",
    "                else:\n",
    "                    progress = np.zeros(len(trip))\n",
    "                \n",
    "                # Find closest points on reference trajectory\n",
    "                trip_deviations = []\n",
    "                for i, p in enumerate(progress):\n",
    "                    ref_idx = min(int(p * (len(reference_traj) - 1)), len(reference_traj) - 1)\n",
    "                    deviation = haversine_distance(\n",
    "                        lat[i], lon[i],\n",
    "                        reference_traj[ref_idx, 0], reference_traj[ref_idx, 1]\n",
    "                    )\n",
    "                    trip_deviations.append(deviation)\n",
    "                \n",
    "                deviations.extend(trip_deviations)\n",
    "            \n",
    "            route_data['deviation_from_avg_route'] = deviations\n",
    "        \n",
    "        route_features.append(route_data)\n",
    "    \n",
    "    return pd.concat(route_features, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline execution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 535,273 rows\n",
      "Dropped 0 rows from trips [10257]\n",
      "Cleaned data: 535,273 rows, 423 trips\n"
     ]
    }
   ],
   "source": [
    "input_path = \"../data/cleaned/kiel_anomalies_labeled_2_fixed.parquet\"\n",
    "df = load_and_clean_data(input_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-17T18:42:43.860830184Z",
     "start_time": "2025-06-17T18:42:43.583619878Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
