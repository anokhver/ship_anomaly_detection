{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from fuzzy_matching import *\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "file_path = '../models/cleaned_atr.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n",
    "\n",
    "df['Destination'] = df['Destination'].fillna(\"nan\").astype('string')\n",
    "df['AisSourcen'] = df['AisSourcen'].fillna(\"nan\").astype('string')\n",
    "\n",
    "df['StartTime'] = pd.to_datetime(df['StartTime'], utc=True)\n",
    "df['EndTime'] = pd.to_datetime(df['EndTime'], utc=True)\n",
    "df['time'] = pd.to_datetime(df['time'], utc=True)  # Appears to have timezone info (+01:00)\n",
    "\n",
    "df['StartPort'] = df['StartPort'].astype('string').astype('category')\n",
    "df['EndPort'] = df['EndPort'].astype('string').astype('category')\n",
    "\n",
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ==================================== Step 1\n",
    "Deal with Inconsistent Records (We can have different formats representing the same thing) if they are there.\n",
    "And convert columns to categorical where appropriate.\n",
    "## ===================================="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_unique_values(df):\n",
    "    \"\"\"Check unique values in each column of the DataFrame.\"\"\"\n",
    "    col_un = {}\n",
    "    for col in df.columns:\n",
    "        clean_series = df[col].dropna()\n",
    "        nunique = clean_series.nunique()\n",
    "        col_un[col] = nunique\n",
    "    return col_un\n",
    "\n",
    "unique_values_before = check_unique_values(df)\n",
    "unique_values_before"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Column: StartPort, Unique values: 2 - seems correct\n",
    "Column: EndPort, Unique values: 2 - seems correct\n",
    "\n",
    "Column: Destination, Unique values: 140 - weird\n",
    "Column: AisSourcen, Unique values: 224 - should check if it is correct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Case normalization\n",
    "text_columns = df.select_dtypes(include=['string']).columns\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].str.upper()  # Ensure string type and uppercase\n",
    "\n",
    "unique_values_after = check_unique_values(df)\n",
    "\n",
    "changed_columns = list(filter(lambda col: unique_values_after[col] != unique_values_before[col], df.columns))\n",
    "df[changed_columns].dropna().nunique() # Check how many unique values are there after case normalization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Case normalization has changed the unique values in the following columns: Destination\n",
    "It seems there are still inconsistencies in the Destination column, so we will need to clean it further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Destination'].unique() #See some examples of the Destination column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Destination'] = df['Destination'].where(\n",
    "        df['Destination'].str.contains(r'[A-Za-z]', na=False),\n",
    "        \"NAN\"\n",
    "    ) # Atle ast one alphabetic\n",
    "\n",
    "df['Destination'] = df['Destination'].apply(\n",
    "    lambda x: \"NAN\" if re.match(r'^[A-Z]{2}$', str(x)) else x\n",
    ") #only country code\n",
    "\n",
    "def find_values_with_special_chars(df):\n",
    "    \"\"\"Find values with special characters in the 'Destination' column.\"\"\"\n",
    "    return [\n",
    "        value for value in df['Destination'].unique()\n",
    "        if re.search(r'[^A-Za-z0-9]', str(value))\n",
    "    ]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dest_before = find_values_with_special_chars(df)\n",
    "print(len(dest_before), \"unique values before cleaning with special characters\")\n",
    "dest_before"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ==================================== Step 2\n",
    "We can see that we have different formats representing the same thing, like 'HAMBURG' and 'DEHAM' ext.\n",
    "The data is incredibly messy, we need to handle country codes, special characters, and different formats.\n",
    "\n",
    "1. Some have country codes\n",
    "2. Some contain starting port too\n",
    "3. Some contain type of facilities (e.g., 'ELBE.RC', 'BREMERHAVEN.VIA.NOK')\n",
    "\n",
    "start > destination\n",
    "## ===================================="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clean all data\n",
    "df['Destination'] = df['Destination'].apply(clean_destination)\n",
    "df['Destination'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create mask for rows containing '>'\n",
    "mask = df['Destination'].str.contains('>', na=False)\n",
    "\n",
    "# Initialize columns (if not already done)\n",
    "df['start_fr_dest'] = None\n",
    "# df['cleaned_destination'] = df['Destination'].copy()\n",
    "\n",
    "# Split and assign values safely\n",
    "df.loc[mask, 'start_fr_dest'] = df.loc[mask, 'Destination'].str.split('>').str[0]\n",
    "df.loc[mask, 'Destination'] = df.loc[mask, 'Destination'].str.split('>').str[1]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **I CREATED CUSTOM FILE WITH FUNCTION WE WILL USE DOWN**\n",
    "\n",
    "df_recombined['Destination'].unique()\n",
    "Now we have somewhat cleaned Destination column, but we still have some inconsistencies. That the same ports have different names.\n",
    "I didn't find quicker way to do it, rather than manually checking the names and creating a list of names that represent the same port.\n",
    "We will only a bit automize this process, by using fuzzy matching to find similar names.\n",
    "And extracting all ports from UpdatedPub150.csv file, which contains ports and their countries.\n",
    "[link](https://msi.nga.mil/Publications/WPI)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ports_cvs = '../data/UpdatedPub150.csv'\n",
    "# port_df = pd.read_csv(ports_cvs)\n",
    "# save_filtered_ports(port_df, country_name='Poland')\n",
    "# save_filtered_ports(port_df, country_name='Germany')\n",
    "# save_filtered_ports(port_df, country_name='Lithuania')\n",
    "# save_filtered_ports(port_df, country_name='Sweden')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_dests = df['Destination'].unique().tolist()\n",
    "\n",
    "# Find matches with threshold of 85\n",
    "matches = find_fuzzy_matches(unique_dests, threshold=75, show_progress=True)\n",
    "\n",
    "# Print results grouped by similarity\n",
    "print_fuzzy_matches(matches, min_score=75, group_similar=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From this I will manually create a list of names that represent the same port, that do not have 100 match.\n",
    "As they can be incorrectly matched, and it would be better to do it manually.\n",
    "\n",
    "https://www.marinetraffic.com/en/ais/details/ports/347?name=HALMSTAD&country=Sweden\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['start_fr_dest'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "name_german = {\n",
    "    'DEHAM': [\"HAMBURG\", \"HAMBUG\", \"HH\", \"HAM\"],\n",
    "    'DEBRV': [\"BREMERHAVEN\", \"BREMENHAVEN\", \"BRV\", \"DEBHV\", \"BHV\"],\n",
    "    'DEBRE': [\"BREMEN\", \"BRE\"],\n",
    "    'DEKEL': [\"KIEL\", \"KEL\"],\n",
    "    'DEHAM.FINKENWERDER': [\"FINKENWERDER\", \"FINKENWERD\"],\n",
    "    'DEHAM.BLEXEN': [\"BLEXEN\"],\n",
    "    'DESTA': [\"STADE\", \"STAD\", \"STA\"],\n",
    "    'DEBRB' : [\"BRUNSBUETTEL\", \"BRUNSBUETT\", \"BRB\"],\n",
    "    'DEHAM.ELBE': [\"ELBE\"],\n",
    "    'DEVTT': [\"HIDDENSEE\", \"VTT\"],\n",
    "    'DEWVN': [\"WILHELMSHAVEN\", \"WVN\"],\n",
    "    'country': [\"DE\"]\n",
    "}\n",
    "\n",
    "name_poland = {\n",
    "    'PLGDN': [\"GDANSK\", \"GDANK\", \"GDN\"],\n",
    "    'PLGDY': [\"GDYNIA\", \"GYDNIA\", \"GYDINIA\", \"GDY\", \"GDYNA\"],\n",
    "    'SZCZECIN': [\"SZCZECIN\", \"SZCZECIN\", \"SZZ\"],\n",
    "     'country': [\"PL\"]\n",
    "}\n",
    "\n",
    "name_lythuania = {\n",
    "    'LTKLJ': [\"KLAIPEDA\", \"KLJ\"],\n",
    "    'country': [\"LT\"]\n",
    "}\n",
    "\n",
    "name_sweden = {\n",
    "    'SEHAD': [\"HALMSTAD\", \"HAD\"],\n",
    "    'SENOK': [\"NOK\"],\n",
    "    'SEAHU': [\"AHUS\", \"AHU\"],\n",
    "    'country': [\"SE\"]\n",
    "}\n",
    "\n",
    "name_russia = {\n",
    "        'RUKGD': [\"KALININGRAD\", \"KALININGRAD\", \"KAL\"],\n",
    "        'country': [\"RU\"]\n",
    "}\n",
    "\n",
    "name_denmark = {\n",
    "    'DKKOB': [\"KOBENHAVN\", \"COPENHAGEN\", \"COPENHAGUE\", \"CPH\"],\n",
    "    'country': [\"DK\"]\n",
    "}\n",
    "\n",
    "name_finland = {\n",
    "    'FIHKO': [\"HANKO\", \"HKO\"],\n",
    "    'country': [\"FI\"]\n",
    "}\n",
    "\n",
    "name_belgium = {\n",
    "    \"BEANR\": [\"BEANR\", \"ANR\"],\n",
    "    'country': [\"BE\"]\n",
    "}\n",
    "full_dict = [name_german, name_poland, name_lythuania, name_sweden, name_russia, name_finland, name_belgium]\n",
    "\n",
    "def replace_with_key(df, column, name_variants):\n",
    "    df[column] = df[column].apply(lambda x: match_names(x, name_variants))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = replace_with_key(df, 'Destination', full_dict)\n",
    "df = replace_with_key(df, 'start_fr_dest', full_dict)\n",
    "df[['Destination']].reset_index().drop_duplicates(subset=['Destination'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[['start_fr_dest']].reset_index().drop_duplicates(subset=['start_fr_dest'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "361854,387562,NORDEN.DEHAM\n",
    "Theoretically, I can switch places for it, but I am lazy\n",
    "\n",
    "Do we need to split it too? like debrv.via.nok?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path = '../data/normalized_destinations.csv'\n",
    "df.to_csv(save_path, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
