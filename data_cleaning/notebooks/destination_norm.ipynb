{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from data_cleaning.processing_utils import (\n",
    "    clean_destination,\n",
    "    match_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:09.921724629Z",
     "start_time": "2025-06-06T16:03:09.913498445Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Remark\n",
    "Destinations are being processed so only official ports are left and nothing else (can be changed by looking for NOTE comment)\n",
    "##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1060708 entries, 0 to 1060707\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count    Dtype              \n",
      "---  ------          --------------    -----              \n",
      " 0   TripID          1060708 non-null  int64              \n",
      " 1   StartLatitude   1060708 non-null  float64            \n",
      " 2   StartLongitude  1060708 non-null  float64            \n",
      " 3   StartTime       1060708 non-null  datetime64[ns, UTC]\n",
      " 4   EndLatitude     1060708 non-null  float64            \n",
      " 5   EndLongitude    1060708 non-null  float64            \n",
      " 6   EndTime         1060708 non-null  datetime64[ns, UTC]\n",
      " 7   StartPort       1060708 non-null  category           \n",
      " 8   EndPort         1060708 non-null  category           \n",
      " 9   time            1060708 non-null  datetime64[ns, UTC]\n",
      " 10  shiptype        1060708 non-null  int64              \n",
      " 11  Length          1060708 non-null  int64              \n",
      " 12  Breadth         1060708 non-null  int64              \n",
      " 13  Draught         1044438 non-null  float64            \n",
      " 14  Latitude        1060708 non-null  float64            \n",
      " 15  Longitude       1060708 non-null  float64            \n",
      " 16  SOG             1060708 non-null  float64            \n",
      " 17  COG             1060708 non-null  float64            \n",
      " 18  TH              1060708 non-null  int64              \n",
      " 19  Destination     1060708 non-null  category           \n",
      " 20  AisSourcen      1060708 non-null  string             \n",
      "dtypes: category(3), datetime64[ns, UTC](3), float64(9), int64(5), string(1)\n",
      "memory usage: 149.7 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = '../../data/type_norm.parquet'\n",
    "output_path = '../../data/prepared.parquet'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "\n",
    "df = pd.read_parquet(file_path)\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:10.239654529Z",
     "start_time": "2025-06-06T16:03:09.920557519Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1\n",
    "Deal with Inconsistent Records (We can have different formats representing the same thing) if they are there.\n",
    "And convert columns to categorical where appropriate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'TripID': 1126,\n 'StartLatitude': 28,\n 'StartLongitude': 34,\n 'StartTime': 953,\n 'EndLatitude': 29,\n 'EndLongitude': 47,\n 'EndTime': 943,\n 'StartPort': 2,\n 'EndPort': 2,\n 'time': 414193,\n 'shiptype': 11,\n 'Length': 107,\n 'Breadth': 36,\n 'Draught': 238,\n 'Latitude': 273,\n 'Longitude': 1285,\n 'SOG': 227,\n 'COG': 3602,\n 'TH': 361,\n 'Destination': 140,\n 'AisSourcen': 224}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_unique_values(df):\n",
    "    \"\"\"Check unique values in each column of the DataFrame.\"\"\"\n",
    "    col_un = {}\n",
    "    for col in df.columns:\n",
    "        clean_series = df[col].dropna()\n",
    "        nunique = clean_series.nunique()\n",
    "        col_un[col] = nunique\n",
    "    return col_un\n",
    "\n",
    "unique_values_before = check_unique_values(df)\n",
    "unique_values_before"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:10.739920568Z",
     "start_time": "2025-06-06T16:03:10.241776723Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Column: StartPort, Unique values: 2 - seems correct\n",
    "Column: EndPort, Unique values: 2 - seems correct\n",
    "\n",
    "Column: Destination, Unique values: 140 - weird\n",
    "Column: AisSourcen, Unique values: 224 - should check if it is correct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Series([], dtype: float64)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case normalization\n",
    "text_columns = df.select_dtypes(include=['string']).columns\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].str.upper()  # Ensure string type and uppercase\n",
    "\n",
    "unique_values_after = check_unique_values(df)\n",
    "\n",
    "changed_columns = list(filter(lambda col: unique_values_after[col] != unique_values_before[col], df.columns))\n",
    "df[changed_columns].dropna().nunique() # Check how many unique values are there after case normalization"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:13.278349020Z",
     "start_time": "2025-06-06T16:03:12.668514660Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Case normalization has changed the unique values in the following columns: Destination\n",
    "It seems there are still inconsistencies in the Destination column, so we will need to clean it further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['HAMBURG', 'DEHAM', 'DEBRE', 'DEBRV', 'ELBE.RC', ..., 'SZCZECIN', 'SEHAD', 'GDANSK.VIANOK', 'GDYNIA.PL', 'GDANKS']\nLength: 140\nCategories (140, object): [''GDYNIA.VIK?0\\\\BPO?_'', '>4_?', 'BE.ANR.>>.PL.GDN', 'BLEXEN.ROAD', ..., 'SEHAD', 'STADE', 'SZCZECIN', 'nan']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Destination'].unique() #See some examples of the Destination column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:14.133835737Z",
     "start_time": "2025-06-06T16:03:14.103031135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Ensure 'Destination' has at least one alphabetic character and is not just a country code\n",
    "df['Destination'] = df['Destination'].apply(\n",
    "    lambda x: \"NAN\" if not re.search(r'[A-Za-z]', str(x)) or re.match(r'^[A-Z]{2}$', str(x)) else x\n",
    ")\n",
    "\n",
    "def find_values_with_special_chars(df):\n",
    "    \"\"\"Find values with special characters in the 'Destination' column.\"\"\"\n",
    "    return [\n",
    "        value for value in df['Destination'].unique()\n",
    "        if re.search(r'[^A-Za-z0-9]', str(value))\n",
    "    ]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:36.398613705Z",
     "start_time": "2025-06-06T16:03:34.463171033Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 unique values before cleaning with special characters\n"
     ]
    },
    {
     "data": {
      "text/plain": "['ELBE.RC',\n 'HAMBURG.:)',\n 'BREMERHAVEN.VIA.NOK',\n 'DE.HAM',\n 'DE.HAMBIVER.ELBE',\n 'ELBE.PS',\n 'HHLO.PS',\n 'HAMBURG.DE',\n 'HAMBURG/.AIRBUS',\n 'BLEXEN.ROAD',\n 'BRV.PS',\n 'HH.FINKENWERDER',\n 'DEBRV.>.DEHAM',\n 'DEHAM.ELBE.PLT',\n 'DEHAM.ELBE',\n 'DEHAM.CTT',\n 'SEAHU.>.DEBRV',\n 'DEBRT.>.DEHAM',\n 'DE.BRV>DEHAM',\n 'DE.BRV>DE.HAM',\n 'HAMBURG..DE',\n 'DEBRV>DEHAM',\n 'DEHAM.EGH',\n 'DEHAM.CTA',\n 'DEHAM.EG',\n 'DE.HAM.............',\n 'DE.WVN.>.DE.HAM',\n 'DEBHV.NOK',\n 'DEBRV.EGH',\n 'DEBRV---->DEHAM',\n 'HAMBURG/EUR',\n 'DEBRV.>.DEIM',\n 'HH.PS',\n 'HH.CTB',\n 'HAM.PS',\n 'HAMBURG....',\n 'HAMBURG???',\n 'GDYNIA.VIA.NOK',\n 'GDYNIA.VIA.NOK.:)',\n 'GDYNIAVIA)NOK',\n 'KLJ.VIA.NOK',\n 'PLGDN.VIA.NOK',\n 'GDYNIA.VIA',\n 'GDYNIA.VIAE',\n 'GDYNIA.VIBIA.NOK',\n 'GDYNIA.VI',\n \"'GDYNIA.VIK?0\\\\\\\\BPO?_'\",\n 'GDYNIA.VIE',\n 'GDANSK.VIA.NOK',\n 'GDYNIA...TH',\n 'GDANSK.VICEL',\n 'KLAIPEDA.VIA.NOK',\n 'KLAIPEDA..=SWIN',\n 'GDANSK....=SWIN',\n 'GDANSK.VIA.KIEL.K',\n 'PL.GDY',\n 'PL.GDY.VIA.NOK',\n 'GDYNIA...!RSBURG',\n 'KLAIPEDA.VIA.NOC',\n 'GDYNIA.VIA.K.CANAL',\n 'GDANSK.PILOT',\n 'GDANSKVIA.NOK)',\n 'GDYNIA....>.BALTIC.2',\n 'GDYNIA....ROJECT',\n 'GDYNIA...!E',\n 'GDYNIA...!STELLE',\n 'PL.GDN',\n 'GDYNIA...!RATION',\n 'GDYNIA....>.HANKO',\n 'GDYNIA...DING.AREA',\n 'DEHAM.>.PLGDY',\n 'DEHAM.>.PLGDY.NOK',\n 'DEHAM.>.PL',\n 'GDYNIA.VIA.KIEL',\n 'GDYNIA.:)',\n 'GDYNIA/N.O.K',\n 'GDANSK....NOK',\n 'GDYNIA>HIDDENSEE',\n 'BE.ANR.>>.PL.GDN',\n 'GDYNIA-PLGDY',\n 'GDYNIA.POLAND',\n 'KIEL.CANAL-TRANS',\n 'PLGDY.VIA.NOK',\n 'GDANSK....KCHIODE',\n 'GDANSK...!',\n 'GDANSK...#INDFARM',\n 'KIEL.CANAL',\n 'GDANSK...!A.KIEL',\n 'GDANSK...',\n 'GDANSKVIA.NOK',\n 'GDYNIA.VICINOUJSCIE',\n 'GDYNIA..VIA.NOK',\n 'GDANSKCIKINGER.WF',\n 'LT.KLAIPEDA',\n 'GDYNIA....Y',\n 'GDANSK./.POLAND',\n 'GDANSK...AS',\n 'GDANSK.VIANOK',\n 'GDYNIA.PL']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_before = find_values_with_special_chars(df)\n",
    "print(len(dest_before), \"unique values before cleaning with special characters\")\n",
    "dest_before"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:36.468360737Z",
     "start_time": "2025-06-06T16:03:36.407674254Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ==================================== Step 2\n",
    "We can see that we have different formats representing the same thing, like 'HAMBURG' and 'DEHAM' ext.\n",
    "The data is incredibly messy, we need to handle country codes, special characters, and different formats.\n",
    "\n",
    "1. Some have country codes\n",
    "2. Some contain starting port too\n",
    "3. Some contain type of facilities (e.g., 'ELBE.RC', 'BREMERHAVEN.VIA.NOK')\n",
    "\n",
    "start > destination\n",
    "## ===================================="
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['HAMBURG', 'DEHAM', 'DEBRE', 'DEBRV', 'ELBE.RC',\n       'BREMERHAVEN.VIA.NOK', 'DE.HAM', 'DE.HAMBIVER.ELBE', 'ELBE.PS',\n       'HHLO.PS', 'HAMBURG.DE', 'HAMBURG.AIRBUS', 'BREMENHAVEN',\n       'FINKENWERDER', 'BLEXEN.ROAD', 'BRV.PS', 'HH.FINKENWERDER',\n       'DEBRV>DEHAM', 'COPENHAGEN', 'DEHAM.ELBE.PLT', 'DEHAM.ELBE',\n       'DEHAM.CTT', 'SEAHU>DEBRV', 'DEBRT>DEHAM', 'STADE', 'DE.BRV>DEHAM',\n       'DE.BRV>DE.HAM', 'DEHAMCTA', 'BREMERHAVEN', 'DEHAM.EGH',\n       'DEHAM.CTA', 'DEHAM.EG', 'HAMBUG', 'DEBHV', 'DE.WVN>DE.HAM',\n       'DEBHV.NOK', 'NAN', 'DEBRV.EGH', 'HAMBURG.EUR', 'DEBRV>DEIM',\n       'GDANSK', 'HH.PS', 'HH.CTB', 'HAM.PS', 'FINKENWERD', 'NORDENHAM',\n       'GDYNIA.VIA.NOK', 'GDYNIA', 'GYDINIA', 'GDYNIAVIANOK',\n       'KLJ.VIA.NOK', 'PLGDY', 'PLGDN.VIA.NOK', 'GDYNIA.VIA',\n       'GDYNIA.VIAE', 'GDYNIA.VIBIA.NOK', 'GDYNIA.VI', 'GDYNIA.VIK0.BPO',\n       'GDYNIA.VIE', 'GDANSK.VIA.NOK', 'GDYNIA.TH', 'GDANSK.VICEL',\n       'KLAIPEDA.VIA.NOK', 'KLAIPEDA', 'KLAIPEDA.SWIN', 'GDANSK.SWIN',\n       'GDANK', 'GDANSK.VIA.KIEL.K', 'PL.GDY', 'PL.GDY.VIA.NOK',\n       'GDYNIA.RSBURG', 'KLAIPEDA.VIA.NOC', 'GDYNIA.VIA.K.CANAL',\n       'GDANSK.PILOT', 'GDANSKVIA.NOK', 'GDYNIA>BALTIC.2',\n       'GDYNIA.ROJECT', 'GDYNYA', 'PLGDN', 'GDYNIA.E', 'GDYNIA.STELLE',\n       'PL.GDN', 'GDYNIA.RATION', 'GDYNIA>HANKO', 'GDYNIA.DING.AREA',\n       'DEHAM>PLGDY', 'NOK', 'GYDNIA', 'DEHAM>PLGDY.NOK', 'DEHAM>PL',\n       'GDYNIA.VIA.KIEL', 'GDANSKARATION', 'KLAIPEDACINDFARM',\n       'GDYNIA.N.O.K', 'GDYNI', 'GDANSK.NOK', 'GDYNIASTRALSUND',\n       'GDYNIA>HIDDENSEE', 'GDYNIAC', 'GDYNIACRT', 'BE.ANR>PL.GDN',\n       'GDYNIAPLGDY', 'GDYNIA.POLAND', 'KIEL.CANALTRANS', 'PLGDY.VIA.NOK',\n       'GDANSK.KCHIODE', 'GDANSK.INDFARM', 'KIEL.CANAL', 'GDANSK.A.KIEL',\n       'GDYNIA.VICINOUJSCIE', 'GDANSKCIKINGER.WF', 'LT.KLAIPEDA',\n       'GDYNIA.Y', 'GDANSK.POLAND', 'HALMSTAD', 'GDYNA', 'GDANSK.AS',\n       'BRUNSBUETTEL', 'KALININGRAD', 'SZCZECIN', 'SEHAD',\n       'GDANSK.VIANOK', 'GDYNIA.PL', 'GDANKS'], dtype=object)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean all data\n",
    "df['Destination'] = df['Destination'].apply(clean_destination)\n",
    "df['Destination'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:46.144979510Z",
     "start_time": "2025-06-06T16:03:36.466038507Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['HAMBURG', 'DEHAM', 'DEBRE', 'DEBRV', 'ELBE.RC',\n       'BREMERHAVEN.VIA.NOK', 'DE.HAM', 'DE.HAMBIVER.ELBE', 'ELBE.PS',\n       'HHLO.PS', 'HAMBURG.DE', 'HAMBURG.AIRBUS', 'BREMENHAVEN',\n       'FINKENWERDER', 'BLEXEN.ROAD', 'BRV.PS', 'HH.FINKENWERDER',\n       'COPENHAGEN', 'DEHAM.ELBE.PLT', 'DEHAM.ELBE', 'DEHAM.CTT', 'STADE',\n       'DEHAMCTA', 'BREMERHAVEN', 'DEHAM.EGH', 'DEHAM.CTA', 'DEHAM.EG',\n       'HAMBUG', 'DEBHV', 'DEBHV.NOK', 'NAN', 'DEBRV.EGH', 'HAMBURG.EUR',\n       'DEIM', 'GDANSK', 'HH.PS', 'HH.CTB', 'HAM.PS', 'FINKENWERD',\n       'NORDENHAM', 'GDYNIA.VIA.NOK', 'GDYNIA', 'GYDINIA', 'GDYNIAVIANOK',\n       'KLJ.VIA.NOK', 'PLGDY', 'PLGDN.VIA.NOK', 'GDYNIA.VIA',\n       'GDYNIA.VIAE', 'GDYNIA.VIBIA.NOK', 'GDYNIA.VI', 'GDYNIA.VIK0.BPO',\n       'GDYNIA.VIE', 'GDANSK.VIA.NOK', 'GDYNIA.TH', 'GDANSK.VICEL',\n       'KLAIPEDA.VIA.NOK', 'KLAIPEDA', 'KLAIPEDA.SWIN', 'GDANSK.SWIN',\n       'GDANK', 'GDANSK.VIA.KIEL.K', 'PL.GDY', 'PL.GDY.VIA.NOK',\n       'GDYNIA.RSBURG', 'KLAIPEDA.VIA.NOC', 'GDYNIA.VIA.K.CANAL',\n       'GDANSK.PILOT', 'GDANSKVIA.NOK', 'BALTIC.2', 'GDYNIA.ROJECT',\n       'GDYNYA', 'PLGDN', 'GDYNIA.E', 'GDYNIA.STELLE', 'PL.GDN',\n       'GDYNIA.RATION', 'HANKO', 'GDYNIA.DING.AREA', 'NOK', 'GYDNIA',\n       'PLGDY.NOK', 'PL', 'GDYNIA.VIA.KIEL', 'GDANSKARATION',\n       'KLAIPEDACINDFARM', 'GDYNIA.N.O.K', 'GDYNI', 'GDANSK.NOK',\n       'GDYNIASTRALSUND', 'HIDDENSEE', 'GDYNIAC', 'GDYNIACRT',\n       'GDYNIAPLGDY', 'GDYNIA.POLAND', 'KIEL.CANALTRANS', 'PLGDY.VIA.NOK',\n       'GDANSK.KCHIODE', 'GDANSK.INDFARM', 'KIEL.CANAL', 'GDANSK.A.KIEL',\n       'GDYNIA.VICINOUJSCIE', 'GDANSKCIKINGER.WF', 'LT.KLAIPEDA',\n       'GDYNIA.Y', 'GDANSK.POLAND', 'HALMSTAD', 'GDYNA', 'GDANSK.AS',\n       'BRUNSBUETTEL', 'KALININGRAD', 'SZCZECIN', 'SEHAD',\n       'GDANSK.VIANOK', 'GDYNIA.PL', 'GDANKS'], dtype=object)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mask for rows containing '>'\n",
    "mask = df['Destination'].str.contains('>', na=False)\n",
    "\n",
    "# Initialize columns (if not already done)\n",
    "# df['start_fr_dest'] = None\n",
    "# df['cleaned_destination'] = df['Destination'].copy()\n",
    "# Split and assign values safely\n",
    "# df.loc[mask, 'start_fr_dest'] = df.loc[mask, 'Destination'].str.split('>').str[0] #NOTE for now we wont bother\n",
    "\n",
    "df.loc[mask, 'Destination'] = df.loc[mask, 'Destination'].str.split('>').str[1]\n",
    "df['Destination'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:46.428915247Z",
     "start_time": "2025-06-06T16:03:46.144086273Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **I CREATED CUSTOM FILE WITH FUNCTION WE WILL USE DOWN**\n",
    "\n",
    "df_recombined['Destination'].unique()\n",
    "Now we have somewhat cleaned Destination column, but we still have some inconsistencies. That the same ports have different names.\n",
    "I didn't find quicker way to do it, rather than manually checking the names and creating a list of names that represent the same port.\n",
    "We will only a bit automize this process, by using fuzzy matching to find similar names.\n",
    "And extracting all ports from UpdatedPub150.csv file, which contains ports and their countries.\n",
    "[link](https://msi.nga.mil/Publications/WPI)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# from fuzzywuzzy import fuzz\n",
    "#\n",
    "# def find_fuzzy_matches(destinations, threshold=80, scorer=fuzz.token_set_ratio, show_progress=False):\n",
    "#     \"\"\"\n",
    "#     Find fuzzy matches among destination names.\n",
    "#\n",
    "#     Parameters:\n",
    "#         destinations (list): List of destination strings to compare\n",
    "#         threshold (int): Minimum similarity score to consider a match (0-100)\n",
    "#         scorer: Fuzzy matching function (default: token_set_ratio)\n",
    "#         show_progress (bool): Whether to print progress during processing\n",
    "#\n",
    "#     Returns:\n",
    "#         dict: Dictionary where keys are original names and values are lists of matches\n",
    "#               with their scores in format [(matched_name, score), ...]\n",
    "#     \"\"\"\n",
    "#     matches = {}\n",
    "#     total = len(destinations)\n",
    "#\n",
    "#     for i, dest in enumerate(destinations, 1):\n",
    "#         # Skip NAN/empty values\n",
    "#         if not dest or str(dest).strip().upper() in ('NAN', 'NULL', ''):\n",
    "#             continue\n",
    "#\n",
    "#         if show_progress:\n",
    "#             print(f\"Processing {i}/{total}: {dest[:30]}...\", end='\\r')\n",
    "#\n",
    "#         # Find matches above threshold (excluding self)\n",
    "#         potential_matches = process.extract(\n",
    "#             dest,\n",
    "#             destinations,\n",
    "#             scorer=scorer,\n",
    "#             limit=None\n",
    "#         )\n",
    "#\n",
    "#         # Filter matches\n",
    "#         good_matches = [\n",
    "#             (match, score)\n",
    "#             for match, score in potential_matches\n",
    "#             if score >= threshold and match != dest\n",
    "#         ]\n",
    "#\n",
    "#         if good_matches:\n",
    "#             matches[dest] = good_matches\n",
    "#\n",
    "#     if show_progress:\n",
    "#         print(\"\\n\" + \"=\" * 50)\n",
    "#\n",
    "#     return matches\n",
    "# def print_fuzzy_matches(matches, min_score=0, group_similar=False):\n",
    "#     \"\"\"\n",
    "#     Print fuzzy matching results in a readable format.\n",
    "#\n",
    "#     Parameters:\n",
    "#         matches (dict): Output from find_fuzzy_matches\n",
    "#         min_score (int): Minimum score to display\n",
    "#         group_similar (bool): Whether to group similar matches together\n",
    "#     \"\"\"\n",
    "#     if not matches:\n",
    "#         print(\"No matches found\")\n",
    "#         return\n",
    "#\n",
    "#     print(f\"\\nFuzzy matches (score ≥ {min_score}):\")\n",
    "#     print(\"=\" * 60)\n",
    "#\n",
    "#     if group_similar:\n",
    "#         # Group similar matches to avoid duplicates\n",
    "#         already_matched = set()\n",
    "#         for dest in sorted(matches.keys()):\n",
    "#             if dest in already_matched:\n",
    "#                 continue\n",
    "#\n",
    "#             print(f\"\\nGroup: {dest}\")\n",
    "#             print(\"-\" * 50)\n",
    "#\n",
    "#             # Include the original in the group\n",
    "#             all_in_group = {dest}\n",
    "#\n",
    "#             for match, score in matches[dest]:\n",
    "#                 if score >= min_score:\n",
    "#                     print(f\"  → {match} (score: {score})\")\n",
    "#                     all_in_group.add(match)\n",
    "#\n",
    "#                     # Also include matches of matches\n",
    "#                     if match in matches:\n",
    "#                         for submatch, subscore in matches[match]:\n",
    "#                             if subscore >= min_score and submatch not in all_in_group:\n",
    "#                                 print(f\"    → {submatch} (score: {subscore})\")\n",
    "#                                 all_in_group.add(submatch)\n",
    "#\n",
    "#             already_matched.update(all_in_group)\n",
    "#     else:\n",
    "#         # Simple listing\n",
    "#         for dest in sorted(matches.keys()):\n",
    "#             print(f\"\\n{dest} matches:\")\n",
    "#             print(\"-\" * 50)\n",
    "#             for match, score in matches[dest]:\n",
    "#                 if score >= min_score:\n",
    "#                     print(f\"  → {match} (score: {score})\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:46.435083817Z",
     "start_time": "2025-06-06T16:03:46.434114886Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# unique_dests = df['Destination'].unique().tolist()\n",
    "#\n",
    "# # Find matches with threshold of 85\n",
    "# matches = find_fuzzy_matches(unique_dests, threshold=75, show_progress=True)\n",
    "#\n",
    "# # Print results grouped by similarity\n",
    "# print_fuzzy_matches(matches, min_score=75, group_similar=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:46.445418227Z",
     "start_time": "2025-06-06T16:03:46.436373329Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "From this I will manually create a list of names that represent the same port, that do not have 100 match.\n",
    "As they can be incorrectly matched, and it would be better to do it manually.\n",
    "\n",
    "https://www.marinetraffic.com/en/ais/details/ports/347?name=HALMSTAD&country=Sweden\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from data_cleaning.processing_utils import match_names\n",
    "\n",
    "def replace_with_key(df, column):\n",
    "    df[column] = df[column].apply(lambda x: match_names(x))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:03:46.459799505Z",
     "start_time": "2025-06-06T16:03:46.447546245Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "           index Destination\n0              0      DE.HAM\n2894        2894      DE.BRE\n5160        5160      DE.BRV\n76207      76207      DK.KOB\n159264    159264      DE.STA\n243332    243332        None\n303836    303836      PL.GDN\n479678    479678      PL.GDY\n486420    486420      LT.KLJ\n532599    532599      DE.KEL\n611349    611349      FI.HKO\n616473    616473      SE.NOK\n684699    684699      DE.VTT\n953480    953480      DE.BRB\n982201    982201      RU.KGD\n998470    998470      PL.SZZ\n1004254  1004254      SE.HAD",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>Destination</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>DE.HAM</td>\n    </tr>\n    <tr>\n      <th>2894</th>\n      <td>2894</td>\n      <td>DE.BRE</td>\n    </tr>\n    <tr>\n      <th>5160</th>\n      <td>5160</td>\n      <td>DE.BRV</td>\n    </tr>\n    <tr>\n      <th>76207</th>\n      <td>76207</td>\n      <td>DK.KOB</td>\n    </tr>\n    <tr>\n      <th>159264</th>\n      <td>159264</td>\n      <td>DE.STA</td>\n    </tr>\n    <tr>\n      <th>243332</th>\n      <td>243332</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>303836</th>\n      <td>303836</td>\n      <td>PL.GDN</td>\n    </tr>\n    <tr>\n      <th>479678</th>\n      <td>479678</td>\n      <td>PL.GDY</td>\n    </tr>\n    <tr>\n      <th>486420</th>\n      <td>486420</td>\n      <td>LT.KLJ</td>\n    </tr>\n    <tr>\n      <th>532599</th>\n      <td>532599</td>\n      <td>DE.KEL</td>\n    </tr>\n    <tr>\n      <th>611349</th>\n      <td>611349</td>\n      <td>FI.HKO</td>\n    </tr>\n    <tr>\n      <th>616473</th>\n      <td>616473</td>\n      <td>SE.NOK</td>\n    </tr>\n    <tr>\n      <th>684699</th>\n      <td>684699</td>\n      <td>DE.VTT</td>\n    </tr>\n    <tr>\n      <th>953480</th>\n      <td>953480</td>\n      <td>DE.BRB</td>\n    </tr>\n    <tr>\n      <th>982201</th>\n      <td>982201</td>\n      <td>RU.KGD</td>\n    </tr>\n    <tr>\n      <th>998470</th>\n      <td>998470</td>\n      <td>PL.SZZ</td>\n    </tr>\n    <tr>\n      <th>1004254</th>\n      <td>1004254</td>\n      <td>SE.HAD</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = replace_with_key(df, 'Destination')\n",
    "# df = replace_with_key(df, 'start_fr_dest', full_dict)\n",
    "df[['Destination']].reset_index().drop_duplicates(subset=['Destination'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:04:12.099589213Z",
     "start_time": "2025-06-06T16:03:46.454617601Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# df[['start_fr_dest']].reset_index().drop_duplicates(subset=['start_fr_dest'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:04:12.100152593Z",
     "start_time": "2025-06-06T16:04:12.089175516Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "          TripID  StartLatitude  StartLongitude                 StartTime  \\\n0          39131          53.57            8.53 2016-01-24 08:06:00+00:00   \n1          39131          53.57            8.53 2016-01-24 08:06:00+00:00   \n2          39131          53.57            8.53 2016-01-24 08:06:00+00:00   \n3          39131          53.57            8.53 2016-01-24 08:06:00+00:00   \n4          39131          53.57            8.53 2016-01-24 08:06:00+00:00   \n...          ...            ...             ...                       ...   \n1060703  2204049          54.36           10.14 2017-04-03 07:54:00+00:00   \n1060704  2204049          54.36           10.14 2017-04-03 07:54:00+00:00   \n1060705  2204049          54.36           10.14 2017-04-03 07:54:00+00:00   \n1060706  2204049          54.36           10.14 2017-04-03 07:54:00+00:00   \n1060707  2204049          54.36           10.14 2017-04-03 07:54:00+00:00   \n\n         EndLatitude  EndLongitude                   EndTime    StartPort  \\\n0              53.53          9.90 2016-01-24 16:44:00+00:00  BREMERHAVEN   \n1              53.53          9.90 2016-01-24 16:44:00+00:00  BREMERHAVEN   \n2              53.53          9.90 2016-01-24 16:44:00+00:00  BREMERHAVEN   \n3              53.53          9.90 2016-01-24 16:44:00+00:00  BREMERHAVEN   \n4              53.53          9.90 2016-01-24 16:44:00+00:00  BREMERHAVEN   \n...              ...           ...                       ...          ...   \n1060703        54.38         18.66 2017-04-04 15:28:00+00:00         KIEL   \n1060704        54.38         18.66 2017-04-04 15:28:00+00:00         KIEL   \n1060705        54.38         18.66 2017-04-04 15:28:00+00:00         KIEL   \n1060706        54.38         18.66 2017-04-04 15:28:00+00:00         KIEL   \n1060707        54.38         18.66 2017-04-04 15:28:00+00:00         KIEL   \n\n         EndPort                      time  ...  Length  Breadth  Draught  \\\n0        HAMBURG 2016-01-24 08:07:00+00:00  ...     277       42    11.54   \n1        HAMBURG 2016-01-24 08:10:00+00:00  ...     277       42    11.54   \n2        HAMBURG 2016-01-24 08:10:00+00:00  ...     277       42    11.54   \n3        HAMBURG 2016-01-24 08:12:00+00:00  ...     277       42    11.54   \n4        HAMBURG 2016-01-24 08:16:00+00:00  ...     277       42    11.54   \n...          ...                       ...  ...     ...      ...      ...   \n1060703   GDYNIA 2017-04-04 13:57:00+00:00  ...      89       13     4.00   \n1060704   GDYNIA 2017-04-04 13:56:00+00:00  ...      89       13     4.00   \n1060705   GDYNIA 2017-04-04 13:55:00+00:00  ...      89       13     4.00   \n1060706   GDYNIA 2017-04-04 13:54:00+00:00  ...      89       13     4.00   \n1060707   GDYNIA 2017-04-04 13:53:00+00:00  ...      89       13     4.00   \n\n         Latitude  Longitude  SOG    COG   TH  Destination  \\\n0           53.57       8.53  0.7  331.2  143       DE.HAM   \n1           53.57       8.53  1.6  315.3  117       DE.HAM   \n2           53.57       8.53  2.8  322.6  100       DE.HAM   \n3           53.57       8.53  2.8  286.3   74       DE.HAM   \n4           53.57       8.53  4.3  333.1  333       DE.HAM   \n...           ...        ...  ...    ...  ...          ...   \n1060703     54.51      18.75  7.2  221.0  215       PL.GDN   \n1060704     54.51      18.75  7.2  221.9  215       PL.GDN   \n1060705     54.51      18.75  7.2  222.1  215       PL.GDN   \n1060706     54.51      18.76  7.2  221.2  215       PL.GDN   \n1060707     54.51      18.76  7.2  221.0  214       PL.GDN   \n\n                   AisSourcen  \n0        DAIS1.81B.90B.71.71A  \n1        DAIS1.81B.90B.71.71A  \n2        DAIS1.81B.90B.71.71A  \n3        DAIS1.81B.90B.71.71A  \n4        DAIS1.81B.90B.71.71A  \n...                       ...  \n1060703                 H7001  \n1060704                 H7001  \n1060705                 H7001  \n1060706                 H7001  \n1060707                 H7001  \n\n[913622 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TripID</th>\n      <th>StartLatitude</th>\n      <th>StartLongitude</th>\n      <th>StartTime</th>\n      <th>EndLatitude</th>\n      <th>EndLongitude</th>\n      <th>EndTime</th>\n      <th>StartPort</th>\n      <th>EndPort</th>\n      <th>time</th>\n      <th>...</th>\n      <th>Length</th>\n      <th>Breadth</th>\n      <th>Draught</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>SOG</th>\n      <th>COG</th>\n      <th>TH</th>\n      <th>Destination</th>\n      <th>AisSourcen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39131</td>\n      <td>53.57</td>\n      <td>8.53</td>\n      <td>2016-01-24 08:06:00+00:00</td>\n      <td>53.53</td>\n      <td>9.90</td>\n      <td>2016-01-24 16:44:00+00:00</td>\n      <td>BREMERHAVEN</td>\n      <td>HAMBURG</td>\n      <td>2016-01-24 08:07:00+00:00</td>\n      <td>...</td>\n      <td>277</td>\n      <td>42</td>\n      <td>11.54</td>\n      <td>53.57</td>\n      <td>8.53</td>\n      <td>0.7</td>\n      <td>331.2</td>\n      <td>143</td>\n      <td>DE.HAM</td>\n      <td>DAIS1.81B.90B.71.71A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39131</td>\n      <td>53.57</td>\n      <td>8.53</td>\n      <td>2016-01-24 08:06:00+00:00</td>\n      <td>53.53</td>\n      <td>9.90</td>\n      <td>2016-01-24 16:44:00+00:00</td>\n      <td>BREMERHAVEN</td>\n      <td>HAMBURG</td>\n      <td>2016-01-24 08:10:00+00:00</td>\n      <td>...</td>\n      <td>277</td>\n      <td>42</td>\n      <td>11.54</td>\n      <td>53.57</td>\n      <td>8.53</td>\n      <td>1.6</td>\n      <td>315.3</td>\n      <td>117</td>\n      <td>DE.HAM</td>\n      <td>DAIS1.81B.90B.71.71A</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>39131</td>\n      <td>53.57</td>\n      <td>8.53</td>\n      <td>2016-01-24 08:06:00+00:00</td>\n      <td>53.53</td>\n      <td>9.90</td>\n      <td>2016-01-24 16:44:00+00:00</td>\n      <td>BREMERHAVEN</td>\n      <td>HAMBURG</td>\n      <td>2016-01-24 08:10:00+00:00</td>\n      <td>...</td>\n      <td>277</td>\n      <td>42</td>\n      <td>11.54</td>\n      <td>53.57</td>\n      <td>8.53</td>\n      <td>2.8</td>\n      <td>322.6</td>\n      <td>100</td>\n      <td>DE.HAM</td>\n      <td>DAIS1.81B.90B.71.71A</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>39131</td>\n      <td>53.57</td>\n      <td>8.53</td>\n      <td>2016-01-24 08:06:00+00:00</td>\n      <td>53.53</td>\n      <td>9.90</td>\n      <td>2016-01-24 16:44:00+00:00</td>\n      <td>BREMERHAVEN</td>\n      <td>HAMBURG</td>\n      <td>2016-01-24 08:12:00+00:00</td>\n      <td>...</td>\n      <td>277</td>\n      <td>42</td>\n      <td>11.54</td>\n      <td>53.57</td>\n      <td>8.53</td>\n      <td>2.8</td>\n      <td>286.3</td>\n      <td>74</td>\n      <td>DE.HAM</td>\n      <td>DAIS1.81B.90B.71.71A</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>39131</td>\n      <td>53.57</td>\n      <td>8.53</td>\n      <td>2016-01-24 08:06:00+00:00</td>\n      <td>53.53</td>\n      <td>9.90</td>\n      <td>2016-01-24 16:44:00+00:00</td>\n      <td>BREMERHAVEN</td>\n      <td>HAMBURG</td>\n      <td>2016-01-24 08:16:00+00:00</td>\n      <td>...</td>\n      <td>277</td>\n      <td>42</td>\n      <td>11.54</td>\n      <td>53.57</td>\n      <td>8.53</td>\n      <td>4.3</td>\n      <td>333.1</td>\n      <td>333</td>\n      <td>DE.HAM</td>\n      <td>DAIS1.81B.90B.71.71A</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1060703</th>\n      <td>2204049</td>\n      <td>54.36</td>\n      <td>10.14</td>\n      <td>2017-04-03 07:54:00+00:00</td>\n      <td>54.38</td>\n      <td>18.66</td>\n      <td>2017-04-04 15:28:00+00:00</td>\n      <td>KIEL</td>\n      <td>GDYNIA</td>\n      <td>2017-04-04 13:57:00+00:00</td>\n      <td>...</td>\n      <td>89</td>\n      <td>13</td>\n      <td>4.00</td>\n      <td>54.51</td>\n      <td>18.75</td>\n      <td>7.2</td>\n      <td>221.0</td>\n      <td>215</td>\n      <td>PL.GDN</td>\n      <td>H7001</td>\n    </tr>\n    <tr>\n      <th>1060704</th>\n      <td>2204049</td>\n      <td>54.36</td>\n      <td>10.14</td>\n      <td>2017-04-03 07:54:00+00:00</td>\n      <td>54.38</td>\n      <td>18.66</td>\n      <td>2017-04-04 15:28:00+00:00</td>\n      <td>KIEL</td>\n      <td>GDYNIA</td>\n      <td>2017-04-04 13:56:00+00:00</td>\n      <td>...</td>\n      <td>89</td>\n      <td>13</td>\n      <td>4.00</td>\n      <td>54.51</td>\n      <td>18.75</td>\n      <td>7.2</td>\n      <td>221.9</td>\n      <td>215</td>\n      <td>PL.GDN</td>\n      <td>H7001</td>\n    </tr>\n    <tr>\n      <th>1060705</th>\n      <td>2204049</td>\n      <td>54.36</td>\n      <td>10.14</td>\n      <td>2017-04-03 07:54:00+00:00</td>\n      <td>54.38</td>\n      <td>18.66</td>\n      <td>2017-04-04 15:28:00+00:00</td>\n      <td>KIEL</td>\n      <td>GDYNIA</td>\n      <td>2017-04-04 13:55:00+00:00</td>\n      <td>...</td>\n      <td>89</td>\n      <td>13</td>\n      <td>4.00</td>\n      <td>54.51</td>\n      <td>18.75</td>\n      <td>7.2</td>\n      <td>222.1</td>\n      <td>215</td>\n      <td>PL.GDN</td>\n      <td>H7001</td>\n    </tr>\n    <tr>\n      <th>1060706</th>\n      <td>2204049</td>\n      <td>54.36</td>\n      <td>10.14</td>\n      <td>2017-04-03 07:54:00+00:00</td>\n      <td>54.38</td>\n      <td>18.66</td>\n      <td>2017-04-04 15:28:00+00:00</td>\n      <td>KIEL</td>\n      <td>GDYNIA</td>\n      <td>2017-04-04 13:54:00+00:00</td>\n      <td>...</td>\n      <td>89</td>\n      <td>13</td>\n      <td>4.00</td>\n      <td>54.51</td>\n      <td>18.76</td>\n      <td>7.2</td>\n      <td>221.2</td>\n      <td>215</td>\n      <td>PL.GDN</td>\n      <td>H7001</td>\n    </tr>\n    <tr>\n      <th>1060707</th>\n      <td>2204049</td>\n      <td>54.36</td>\n      <td>10.14</td>\n      <td>2017-04-03 07:54:00+00:00</td>\n      <td>54.38</td>\n      <td>18.66</td>\n      <td>2017-04-04 15:28:00+00:00</td>\n      <td>KIEL</td>\n      <td>GDYNIA</td>\n      <td>2017-04-04 13:53:00+00:00</td>\n      <td>...</td>\n      <td>89</td>\n      <td>13</td>\n      <td>4.00</td>\n      <td>54.51</td>\n      <td>18.76</td>\n      <td>7.2</td>\n      <td>221.0</td>\n      <td>214</td>\n      <td>PL.GDN</td>\n      <td>H7001</td>\n    </tr>\n  </tbody>\n</table>\n<p>913622 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:04:13.520156351Z",
     "start_time": "2025-06-06T16:04:12.089468925Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "df.to_parquet(output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T16:04:14.315093485Z",
     "start_time": "2025-06-06T16:04:13.516481458Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
