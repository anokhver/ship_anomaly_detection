{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from data_cleaning.utils.normalization_utils import (\n",
    "    clean_destination\n",
    ")\n",
    "from tqdm.notebook import tqdm # For progress bars in notebook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Remark\n",
    "Destinations are being processed so only official ports are left and nothing else\n",
    "##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "file_path = 'data/1_merged_typed_data.parquet'\n",
    "output_path = 'data/2_destination_norm.parquet'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "\n",
    "df = pd.read_parquet(file_path)\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## --- Step 1\n",
    "Deal with Inconsistent Records (We can have different formats representing the same thing) if they are there.\n",
    "And convert columns to categorical where appropriate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_unique_values(df):\n",
    "    \"\"\"Check unique values in each column of the DataFrame.\"\"\"\n",
    "    col_un = {}\n",
    "    for col in df.columns:\n",
    "        clean_series = df[col].dropna()\n",
    "        nunique = clean_series.nunique()\n",
    "        col_un[col] = nunique\n",
    "    return col_un\n",
    "\n",
    "unique_values_before = check_unique_values(df)\n",
    "unique_values_before"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Column: StartPort, Unique values: 2 - seems correct\n",
    "Column: EndPort, Unique values: 2 - seems correct\n",
    "\n",
    "Column: Destination, Unique values: 139 - weird"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Case normalization\n",
    "text_columns = df.select_dtypes(include=['string']).columns\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].str.upper()  # Ensure string type and uppercase\n",
    "\n",
    "unique_values_after = check_unique_values(df)\n",
    "\n",
    "changed_columns = list(filter(lambda col: unique_values_after[col] != unique_values_before[col], df.columns))\n",
    "df[changed_columns].dropna().nunique() # Check how many unique values are there after case normalization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Case normalization has changed the unique values in the following columns: Destination\n",
    "It seems there are still inconsistencies in the Destination column, so we will need to clean it further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Destination'].unique() #See some examples of the Destination column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ensure 'Destination' has at least one alphabetic character and is not just a country code\n",
    "df['Destination'] = df['Destination'].apply(\n",
    "    lambda x: pd.NA if not re.search(r'[A-Za-z]', str(x)) or re.match(r'^[A-Z]{2}$', str(x)) else x\n",
    ")\n",
    "\n",
    "def find_values_with_special_chars(df):\n",
    "    \"\"\"Find values with special characters in the 'Destination' column.\"\"\"\n",
    "    return [\n",
    "        value for value in df['Destination'].unique()\n",
    "        if re.search(r'[^A-Za-z0-9]', str(value))\n",
    "    ]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dest_before = find_values_with_special_chars(df)\n",
    "print(len(dest_before), \"unique values before cleaning with special characters\")\n",
    "dest_before"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## --- Step 2\n",
    "We can see that we have different formats representing the same thing, like 'HAMBURG' and 'DEHAM' ext.\n",
    "The data is incredibly messy, we need to handle country codes, special characters, and different formats.\n",
    "\n",
    "1. Some have country codes\n",
    "2. Some contain starting port too\n",
    "3. Some contain type of facilities (e.g., 'ELBE.RC', 'BREMERHAVEN.VIA.NOK')\n",
    "\n",
    "start > destination\n",
    "##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clean all data\n",
    "df['Destination'] = df['Destination'].progress_apply(clean_destination)\n",
    "df['Destination'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create mask for rows containing '>'\n",
    "mask = df['Destination'].str.contains('>', na=False)\n",
    "\n",
    "# Initialize columns (if not already done)\n",
    "# df['start_fr_dest'] = None\n",
    "# df['cleaned_destination'] = df['Destination'].copy()\n",
    "# Split and assign values safely\n",
    "# df.loc[mask, 'start_fr_dest'] = df.loc[mask, 'Destination'].str.split('>').str[0] #NOTE for now we wont bother\n",
    "\n",
    "df.loc[mask, 'Destination'] = df.loc[mask, 'Destination'].str.split('>').str[1]\n",
    "df['Destination'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **I CREATED CUSTOM FILE WITH FUNCTION WE WILL USE DOWN**\n",
    "\n",
    "df_recombined['Destination'].unique()\n",
    "Now we have somewhat cleaned Destination column, but we still have some inconsistencies. That the same ports have different names.\n",
    "I didn't find quicker way to do it, rather than manually checking the names and creating a list of names that represent the same port.\n",
    "We will only a bit automize this process, by using fuzzy matching to find similar names.\n",
    "And extracting all ports from UpdatedPub150.csv file, which contains ports and their countries.\n",
    "[link](https://msi.nga.mil/Publications/WPI)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from fuzzywuzzy import fuzz\n",
    "#\n",
    "# def find_fuzzy_matches(destinations, threshold=80, scorer=fuzz.token_set_ratio, show_progress=False):\n",
    "#     \"\"\"\n",
    "#     Find fuzzy matches among destination names.\n",
    "#\n",
    "#     Parameters:\n",
    "#         destinations (list): List of destination strings to compare\n",
    "#         threshold (int): Minimum similarity score to consider a match (0-100)\n",
    "#         scorer: Fuzzy matching function (default: token_set_ratio)\n",
    "#         show_progress (bool): Whether to print progress during processing\n",
    "#\n",
    "#     Returns:\n",
    "#         dict: Dictionary where keys are original names and values are lists of matches\n",
    "#               with their scores in format [(matched_name, score), ...]\n",
    "#     \"\"\"\n",
    "#     matches = {}\n",
    "#     total = len(destinations)\n",
    "#\n",
    "#     for i, dest in enumerate(destinations, 1):\n",
    "#         # Skip NAN/empty values\n",
    "#         if not dest or str(dest).strip().upper() in ('NAN', 'NULL', ''):\n",
    "#             continue\n",
    "#\n",
    "#         if show_progress:\n",
    "#             print(f\"Processing {i}/{total}: {dest[:30]}...\", end='\\r')\n",
    "#\n",
    "#         # Find matches above threshold (excluding self)\n",
    "#         potential_matches = process.extract(\n",
    "#             dest,\n",
    "#             destinations,\n",
    "#             scorer=scorer,\n",
    "#             limit=None\n",
    "#         )\n",
    "#\n",
    "#         # Filter matches\n",
    "#         good_matches = [\n",
    "#             (match, score)\n",
    "#             for match, score in potential_matches\n",
    "#             if score >= threshold and match != dest\n",
    "#         ]\n",
    "#\n",
    "#         if good_matches:\n",
    "#             matches[dest] = good_matches\n",
    "#\n",
    "#     if show_progress:\n",
    "#         print(\"\\n\" + \"=\" * 50)\n",
    "#\n",
    "#     return matches\n",
    "# def print_fuzzy_matches(matches, min_score=0, group_similar=False):\n",
    "#     \"\"\"\n",
    "#     Print fuzzy matching results in a readable format.\n",
    "#\n",
    "#     Parameters:\n",
    "#         matches (dict): Output from find_fuzzy_matches\n",
    "#         min_score (int): Minimum score to display\n",
    "#         group_similar (bool): Whether to group similar matches together\n",
    "#     \"\"\"\n",
    "#     if not matches:\n",
    "#         print(\"No matches found\")\n",
    "#         return\n",
    "#\n",
    "#     print(f\"\\nFuzzy matches (score ≥ {min_score}):\")\n",
    "#     print(\"=\" * 60)\n",
    "#\n",
    "#     if group_similar:\n",
    "#         # Group similar matches to avoid duplicates\n",
    "#         already_matched = set()\n",
    "#         for dest in sorted(matches.keys()):\n",
    "#             if dest in already_matched:\n",
    "#                 continue\n",
    "#\n",
    "#             print(f\"\\nGroup: {dest}\")\n",
    "#             print(\"-\" * 50)\n",
    "#\n",
    "#             # Include the original in the group\n",
    "#             all_in_group = {dest}\n",
    "#\n",
    "#             for match, score in matches[dest]:\n",
    "#                 if score >= min_score:\n",
    "#                     print(f\"  → {match} (score: {score})\")\n",
    "#                     all_in_group.add(match)\n",
    "#\n",
    "#                     # Also include matches of matches\n",
    "#                     if match in matches:\n",
    "#                         for submatch, subscore in matches[match]:\n",
    "#                             if subscore >= min_score and submatch not in all_in_group:\n",
    "#                                 print(f\"    → {submatch} (score: {subscore})\")\n",
    "#                                 all_in_group.add(submatch)\n",
    "#\n",
    "#             already_matched.update(all_in_group)\n",
    "#     else:\n",
    "#         # Simple listing\n",
    "#         for dest in sorted(matches.keys()):\n",
    "#             print(f\"\\n{dest} matches:\")\n",
    "#             print(\"-\" * 50)\n",
    "#             for match, score in matches[dest]:\n",
    "#                 if score >= min_score:\n",
    "#                     print(f\"  → {match} (score: {score})\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# unique_dests = df['Destination'].unique().tolist()\n",
    "#\n",
    "# # Find matches with threshold of 85\n",
    "# matches = find_fuzzy_matches(unique_dests, threshold=75, show_progress=True)\n",
    "#\n",
    "# # Print results grouped by similarity\n",
    "# print_fuzzy_matches(matches, min_score=75, group_similar=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "From this I will manually create a list of names that represent the same port, that do not have 100 match.\n",
    "As they can be incorrectly matched, and it would be better to do it manually.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from data_cleaning.utils.normalization_utils import match_names\n",
    "df['Destination'] = df['Destination'].progress_apply(lambda x: match_names(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = replace_with_key(df, 'start_fr_dest', full_dict)\n",
    "df[['Destination']].reset_index().drop_duplicates(subset=['Destination'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df[['start_fr_dest']].reset_index().drop_duplicates(subset=['start_fr_dest'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "len(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_parquet(output_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
