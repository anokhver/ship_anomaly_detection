{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from data_cleaning.utils.normalization_utils import (\n",
    "    clean_destination\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:29:52.601220191Z",
     "start_time": "2025-06-15T14:29:52.589060865Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Remark\n",
    "Destinations are being processed so only official ports are left and nothing else\n",
    "##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1060708 entries, 225260 to 954242\n",
      "Data columns (total 20 columns):\n",
      " #   Column          Non-Null Count    Dtype              \n",
      "---  ------          --------------    -----              \n",
      " 0   TripID          1060708 non-null  int64              \n",
      " 1   StartLatitude   1060708 non-null  float64            \n",
      " 2   StartLongitude  1060708 non-null  float64            \n",
      " 3   StartTime       1060708 non-null  datetime64[ns, UTC]\n",
      " 4   EndLatitude     1060708 non-null  float64            \n",
      " 5   EndLongitude    1060708 non-null  float64            \n",
      " 6   EndTime         1060708 non-null  datetime64[ns, UTC]\n",
      " 7   StartPort       1060708 non-null  string             \n",
      " 8   EndPort         1060708 non-null  string             \n",
      " 9   time            1060708 non-null  datetime64[ns, UTC]\n",
      " 10  shiptype        1060708 non-null  int64              \n",
      " 11  Length          1060708 non-null  int64              \n",
      " 12  Breadth         1060708 non-null  int64              \n",
      " 13  Draught         1044438 non-null  float64            \n",
      " 14  Latitude        1060708 non-null  float64            \n",
      " 15  Longitude       1060708 non-null  float64            \n",
      " 16  SOG             1060708 non-null  float64            \n",
      " 17  COG             1060708 non-null  float64            \n",
      " 18  TH              1060708 non-null  int64              \n",
      " 19  Destination     1054810 non-null  string             \n",
      "dtypes: datetime64[ns, UTC](3), float64(9), int64(5), string(3)\n",
      "memory usage: 169.9 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = '../data/1_merged_typed_data.parquet'\n",
    "output_path = '../data/2_destination_norm.parquet'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "\n",
    "df = pd.read_parquet(file_path)\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:29:53.738830121Z",
     "start_time": "2025-06-15T14:29:53.274402085Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## --- Step 1\n",
    "Deal with Inconsistent Records (We can have different formats representing the same thing) if they are there.\n",
    "And convert columns to categorical where appropriate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'TripID': 1126,\n 'StartLatitude': 28,\n 'StartLongitude': 34,\n 'StartTime': 953,\n 'EndLatitude': 29,\n 'EndLongitude': 47,\n 'EndTime': 943,\n 'StartPort': 2,\n 'EndPort': 2,\n 'time': 414193,\n 'shiptype': 11,\n 'Length': 107,\n 'Breadth': 36,\n 'Draught': 238,\n 'Latitude': 273,\n 'Longitude': 1285,\n 'SOG': 227,\n 'COG': 3602,\n 'TH': 361,\n 'Destination': 139}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_unique_values(df):\n",
    "    \"\"\"Check unique values in each column of the DataFrame.\"\"\"\n",
    "    col_un = {}\n",
    "    for col in df.columns:\n",
    "        clean_series = df[col].dropna()\n",
    "        nunique = clean_series.nunique()\n",
    "        col_un[col] = nunique\n",
    "    return col_un\n",
    "\n",
    "unique_values_before = check_unique_values(df)\n",
    "unique_values_before"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:29:54.440823950Z",
     "start_time": "2025-06-15T14:29:53.901500158Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Column: StartPort, Unique values: 2 - seems correct\n",
    "Column: EndPort, Unique values: 2 - seems correct\n",
    "\n",
    "Column: Destination, Unique values: 139 - weird"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Destination    138\ndtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case normalization\n",
    "text_columns = df.select_dtypes(include=['string']).columns\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].str.upper()  # Ensure string type and uppercase\n",
    "\n",
    "unique_values_after = check_unique_values(df)\n",
    "\n",
    "changed_columns = list(filter(lambda col: unique_values_after[col] != unique_values_before[col], df.columns))\n",
    "df[changed_columns].dropna().nunique() # Check how many unique values are there after case normalization"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:29:55.813048510Z",
     "start_time": "2025-06-15T14:29:54.533556022Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Case normalization has changed the unique values in the following columns: Destination\n",
    "It seems there are still inconsistencies in the Destination column, so we will need to clean it further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<StringArray>\n[          'DEHAM', 'DEBRV---->DEHAM',         'HAMBURG',      'DEHAM.ELBE',\n  'DEHAM.ELBE.PLT',     'BREMENHAVEN', 'HAMBURG/.AIRBUS',           'STADE',\n 'DE.WVN.>.DE.HAM',       'DEHAM.CTA',\n ...\n           'HH.PS',          'HH.CTB',        'HALMSTAD',              'HH',\n      'FINKENWERD',          'GDANKS',       'NORDENHAM',     'GDANSK...AS',\n     'KALININGRAD',    'BRUNSBUETTEL']\nLength: 139, dtype: string"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Destination'].unique() #See some examples of the Destination column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:29:55.877232818Z",
     "start_time": "2025-06-15T14:29:55.874076613Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Ensure 'Destination' has at least one alphabetic character and is not just a country code\n",
    "df['Destination'] = df['Destination'].apply(\n",
    "    lambda x: pd.NA if not re.search(r'[A-Za-z]', str(x)) or re.match(r'^[A-Z]{2}$', str(x)) else x\n",
    ")\n",
    "\n",
    "def find_values_with_special_chars(df):\n",
    "    \"\"\"Find values with special characters in the 'Destination' column.\"\"\"\n",
    "    return [\n",
    "        value for value in df['Destination'].unique()\n",
    "        if re.search(r'[^A-Za-z0-9]', str(value))\n",
    "    ]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:29:57.511254697Z",
     "start_time": "2025-06-15T14:29:55.878352704Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 unique values before cleaning with special characters\n"
     ]
    },
    {
     "data": {
      "text/plain": "['DEBRV---->DEHAM',\n 'DEHAM.ELBE',\n 'DEHAM.ELBE.PLT',\n 'HAMBURG/.AIRBUS',\n 'DE.WVN.>.DE.HAM',\n 'DEHAM.CTA',\n 'DEBRV>DEHAM',\n 'DE.HAM',\n 'DEBHV.NOK',\n 'DEHAM.EGH',\n 'HAMBURG..DE',\n 'DEHAM.CTT',\n 'DE.BRV>DEHAM',\n 'DE.BRV>DE.HAM',\n 'HAMBURG/EUR',\n 'DEBRV.>.DEHAM',\n 'DEHAM.EG',\n <NA>,\n 'KLAIPEDA.VIA.NOK',\n 'PLGDY.VIA.NOK',\n 'GDYNIA.POLAND',\n 'GDYNIA.VIA.NOK',\n 'GDANSKVIA.NOK',\n 'PLGDN.VIA.NOK',\n 'DEHAM.>.PLGDY',\n 'GDANSK.VIANOK',\n 'BLEXEN.ROAD',\n 'GDANSK.VIA.NOK',\n 'GDYNIA.VIA.NOK.:)',\n 'GDYNIA.:)',\n 'GDYNIAVIA)NOK',\n 'KLJ.VIA.NOK',\n 'BE.ANR.>>.PL.GDN',\n 'KIEL.CANAL-TRANS',\n 'GDYNIA-PLGDY',\n 'KLAIPEDA..=SWIN',\n 'GDYNIA/N.O.K',\n 'PL.GDN',\n 'GDANSK....=SWIN',\n 'SEAHU.>.DEBRV',\n 'DEHAM.>.PLGDY.NOK',\n 'DEHAM.>.PL',\n 'GDYNIA.VIA.K.CANAL',\n 'GDYNIA.VICINOUJSCIE',\n 'GDANSK.PILOT',\n 'DE.HAM.............',\n 'GDYNIA..VIA.NOK',\n 'DEBRT.>.DEHAM',\n 'DEBRV.EGH',\n 'GDANSK.VIA.KIEL.K',\n 'GDYNIA...TH',\n 'GDANSK.VICEL',\n 'GDANSK...!',\n 'GDANSK...#INDFARM',\n 'GDANSK....KCHIODE',\n 'PL.GDY.VIA.NOK',\n 'PL.GDY',\n 'ELBE.RC',\n 'GDYNIA....>.BALTIC.2',\n 'GDYNIA....Y',\n 'GDYNIA...!E',\n 'GDYNIA....ROJECT',\n 'HH.FINKENWERDER',\n 'GDANSKCIKINGER.WF',\n 'GDYNIA.VI',\n 'GDYNIA...!RSBURG',\n 'GDYNIA...!RATION',\n 'GDYNIA....>.HANKO',\n 'GDYNIA...DING.AREA',\n 'GDANSK....NOK',\n 'BREMERHAVEN.VIA.NOK',\n 'KIEL.CANAL',\n 'GDANSK...!A.KIEL',\n 'GDANSK...',\n 'GDYNIA>HIDDENSEE',\n 'GDYNIA...!STELLE',\n 'GDYNIA.VIA',\n 'GDYNIA.VIBIA.NOK',\n 'GDYNIA.VIAE',\n \"'GDYNIA.VIK?0\\\\\\\\BPO?_'\",\n 'GDYNIA.VIE',\n 'GDANSK./.POLAND',\n 'GDYNIA.VIA.KIEL',\n 'GDANSKVIA.NOK)',\n 'LT.KLAIPEDA',\n 'HAMBURG.DE',\n 'KLAIPEDA.VIA.NOC',\n 'HAMBURG.:)',\n 'DE.HAMBIVER.ELBE',\n 'ELBE.PS',\n 'HHLO.PS',\n 'HAMBURG???',\n 'BRV.PS',\n 'HAMBURG....',\n 'HAM.PS',\n 'DEBRV.>.DEIM',\n 'GDYNIA.PL',\n 'HH.PS',\n 'HH.CTB',\n 'GDANSK...AS']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_before = find_values_with_special_chars(df)\n",
    "print(len(dest_before), \"unique values before cleaning with special characters\")\n",
    "dest_before"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:29:57.546917618Z",
     "start_time": "2025-06-15T14:29:57.540703147Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## --- Step 2\n",
    "We can see that we have different formats representing the same thing, like 'HAMBURG' and 'DEHAM' ext.\n",
    "The data is incredibly messy, we need to handle country codes, special characters, and different formats.\n",
    "\n",
    "1. Some have country codes\n",
    "2. Some contain starting port too\n",
    "3. Some contain type of facilities (e.g., 'ELBE.RC', 'BREMERHAVEN.VIA.NOK')\n",
    "\n",
    "start > destination\n",
    "##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['DEHAM', 'DEBRV.>DEHAM', 'HAMBURG', 'DEHAM.ELBE', 'DEHAM.ELBE.PLT',\n       'BREMENHAVEN', 'HAMBURG.AIRBUS', 'STADE', 'DE.WVN.>.DE.HAM',\n       'DEHAM.CTA', 'DEBRV>DEHAM', 'FINKENWERDER', 'DE.HAM', 'DEBRV',\n       'DEBHV.NOK', 'DEHAM.EGH', 'HAMBURG.DE', 'DEHAM.CTT',\n       'DE.BRV>DEHAM', 'BREMERHAVEN', 'DEBRE', 'DE.BRV>DE.HAM',\n       'HAMBURG.EUR', 'DEBRV.>.DEHAM', 'DEHAM.EG', None,\n       'KLAIPEDA.VIA.NOK', 'GDANSK', 'PLGDY.VIA.NOK', 'PLGDY', 'HAMBUG',\n       'GDYNIA.POLAND', 'GDYNIA.VIA.NOK', 'GDANSKVIA.NOK', 'GDYNIA',\n       'KLAIPEDA', 'PLGDN.VIA.NOK', 'PLGDN', 'DEHAM.>.PLGDY',\n       'GDANSK.VIANOK', 'GYDINIA', 'NOK', 'GYDNIA', 'DEBHV',\n       'BLEXEN.ROAD', 'GDANSK.VIA.NOK', 'GDYNIAVIANOK', 'KLJ.VIA.NOK',\n       'BE.ANR.>.PL.GDN', 'KIEL.CANAL.TRANS', 'GDYNIA.PLGDY',\n       'KLAIPEDA.SWIN', 'GDYNIA.N.O.K', 'GDYNI', 'PL.GDN', 'GDANSK.SWIN',\n       'SEAHU.>.DEBRV', 'DEHAM.>.PLGDY.NOK', 'DEHAM.>.PL', 'GDYNYA',\n       'GDYNIA.VIA.K.CANAL', 'GDYNIA.VICINOUJSCIE', 'GDANSK.PILOT',\n       'DEBRT.>.DEHAM', 'DEBRV.EGH', 'GDANSK.VIA.KIEL.K', 'DEHAMCTA',\n       'GDYNIA.TH', 'GDANSK.VICEL', 'GDANSK.INDFARM', 'GDANSK.KCHIODE',\n       'PL.GDY.VIA.NOK', 'PL.GDY', 'ELBE.RC', 'GDYNIA.>.BALTIC.2',\n       'GDYNIA.Y', 'GDYNIA.E', 'GDYNIA.ROJECT', 'HH.FINKENWERDER',\n       'GDANSKCIKINGER.WF', 'GDYNIA.VI', 'GDYNIA.RSBURG', 'GDYNIA.RATION',\n       'GDYNIA.>.HANKO', 'GDYNIA.DING.AREA', 'GDANSK.NOK',\n       'BREMERHAVEN.VIA.NOK', 'KIEL.CANAL', 'GDANSK.A.KIEL',\n       'GDANSKARATION', 'GDYNIAC', 'KLAIPEDACINDFARM', 'GDYNIASTRALSUND',\n       'GDYNIA>HIDDENSEE', 'GDYNIACRT', 'GDYNIA.STELLE', 'GDYNIA.VIA',\n       'GDYNIA.VIBIA.NOK', 'GDYNIA.VIAE', 'GDYNIA.VIK0.BPO', 'GDYNIA.VIE',\n       'GDANSK.POLAND', 'GDYNIA.VIA.KIEL', 'LT.KLAIPEDA', 'GDANK',\n       'KLAIPEDA.VIA.NOC', 'DE.HAMBIVER.ELBE', 'ELBE.PS', 'HHLO.PS',\n       'COPENHAGEN', 'BRV.PS', 'GDYNA', 'HAM.PS', 'DEBRV.>.DEIM',\n       'GDYNIA.PL', 'SZCZECIN', 'SEHAD', 'HH.PS', 'HH.CTB', 'HALMSTAD',\n       'FINKENWERD', 'GDANKS', 'NORDENHAM', 'GDANSK.AS', 'KALININGRAD',\n       'BRUNSBUETTEL'], dtype=object)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean all data\n",
    "df['Destination'] = df['Destination'].apply(clean_destination)\n",
    "df['Destination'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:30:07.355082028Z",
     "start_time": "2025-06-15T14:29:57.547170400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['DEHAM', 'HAMBURG', 'DEHAM.ELBE', 'DEHAM.ELBE.PLT', 'BREMENHAVEN',\n       'HAMBURG.AIRBUS', 'STADE', '.DE.HAM', 'DEHAM.CTA', 'FINKENWERDER',\n       'DE.HAM', 'DEBRV', 'DEBHV.NOK', 'DEHAM.EGH', 'HAMBURG.DE',\n       'DEHAM.CTT', 'BREMERHAVEN', 'DEBRE', 'HAMBURG.EUR', '.DEHAM',\n       'DEHAM.EG', None, 'KLAIPEDA.VIA.NOK', 'GDANSK', 'PLGDY.VIA.NOK',\n       'PLGDY', 'HAMBUG', 'GDYNIA.POLAND', 'GDYNIA.VIA.NOK',\n       'GDANSKVIA.NOK', 'GDYNIA', 'KLAIPEDA', 'PLGDN.VIA.NOK', 'PLGDN',\n       '.PLGDY', 'GDANSK.VIANOK', 'GYDINIA', 'NOK', 'GYDNIA', 'DEBHV',\n       'BLEXEN.ROAD', 'GDANSK.VIA.NOK', 'GDYNIAVIANOK', 'KLJ.VIA.NOK',\n       '.PL.GDN', 'KIEL.CANAL.TRANS', 'GDYNIA.PLGDY', 'KLAIPEDA.SWIN',\n       'GDYNIA.N.O.K', 'GDYNI', 'PL.GDN', 'GDANSK.SWIN', '.DEBRV',\n       '.PLGDY.NOK', '.PL', 'GDYNYA', 'GDYNIA.VIA.K.CANAL',\n       'GDYNIA.VICINOUJSCIE', 'GDANSK.PILOT', 'DEBRV.EGH',\n       'GDANSK.VIA.KIEL.K', 'DEHAMCTA', 'GDYNIA.TH', 'GDANSK.VICEL',\n       'GDANSK.INDFARM', 'GDANSK.KCHIODE', 'PL.GDY.VIA.NOK', 'PL.GDY',\n       'ELBE.RC', '.BALTIC.2', 'GDYNIA.Y', 'GDYNIA.E', 'GDYNIA.ROJECT',\n       'HH.FINKENWERDER', 'GDANSKCIKINGER.WF', 'GDYNIA.VI',\n       'GDYNIA.RSBURG', 'GDYNIA.RATION', '.HANKO', 'GDYNIA.DING.AREA',\n       'GDANSK.NOK', 'BREMERHAVEN.VIA.NOK', 'KIEL.CANAL', 'GDANSK.A.KIEL',\n       'GDANSKARATION', 'GDYNIAC', 'KLAIPEDACINDFARM', 'GDYNIASTRALSUND',\n       'HIDDENSEE', 'GDYNIACRT', 'GDYNIA.STELLE', 'GDYNIA.VIA',\n       'GDYNIA.VIBIA.NOK', 'GDYNIA.VIAE', 'GDYNIA.VIK0.BPO', 'GDYNIA.VIE',\n       'GDANSK.POLAND', 'GDYNIA.VIA.KIEL', 'LT.KLAIPEDA', 'GDANK',\n       'KLAIPEDA.VIA.NOC', 'DE.HAMBIVER.ELBE', 'ELBE.PS', 'HHLO.PS',\n       'COPENHAGEN', 'BRV.PS', 'GDYNA', 'HAM.PS', '.DEIM', 'GDYNIA.PL',\n       'SZCZECIN', 'SEHAD', 'HH.PS', 'HH.CTB', 'HALMSTAD', 'FINKENWERD',\n       'GDANKS', 'NORDENHAM', 'GDANSK.AS', 'KALININGRAD', 'BRUNSBUETTEL'],\n      dtype=object)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mask for rows containing '>'\n",
    "mask = df['Destination'].str.contains('>', na=False)\n",
    "\n",
    "# Initialize columns (if not already done)\n",
    "# df['start_fr_dest'] = None\n",
    "# df['cleaned_destination'] = df['Destination'].copy()\n",
    "# Split and assign values safely\n",
    "# df.loc[mask, 'start_fr_dest'] = df.loc[mask, 'Destination'].str.split('>').str[0] #NOTE for now we wont bother\n",
    "\n",
    "df.loc[mask, 'Destination'] = df.loc[mask, 'Destination'].str.split('>').str[1]\n",
    "df['Destination'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:30:07.723418535Z",
     "start_time": "2025-06-15T14:30:07.394875237Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **I CREATED CUSTOM FILE WITH FUNCTION WE WILL USE DOWN**\n",
    "\n",
    "df_recombined['Destination'].unique()\n",
    "Now we have somewhat cleaned Destination column, but we still have some inconsistencies. That the same ports have different names.\n",
    "I didn't find quicker way to do it, rather than manually checking the names and creating a list of names that represent the same port.\n",
    "We will only a bit automize this process, by using fuzzy matching to find similar names.\n",
    "And extracting all ports from UpdatedPub150.csv file, which contains ports and their countries.\n",
    "[link](https://msi.nga.mil/Publications/WPI)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# from fuzzywuzzy import fuzz\n",
    "#\n",
    "# def find_fuzzy_matches(destinations, threshold=80, scorer=fuzz.token_set_ratio, show_progress=False):\n",
    "#     \"\"\"\n",
    "#     Find fuzzy matches among destination names.\n",
    "#\n",
    "#     Parameters:\n",
    "#         destinations (list): List of destination strings to compare\n",
    "#         threshold (int): Minimum similarity score to consider a match (0-100)\n",
    "#         scorer: Fuzzy matching function (default: token_set_ratio)\n",
    "#         show_progress (bool): Whether to print progress during processing\n",
    "#\n",
    "#     Returns:\n",
    "#         dict: Dictionary where keys are original names and values are lists of matches\n",
    "#               with their scores in format [(matched_name, score), ...]\n",
    "#     \"\"\"\n",
    "#     matches = {}\n",
    "#     total = len(destinations)\n",
    "#\n",
    "#     for i, dest in enumerate(destinations, 1):\n",
    "#         # Skip NAN/empty values\n",
    "#         if not dest or str(dest).strip().upper() in ('NAN', 'NULL', ''):\n",
    "#             continue\n",
    "#\n",
    "#         if show_progress:\n",
    "#             print(f\"Processing {i}/{total}: {dest[:30]}...\", end='\\r')\n",
    "#\n",
    "#         # Find matches above threshold (excluding self)\n",
    "#         potential_matches = process.extract(\n",
    "#             dest,\n",
    "#             destinations,\n",
    "#             scorer=scorer,\n",
    "#             limit=None\n",
    "#         )\n",
    "#\n",
    "#         # Filter matches\n",
    "#         good_matches = [\n",
    "#             (match, score)\n",
    "#             for match, score in potential_matches\n",
    "#             if score >= threshold and match != dest\n",
    "#         ]\n",
    "#\n",
    "#         if good_matches:\n",
    "#             matches[dest] = good_matches\n",
    "#\n",
    "#     if show_progress:\n",
    "#         print(\"\\n\" + \"=\" * 50)\n",
    "#\n",
    "#     return matches\n",
    "# def print_fuzzy_matches(matches, min_score=0, group_similar=False):\n",
    "#     \"\"\"\n",
    "#     Print fuzzy matching results in a readable format.\n",
    "#\n",
    "#     Parameters:\n",
    "#         matches (dict): Output from find_fuzzy_matches\n",
    "#         min_score (int): Minimum score to display\n",
    "#         group_similar (bool): Whether to group similar matches together\n",
    "#     \"\"\"\n",
    "#     if not matches:\n",
    "#         print(\"No matches found\")\n",
    "#         return\n",
    "#\n",
    "#     print(f\"\\nFuzzy matches (score ≥ {min_score}):\")\n",
    "#     print(\"=\" * 60)\n",
    "#\n",
    "#     if group_similar:\n",
    "#         # Group similar matches to avoid duplicates\n",
    "#         already_matched = set()\n",
    "#         for dest in sorted(matches.keys()):\n",
    "#             if dest in already_matched:\n",
    "#                 continue\n",
    "#\n",
    "#             print(f\"\\nGroup: {dest}\")\n",
    "#             print(\"-\" * 50)\n",
    "#\n",
    "#             # Include the original in the group\n",
    "#             all_in_group = {dest}\n",
    "#\n",
    "#             for match, score in matches[dest]:\n",
    "#                 if score >= min_score:\n",
    "#                     print(f\"  → {match} (score: {score})\")\n",
    "#                     all_in_group.add(match)\n",
    "#\n",
    "#                     # Also include matches of matches\n",
    "#                     if match in matches:\n",
    "#                         for submatch, subscore in matches[match]:\n",
    "#                             if subscore >= min_score and submatch not in all_in_group:\n",
    "#                                 print(f\"    → {submatch} (score: {subscore})\")\n",
    "#                                 all_in_group.add(submatch)\n",
    "#\n",
    "#             already_matched.update(all_in_group)\n",
    "#     else:\n",
    "#         # Simple listing\n",
    "#         for dest in sorted(matches.keys()):\n",
    "#             print(f\"\\n{dest} matches:\")\n",
    "#             print(\"-\" * 50)\n",
    "#             for match, score in matches[dest]:\n",
    "#                 if score >= min_score:\n",
    "#                     print(f\"  → {match} (score: {score})\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:30:07.728583151Z",
     "start_time": "2025-06-15T14:30:07.726993201Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# unique_dests = df['Destination'].unique().tolist()\n",
    "#\n",
    "# # Find matches with threshold of 85\n",
    "# matches = find_fuzzy_matches(unique_dests, threshold=75, show_progress=True)\n",
    "#\n",
    "# # Print results grouped by similarity\n",
    "# print_fuzzy_matches(matches, min_score=75, group_similar=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:30:07.739909036Z",
     "start_time": "2025-06-15T14:30:07.730331670Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "From this I will manually create a list of names that represent the same port, that do not have 100 match.\n",
    "As they can be incorrectly matched, and it would be better to do it manually.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from data_cleaning.utils.normalization_utils import match_names\n",
    "\n",
    "def replace_with_key(df, column):\n",
    "    df[column] = df[column].apply(lambda x: match_names(x))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:30:07.754137637Z",
     "start_time": "2025-06-15T14:30:07.741229905Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = replace_with_key(df, 'Destination')\n",
    "# df = replace_with_key(df, 'start_fr_dest', full_dict)\n",
    "df[['Destination']].reset_index().drop_duplicates(subset=['Destination'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-06-15T14:30:07.747484763Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df[['start_fr_dest']].reset_index().drop_duplicates(subset=['start_fr_dest'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_parquet(output_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
