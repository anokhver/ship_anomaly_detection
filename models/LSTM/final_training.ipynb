{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.11.2)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/workspace/venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import precision_score, f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of a Long Short-Term Memory (LSTM) model for time-series forecasting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 num_layers,\n",
    "                 output_size,\n",
    "                 dropout=0):\n",
    "\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Encode\n",
    "        _, (hidden, cell) = self.encoder(x)\n",
    "\n",
    "        # Prepare decoder input: repeat the encoded representation for each timestep\n",
    "        # Use the last hidden state as the encoded representation\n",
    "        encoded = hidden[-1].unsqueeze(1).repeat(1, seq_len, 1)  # Shape: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Decode: Reconstruct the sequence\n",
    "        decoder_output, _ = self.decoder(encoded, (hidden, cell))\n",
    "\n",
    "        # Reconstruct\n",
    "        reconstructed = self.output_layer(decoder_output)\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "    def get_reconstruction_error(self, x):\n",
    "        \"\"\"Calculate reconstruction error for anomaly detection\"\"\"\n",
    "        with torch.no_grad():\n",
    "            reconstructed = self.forward(x)\n",
    "            # Calculate MSE for each sequence\n",
    "            mse = torch.mean((x - reconstructed) ** 2, dim=(1, 2)) #TODO check proper loss\n",
    "            return mse.cpu().numpy()\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Get the encoded representation of input sequences\"\"\"\n",
    "        with torch.no_grad():\n",
    "            _, (hidden, _) = self.encoder(x)\n",
    "            return hidden[-1]  # Return the last layer's hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"LSTM_preprocessed.parquet\"\n",
    "\n",
    "OUTPUT_DIR_AE = \"models_per_route_lstm_ae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "FEATURE_COLUMNS = [ \"speed_over_ground\", \"course_over_ground\",\n",
    "                    # \"longitude\", \"latitude\"\n",
    "                    \"x_km\", \"y_km\"\n",
    "                    ]\n",
    "\n",
    "SEQUENCE_LENGTH = 15\n",
    "SEQUENCE_STEP_LENGTH = 7\n",
    "\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SIZE = 0.2\n",
    "\n",
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "AUTOENCODER_THRESHOLD_PERCENTILE = 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Utils data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "    def get_valid_trips(self, df):\n",
    "        \"\"\"\n",
    "        Get trips that were actually labeled\n",
    "        \"\"\"\n",
    "\n",
    "        df = df[df[FEATURE_COLUMNS].notna().all(axis=1) & df[\"y_true\"].notna()]\n",
    "        df = df.sort_values(['trip_id', 'time_stamp'])\n",
    "\n",
    "        features_per_trip = df.groupby('trip_id')[FEATURE_COLUMNS].apply(lambda x: x.values.tolist()).reset_index()\n",
    "        features_per_trip.columns = [\"trip_id\", \"features\"]\n",
    "\n",
    "        labels_per_trip = df.groupby('trip_id')['y_true'].apply(list).reset_index()\n",
    "        df = features_per_trip.merge(labels_per_trip, on='trip_id')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def scale_sequences(self, sequences, fit=False):\n",
    "        print(f\"Scaling sequences with shape: {sequences.shape}\")\n",
    "        if fit:\n",
    "            self.scaler.fit(sequences.reshape(-1, sequences.shape[-1]))\n",
    "\n",
    "        transformed_sequences = self.scaler.transform(sequences.reshape(-1, sequences.shape[-1]))\n",
    "        return transformed_sequences.reshape(sequences.shape)\n",
    "\n",
    "    def create_sequences(self, df, seq_length):\n",
    "        xs, ys, trip_ids = [], [], []\n",
    "        for trip_id, group in df.groupby('trip_id'):\n",
    "            features = np.concat(group[\"features\"].values)\n",
    "            labels = np.concat(group[\"y_true\"].values)\n",
    "\n",
    "\n",
    "            for i in range(0, len(labels) - seq_length - 1, SEQUENCE_STEP_LENGTH):\n",
    "                seq_features = features[i:i+seq_length]\n",
    "                seq_labels = labels[i:i+seq_length]\n",
    "\n",
    "                if np.all(seq_labels > 0):\n",
    "                    seq_label = 1\n",
    "                else:\n",
    "                    seq_label = 0\n",
    "\n",
    "                # seq_label = int(np.sum(seq_labels) > (len(seq_labels) / 2))\n",
    "\n",
    "\n",
    "                xs.append(seq_features)\n",
    "                ys.append(seq_label)\n",
    "                trip_ids.append(trip_id)\n",
    "        return np.array(xs), np.array(ys), np.array(trip_ids)\n",
    "\n",
    "    def separate_sequences_by_anomaly_type(self, sequences, labels):\n",
    "        normal_sequences = sequences[labels == 0]\n",
    "        anomaly_sequences = sequences[labels == 1]\n",
    "        return normal_sequences, anomaly_sequences\n",
    "\n",
    "    def get_loader(self, sequences, labels, batch_size, shuffle=True):\n",
    "        sequences_tensor = torch.tensor(sequences, dtype=torch.float32)\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        dataset = TensorDataset(sequences_tensor, labels_tensor)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        df = self.get_valid_trips(df)\n",
    "        sequences, labels, trip_ids = self.create_sequences(df, SEQUENCE_LENGTH)\n",
    "        unique_trip_ids = np.unique(trip_ids)\n",
    "        train_trips, val_trips = train_test_split(unique_trip_ids, test_size=VALIDATION_SIZE, random_state=42)\n",
    "\n",
    "        train_X = sequences[np.isin(trip_ids, train_trips) & (labels == 0)]\n",
    "        train_y = labels[np.isin(trip_ids, train_trips) & (labels == 0)]\n",
    "\n",
    "        val_X = sequences[np.isin(trip_ids, val_trips) & (labels == 0)]\n",
    "        val_y = labels[np.isin(trip_ids, val_trips) & (labels == 0)]\n",
    "\n",
    "        anomaly_X = sequences[labels == 1]\n",
    "        anomaly_labels = labels[labels == 1]\n",
    "\n",
    "        train_X = self.scale_sequences(train_X, fit=True)\n",
    "        val_X = self.scale_sequences(val_X)\n",
    "        anomaly_X = self.scale_sequences(anomaly_X)\n",
    "\n",
    "        print(f\"Training normal sequences: {len(train_X)}\")\n",
    "        print(f\"Validation normal sequences: {len(val_X)}\")\n",
    "        print(f\"Anomaly sequences: {len(anomaly_X)}\")\n",
    "\n",
    "        train_loader = self.get_loader(train_X, train_y, BATCH_SIZE)\n",
    "        val_loader = self.get_loader(val_X, val_y, BATCH_SIZE, shuffle=False)\n",
    "        anomaly_loader = self.get_loader(anomaly_X, anomaly_labels, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, anomaly_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data_route(route, datapath=DATA_PATH):\n",
    "    \"\"\"\n",
    "    Load and prepare data for a specific route using new logic\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(datapath)\n",
    "    df.sort_values(['trip_id', 'time_stamp'], inplace=True)\n",
    "    df_route = df[df['start_port'] == route].copy()\n",
    "\n",
    "    print(f\"Loaded {len(df_route)} data points for route {route}\")\n",
    "    print(f\"Number of unique trips: {len(df_route['trip_id'].unique())}\")\n",
    "\n",
    "    return df_route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def verbous_metrics(val_labels, preds):\n",
    "    m = get_all_metrics(val_labels,preds)\n",
    "    verbose_metrics(m)\n",
    "\n",
    "def get_all_metrics(true_labels, predictions):\n",
    "\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "\n",
    "    tn = np.sum((true_labels == 0) & (predictions == 0))\n",
    "    fp = np.sum((true_labels == 0) & (predictions == 1))\n",
    "    fn = np.sum((true_labels == 1) & (predictions == 0))\n",
    "    tp = np.sum((true_labels == 1) & (predictions == 1))\n",
    "\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    mcc = matthews_corrcoef(true_labels, predictions)\n",
    "\n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'specificity': specificity,\n",
    "        'true_positives': tp,\n",
    "        'true_negatives': tn,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn,\n",
    "        'mcc': mcc,\n",
    "        'unlabeled_sequences': np.sum(true_labels == -1),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def verbose_metrics(metrics):\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        if (value != 0 and value != 1):\n",
    "            print(f\"{key.replace('_', ' ').title()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def validation(model,\n",
    "               val_loader,\n",
    "               loss,\n",
    "               device,\n",
    "               threshold = 0,\n",
    "               validate = True\n",
    "               ):\n",
    "\n",
    "    errors = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_X, val_y in val_loader:  # Only normal samples\n",
    "            val_X = val_X.to(device)\n",
    "            reconstructed = model(val_X)\n",
    "\n",
    "            # Save per-sequence max error\n",
    "            batch_errors = loss(reconstructed, val_X).amax(dim=(1, 2)).cpu().numpy()\n",
    "            errors.extend(batch_errors)\n",
    "            val_labels.extend(val_y.cpu().numpy())\n",
    "\n",
    "    if validate:\n",
    "        # Calculate threshold based on validation data\n",
    "        threshold = np.percentile(errors, AUTOENCODER_THRESHOLD_PERCENTILE)\n",
    "        print(f\"Validation threshold: {threshold:.6f}\")\n",
    "\n",
    "    preds = (np.array(errors) > threshold).astype(int)\n",
    "    verbous_metrics(val_labels,preds)\n",
    "  \n",
    "    return errors, preds, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "def train(model,\n",
    "          train_loader, val_loader, anomaly_loader,\n",
    "          optimizer,\n",
    "          loss,\n",
    "          device,\n",
    "          EPOCHS=20):\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss_history = []\n",
    "\n",
    "        # ----- Training Loop -----\n",
    "        model.train()\n",
    "        for batch_X, _ in tqdm(train_loader):\n",
    "            batch_X = batch_X.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            noise = torch.randn_like(batch_X) * 0.1 # NEW\n",
    "            noisy_input = batch_X + noise\n",
    "            \n",
    "            out = model(noisy_input)\n",
    "            batch_loss = loss(out, batch_X).mean()\n",
    "\n",
    "            loss_history.append(batch_loss.item())\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Training loss: {np.mean(loss_history):.6f}\")\n",
    "\n",
    "        # ----- Validation on NORMAL data (for threshold calculation) -----\n",
    "        model.eval()\n",
    "        print(\"---Val\")\n",
    "        normal_errors, _, threshold = validation(model, val_loader, loss, device)\n",
    "        print(\"---Anom\")\n",
    "        anomaly_errors, _, _ = validation(model, anomaly_loader, loss, device, threshold=threshold, validate=False)\n",
    "\n",
    "    return normal_errors, anomaly_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_errors(normal_errors, anomaly_errors):\n",
    "\n",
    "    plt.hist(normal_errors, bins=100, alpha=0.5, label=\"Normal\")\n",
    "    plt.hist(anomaly_errors, bins=100, alpha=0.5, label=\"Anomaly\")\n",
    "    # plt.axvline(thresh[best_idx], color=\"red\", linestyle=\"--\", label=\"Best Threshold\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Reconstruction Error Distribution (log scale)\")\n",
    "    plt.xlabel(\"Max Reconstruction Error\")\n",
    "    plt.ylabel(\"Log Count\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_boxplot(normal_errors, anomaly_errors):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    error_data = [normal_errors, anomaly_errors]\n",
    "    plt.boxplot(error_data, labels=['Normal', 'Anomaly'])\n",
    "    plt.ylabel('Reconstruction Error')\n",
    "    plt.title('Reconstruction Error Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Kiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_KIEL = load_data_route(\"KIEL\")\n",
    "dataset_KIEL = Dataset(df_KIEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_loader_K, val_loader_K, anomaly_loader_K = dataset_KIEL.preprocess(df_KIEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Bremenhaven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_BREM = load_data_route(\"BREMERHAVEN\")\n",
    "dataset_BREM = Dataset(df_BREM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_loader_B, val_loader_B, anomaly_loader_B = dataset_BREM.preprocess(df_BREM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KIEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T22:10:03.853049556Z",
     "start_time": "2025-07-05T22:10:03.625861748Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_K = LSTMModel(\n",
    "    input_size=len(FEATURE_COLUMNS),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    output_size=len(FEATURE_COLUMNS),\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "optimizer_K = torch.optim.AdamW(model_K.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "loss_K = nn.MSELoss(reduction='none') \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_K = model_K.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_errors_K, anomaly_errors_K = train(model_K,\n",
    "        train_loader_K, val_loader_K, anomaly_loader_K,\n",
    "          optimizer_K,\n",
    "          loss_K,\n",
    "          device,\n",
    "          EPOCHS=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_errors(normal_errors_K, anomaly_errors_K )\n",
    "plot_boxplot(normal_errors_K, anomaly_errors_K )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BREMERHAVEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = LSTMModel(\n",
    "    input_size=len(FEATURE_COLUMNS),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    output_size=len(FEATURE_COLUMNS),\n",
    "    dropout=0.4\n",
    ")\n",
    "\n",
    "optimizer_B = torch.optim.AdamW(model_B.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "loss_B = nn.MSELoss(reduction='none') \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_K = model_K.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_errors_B, anomaly_errors_B = train(model_B,\n",
    "        train_loader_B, val_loader_B, anomaly_loader_B,\n",
    "          optimizer_B,\n",
    "          loss_B,\n",
    "          device,\n",
    "          EPOCHS=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(normal_errors_B, anomaly_errors_B)\n",
    "plot_boxplot(normal_errors_B, anomaly_errors_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Save to file\n",
    "output_dir = Path(OUTPUT_DIR_AE)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def save_model(model, scaler, threshold, route_name):\n",
    "    # Move model to CPU before saving\n",
    "    model.cpu()\n",
    "    # Save everything needed for inference\n",
    "    lstm_artifacts = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"model_config\": {\n",
    "            \"input_size\": model.input_size,\n",
    "            \"hidden_size\": model.hidden_size, \n",
    "            \"num_layers\": model.num_layers,\n",
    "            \"dropout\": model.dropout\n",
    "        },\n",
    "        \"scaler\": scaler,\n",
    "        \"threshold\": threshold,\n",
    "        \"seq_step_length\": SEQUENCE_STEP_LENGTH,\n",
    "        \"sequence_length\": SEQUENCE_LENGTH,\n",
    "        \"features\": FEATURE_COLUMNS,\n",
    "        \"model_type\": \"lstm\"\n",
    "    }\n",
    "    \n",
    "\n",
    "    # Save for the specific route\n",
    "    model_filename = output_dir / f\"{route_name}_lstm_model.pkl\"\n",
    "    joblib.dump(lstm_artifacts, model_filename)\n",
    "\n",
    "    print(f\"LSTM model saved to {model_filename}\")\n",
    "    return model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model saved to models_per_route_lstm_ae/BREMERHAVEN_lstm_model.pkl\n",
      "LSTM model saved to models_per_route_lstm_ae/KIEL_lstm_model.pkl\n",
      "Dispatcher saved to models_per_route_lstm_ae/dispatcher.pkl\n"
     ]
    }
   ],
   "source": [
    "models_filenames = []\n",
    "dispatcher_lstm = {}\n",
    "\n",
    "threshold_B = np.percentile(normal_errors_B, AUTOENCODER_THRESHOLD_PERCENTILE)\n",
    "threshold_K = np.percentile(normal_errors_K, AUTOENCODER_THRESHOLD_PERCENTILE)\n",
    "scaler_K = dataset_KIEL.scaler\n",
    "scaler_B = dataset_BREM.scaler\n",
    "\n",
    "# Save BREMERHAVEN model\n",
    "model_filename_BREM = save_model(model_B, scaler_B, threshold_B, \"BREMERHAVEN\")\n",
    "models_filenames.append(model_filename_BREM)\n",
    "dispatcher_lstm[\"BREMERHAVEN\"] = str(model_filename_BREM)\n",
    "\n",
    "# Save KIEL model\n",
    "model_filename_KIEL = save_model(model_K, scaler_K, threshold_K, \"KIEL\")\n",
    "models_filenames.append(model_filename_KIEL)\n",
    "dispatcher_lstm[\"KIEL\"] = str(model_filename_KIEL)\n",
    "\n",
    "# Save dispatcher\n",
    "dispatcher_file = output_dir / \"dispatcher.pkl\"\n",
    "joblib.dump(dispatcher_lstm, dispatcher_file)\n",
    "print(f\"Dispatcher saved to {dispatcher_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
