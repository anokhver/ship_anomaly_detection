{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LSTM Vessel Trajectory Anomaly Detection Pipeline\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    average_precision_score, confusion_matrix, classification_report\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DROP_TRIPS = [10257]\n",
    "\n",
    "#Used features\n",
    "BASE_COLUMNS = [\n",
    "    \"speed_over_ground\", \"dv\", \"dcourse\", \"ddraft\",\n",
    "    \"zone\", \"x_km\", \"y_km\", \"dist_to_ref\", \"route_dummy\"\n",
    "]\n",
    "\n",
    "ZONES = [[53.8, 53.5, 8.6, 8.14], [53.66, 53.0, 11.0, 9.5]]\n",
    "R_PORT, R_APP = 5.0, 15.0\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "sklearn.random.seed(RANDOM_STATE)\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=0\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "───────────────────────────── Custom Data Generator ──────────────────────────────"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VesselSequenceGenerator(Sequence):\n",
    "    \"\"\"Custom data generator for LSTM training with vessel trajectory sequences.\"\"\"\n",
    "\n",
    "    def __init__(self, data, labels, sequence_length, batch_size, shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(data))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X = np.array([self.data[i] for i in batch_indices])\n",
    "        y = np.array([self.labels[i] for i in batch_indices])\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "───────────────────────────── Sequence Creation ──────────────────────────────"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_sequences_from_trips(df_route: pd.DataFrame, sequence_length: int,\n",
    "                               overlap_ratio: float = 0.5) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create sequences from trip data for LSTM training.\n",
    "    Returns sequences, labels, and trip_ids for each sequence.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    trip_ids = []\n",
    "\n",
    "    step_size = max(1, int(sequence_length * (1 - overlap_ratio)))\n",
    "\n",
    "    for trip_id, trip_data in tqdm(df_route.groupby('trip_id'), desc=\"Creating sequences\"):\n",
    "        trip_data = trip_data.sort_values('time_stamp').reset_index(drop=True)\n",
    "\n",
    "        # Get features and labels\n",
    "        features = trip_data[BASE_COLUMNS].fillna(0).values\n",
    "        trip_labels = trip_data['y_true'].values\n",
    "\n",
    "        # Create overlapping sequences\n",
    "        for start_idx in range(0, len(features) - sequence_length + 1, step_size):\n",
    "            end_idx = start_idx + sequence_length\n",
    "            seq_features = features[start_idx:end_idx]\n",
    "            seq_labels = trip_labels[start_idx:end_idx]\n",
    "\n",
    "            # Label sequence as anomalous if any point in sequence is anomalous\n",
    "            sequence_label = int(np.any(seq_labels == 1))\n",
    "\n",
    "            sequences.append(seq_features)\n",
    "            labels.append(sequence_label)\n",
    "            trip_ids.append(trip_id)\n",
    "\n",
    "    return np.array(sequences), np.array(labels), np.array(trip_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-───────────────────────────── LSTM Model Definitions ──────────────────────────────"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_lstm_autoencoder(input_shape: Tuple[int, int], lstm_units: int = 64,\n",
    "                          dense_units: int = 32, dropout_rate: float = 0.2) -> Model:\n",
    "    \"\"\"\n",
    "    Build LSTM Autoencoder for anomaly detection.\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    encoded = LSTM(lstm_units, return_sequences=True)(input_layer)\n",
    "    encoded = Dropout(dropout_rate)(encoded)\n",
    "    encoded = LSTM(lstm_units // 2, return_sequences=False)(encoded)\n",
    "    encoded = Dropout(dropout_rate)(encoded)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = Dense(dense_units, activation='relu')(encoded)\n",
    "    bottleneck = Dropout(dropout_rate)(bottleneck)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Dense(lstm_units // 2, activation='relu')(bottleneck)\n",
    "    decoded = Dropout(dropout_rate)(decoded)\n",
    "    decoded = tf.keras.layers.RepeatVector(input_shape[0])(decoded)\n",
    "    decoded = LSTM(lstm_units // 2, return_sequences=True)(decoded)\n",
    "    decoded = Dropout(dropout_rate)(decoded)\n",
    "    decoded = LSTM(lstm_units, return_sequences=True)(decoded)\n",
    "    decoded = TimeDistributed(Dense(input_shape[1]))(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    return autoencoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_lstm_classifier(input_shape: Tuple[int, int], lstm_units: int = 64,\n",
    "                         dense_units: int = 32, dropout_rate: float = 0.2) -> Model:\n",
    "    \"\"\"\n",
    "    Build LSTM Classifier for anomaly detection.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units // 2, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_lstm_autoencoder(X_train_normal: np.ndarray,\n",
    "                           X_val: np.ndarray,\n",
    "                           model_path: str,\n",
    "                           epochs: int) -> Tuple[Model, Dict]:\n",
    "    \"\"\"\n",
    "    Train LSTM Autoencoder on normal data only.\n",
    "    \"\"\"\n",
    "    input_shape = (X_train_normal.shape[1], X_train_normal.shape[2])\n",
    "    model = build_lstm_autoencoder(input_shape, LSTM_UNITS, DENSE_UNITS, DROPOUT_RATE)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=PATIENCE, restore_best_weights=True, monitor='val_loss'),\n",
    "        ModelCheckpoint(model_path.replace('.h5', '.keras'), save_best_only=True, monitor='val_loss'),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=PATIENCE//2, min_lr=1e-6, monitor='val_loss')\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_normal, X_train_normal,  # Autoencoder: input = output\n",
    "        validation_data=(X_val, X_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model, history.history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_lstm_classifier(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                         X_val: np.ndarray, y_val: np.ndarray,\n",
    "                         model_path: str,\n",
    "                         epochs: int) -> Tuple[Model, Dict]:\n",
    "    \"\"\"\n",
    "    Train LSTM Classifier for binary anomaly classification.\n",
    "    \"\"\"\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_lstm_classifier(input_shape, LSTM_UNITS, DENSE_UNITS, DROPOUT_RATE)\n",
    "\n",
    "    # Handle class imbalance\n",
    "    class_weight = {\n",
    "        0: len(y_train) / (2 * np.sum(y_train == 0)),\n",
    "        1: len(y_train) / (2 * np.sum(y_train == 1))\n",
    "    }\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=PATIENCE, restore_best_weights=True, monitor='val_loss'),\n",
    "        ModelCheckpoint(model_path.replace('.h5', '.keras'), save_best_only=True, monitor='val_loss'),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=PATIENCE//2, min_lr=1e-6, monitor='val_loss')\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model, history.history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test: np.ndarray, y_test: np.ndarray,\n",
    "                   model_type: str = 'classifier',\n",
    "                   threshold = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate anomaly detection model (autoencoder or classifier).\n",
    "\n",
    "    Args:\n",
    "        model: trained model\n",
    "        X_test: input test data\n",
    "        y_test: true labels (0 = normal, 1 = anomaly)\n",
    "        model_type: 'autoencoder' or 'classifier'\n",
    "        threshold: optional float to override default threshold\n",
    "\n",
    "    Returns:\n",
    "        Dict with evaluation metrics and predictions\n",
    "    \"\"\"\n",
    "    if model_type == 'autoencoder':\n",
    "        X_pred = model.predict(X_test, verbose=0)\n",
    "        reconstruction_errors = np.mean(np.square(X_test - X_pred), axis=(1, 2))\n",
    "        y_scores = reconstruction_errors\n",
    "\n",
    "        if threshold is None:\n",
    "            threshold = np.percentile(reconstruction_errors, 95)\n",
    "\n",
    "        y_pred = (reconstruction_errors > threshold).astype(int)\n",
    "\n",
    "    else:  # classifier\n",
    "        y_pred_prob = model.predict(X_test, verbose=0).flatten()\n",
    "        y_scores = y_pred_prob\n",
    "\n",
    "        if threshold is None:\n",
    "            threshold = 0.5\n",
    "\n",
    "        y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'pr_auc': average_precision_score(y_test, y_scores),\n",
    "        'roc_auc': roc_auc_score(y_test, y_scores) if len(np.unique(y_test)) > 1 else None,\n",
    "        'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'threshold_used': threshold,\n",
    "        'y_pred': y_pred.tolist(),\n",
    "        'y_scores': y_scores.tolist()\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "───────────────────────────── Main Training Pipeline ──────────────────────────────\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def stratified_trip_split(trip_ids, labels, train_ratio=0.7, val_ratio=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Splits trip IDs into train/val/test ensuring a balanced distribution of anomalous/normal trips.\n",
    "\n",
    "    Args:\n",
    "        trip_ids: Array of trip IDs for each sample\n",
    "        labels: Array of labels (0 or 1) for each sample\n",
    "        train_ratio: Proportion of data for training\n",
    "        val_ratio: Proportion of data for validation\n",
    "        seed: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        train_mask, val_mask, test_mask: Boolean masks for the input arrays\n",
    "    \"\"\"\n",
    "    unique_trips = np.unique(trip_ids)\n",
    "    trip_labels = []\n",
    "\n",
    "    for trip in unique_trips:\n",
    "        is_anomalous = np.any(labels[trip_ids == trip])\n",
    "        trip_labels.append(1 if is_anomalous else 0)\n",
    "\n",
    "    trip_labels = np.array(trip_labels)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=1 - train_ratio, random_state=seed)\n",
    "    train_idx, rest_idx = next(sss.split(unique_trips, trip_labels))\n",
    "\n",
    "    rest_trips = unique_trips[rest_idx]\n",
    "    rest_labels = trip_labels[rest_idx]\n",
    "\n",
    "    val_ratio_adjusted = val_ratio / (1 - train_ratio)\n",
    "    sss_val = StratifiedShuffleSplit(n_splits=1, test_size=1 - val_ratio_adjusted, random_state=seed)\n",
    "    val_idx, test_idx = next(sss_val.split(rest_trips, rest_labels))\n",
    "\n",
    "    train_trips = unique_trips[train_idx]\n",
    "    val_trips = rest_trips[val_idx]\n",
    "    test_trips = rest_trips[test_idx]\n",
    "\n",
    "    train_mask = np.isin(trip_ids, train_trips)\n",
    "    val_mask = np.isin(trip_ids, val_trips)\n",
    "    test_mask = np.isin(trip_ids, test_trips)\n",
    "\n",
    "    return train_mask, val_mask, test_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_route_results( test_results: dict) -> None:\n",
    "    \"\"\"\n",
    "    Print formatted results for a specific route and model type.\n",
    "\n",
    "    Args:\n",
    "        route (str): Route name.\n",
    "        model_type (str): Type of model (e.g., 'classifier', 'autoencoder').\n",
    "        route_results (dict): Dictionary containing training results (e.g., training time).\n",
    "        test_results (dict): Dictionary containing test evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"  * Precision  = TP / (TP + FP)        | Correctly flagged anomalies\")\n",
    "    print(f\"    → {test_results['precision']:.4f}\")\n",
    "\n",
    "    print(f\"  * Recall     = TP / (TP + FN)        | Detected actual anomalies\")\n",
    "    print(f\"    → {test_results['recall']:.4f}\")\n",
    "\n",
    "    print(f\"  * F1 Score   = Harmonic mean         | Balance of precision & recall\")\n",
    "    print(f\"    → {test_results['f1_score']:.4f}\")\n",
    "\n",
    "    print(f\"  * PR AUC     = Precision-Recall AUC  | Better for rare anomalies\")\n",
    "    print(f\"    → {test_results['pr_auc']:.4f}\")\n",
    "\n",
    "    if test_results['roc_auc'] is not None:\n",
    "        print(f\"  * ROC AUC    = ROC Curve AUC         | Overall separability of classes\")\n",
    "        print(f\"    → {test_results['roc_auc']:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_lstm_pipeline(data_path: str,\n",
    "                        output_dir: str,\n",
    "                        epochs: int,\n",
    "                        model_type: str = 'classifier'\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Main training pipeline for LSTM-based vessel anomaly detection.\n",
    "\n",
    "    Args:\n",
    "        data_path: Path to the parquet file\n",
    "        output_dir: Directory to save models and results\n",
    "        model_type: 'classifier' or 'autoencoder'\n",
    "        epochs: number of epochs to train the model\n",
    "    \"\"\"\n",
    "    print(f\"Starting LSTM {model_type} training pipeline...\")\n",
    "\n",
    "    # Load and prepare data\n",
    "    df = pd.read_parquet(data_path)\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for route in df.route_id.unique():\n",
    "        print(f\"\\n=== Training LSTM {model_type} for route: {route} ===\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare route data\n",
    "        fr =  df[df.route_id == route].copy()\n",
    "\n",
    "        # Create sequences\n",
    "        sequences, labels, trip_ids = create_sequences_from_trips(fr, SEQUENCE_LENGTH, OVERLAP_RATIO)\n",
    "\n",
    "        if len(sequences) == 0:\n",
    "            print(f\"  * No sequences created for route {route}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  * Created {len(sequences)} sequences\")\n",
    "        print(f\"  * Normal sequences: {np.sum(labels == 0)}, Anomalous sequences: {np.sum(labels == 1)}\")\n",
    "\n",
    "        # Scale features\n",
    "        scaler = MinMaxScaler()\n",
    "        n_samples, n_timesteps, n_features = sequences.shape\n",
    "        sequences_scaled = scaler.fit_transform(sequences.reshape(-1, n_features)).reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "        train_mask, val_mask, test_mask = stratified_trip_split(trip_ids, labels)\n",
    "\n",
    "        X_train, y_train = sequences_scaled[train_mask], labels[train_mask]\n",
    "        X_val, y_val = sequences_scaled[val_mask], labels[val_mask]\n",
    "        X_test, y_test = sequences_scaled[test_mask], labels[test_mask]\n",
    "\n",
    "        # Train model\n",
    "        model_path = f\"{output_dir}/lstm_{model_type}_{route}_{epochs}.h5\"\n",
    "\n",
    "        if model_type == 'autoencoder':\n",
    "            # Train only on normal sequences for autoencoder\n",
    "            X_train_normal = X_train[y_train == 0]\n",
    "            X_val_normal = X_val[y_val == 0]\n",
    "\n",
    "            if len(X_train_normal) == 0:\n",
    "                print(f\"  * No normal training sequences for route {route}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            model, history = train_lstm_autoencoder(X_train_normal, X_val_normal, model_path, epochs)\n",
    "        else:\n",
    "            if len(np.unique(y_train)) < 2:\n",
    "                print(f\"  * Insufficient class diversity for route {route}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            model, history = train_lstm_classifier(X_train, y_train, X_val, y_val, model_path, epochs)\n",
    "\n",
    "        # Evaluate model\n",
    "        test_results = evaluate_model(model, X_test, y_test, model_type)\n",
    "\n",
    "        # Save results\n",
    "        route_results = {\n",
    "            'route': route,\n",
    "            'model_type': model_type,\n",
    "            'training_time': time.time() - t0,\n",
    "            'n_sequences': len(sequences),\n",
    "            'n_train': len(X_train),\n",
    "            'n_val': len(X_val),\n",
    "            'n_test': len(X_test),\n",
    "            'test_metrics': test_results,\n",
    "            'training_history': history\n",
    "        }\n",
    "\n",
    "        results[route] = route_results\n",
    "\n",
    "        # Save scaler\n",
    "        joblib.dump(scaler, f\"{output_dir}/scaler_{route}_{epochs}.pkl\")\n",
    "        print(\"=\" * 5 + f\"Results for route {route} with {model_type} \" + \"=\" * 5)\n",
    "        print(f\"  * Training completed in {route_results['training_time']:.1f}s\")\n",
    "        print_route_results(test_results)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    # Save overall results\n",
    "    joblib.dump(results, f\"{output_dir}/training_results_{epochs}.pkl\")\n",
    "    print(f\"\\nTraining pipeline completed. Results saved to {output_dir}\")\n",
    "\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-───────────────────────────── LSTM Training Start ──────────────────────────────"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LSTM-specific parameters\n",
    "SEQUENCE_LENGTH = 50  # Number of time steps to look back\n",
    "OVERLAP_RATIO = 0.5   # Overlap between sequences (0.5 = 50% overlap)\n",
    "LSTM_UNITS = 64       # Number of LSTM units\n",
    "DENSE_UNITS = 32      # Dense layer units\n",
    "DROPOUT_RATE = 0.2    # Dropout rate\n",
    "BATCH_SIZE = 32       # Training batch size\n",
    "PATIENCE = 15         # Early stopping patience\n",
    "LEARNING_RATE = 0.001 # Learning rate\n",
    "\n",
    "data_path = \"data/LSTM_preprocessed.parquet\"\n",
    "output_dir = \"lstm_models\"\n",
    "\n",
    "EPOCHS = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "results_classifier = train_lstm_pipeline(data_path, f\"{output_dir}_classifier\", EPOCHS, \"classifier\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train autoencoder\n",
    "results_autoencoder = train_lstm_pipeline(data_path, f\"{output_dir}_autoencoder\", EPOCHS, \"autoencoder\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
