{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 18:07:24.490203: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-18 18:07:24.527162: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# LSTM Vessel Trajectory Anomaly Detection Pipeline\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import Sequence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.497120337Z",
     "start_time": "2025-06-18T16:07:23.517343327Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DROP_TRIPS = [10257]\n",
    "\n",
    "#Used features\n",
    "BASE_COLUMNS = [\n",
    "    \"speed_over_ground\", \"dv\", \"dcourse\", \"ddraft\",\n",
    "    \"zone\", \"x_km\", \"y_km\", \"dist_to_ref\", \"route_dummy\"\n",
    "]\n",
    "\n",
    "ZONES = [[53.8, 53.5, 8.6, 8.14], [53.66, 53.0, 11.0, 9.5]]\n",
    "R_PORT, R_APP = 5.0, 15.0\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "TF_ENABLE_ONEDNN_OPTS=0\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.501771804Z",
     "start_time": "2025-06-18T16:07:26.500605047Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "───────────────────────────── Custom Data Generator ──────────────────────────────"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class VesselSequenceGenerator(Sequence):\n",
    "    \"\"\"Custom data generator for LSTM training with vessel trajectory sequences.\"\"\"\n",
    "\n",
    "    def __init__(self, data, labels, sequence_length, batch_size, shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(data))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X = np.array([self.data[i] for i in batch_indices])\n",
    "        y = np.array([self.labels[i] for i in batch_indices])\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.512590879Z",
     "start_time": "2025-06-18T16:07:26.505129845Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "───────────────────────────── Sequence Creation ──────────────────────────────"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_sequences_from_trips(df_route: pd.DataFrame, sequence_length: int,\n",
    "                               overlap_ratio: float = 0.5) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create sequences from trip data for LSTM training.\n",
    "    Returns sequences, labels, and trip_ids for each sequence.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    trip_ids = []\n",
    "\n",
    "    step_size = max(1, int(sequence_length * (1 - overlap_ratio)))\n",
    "\n",
    "    for trip_id, trip_data in tqdm(df_route.groupby('trip_id'), desc=\"Creating sequences\"):\n",
    "        trip_data = trip_data.sort_values('time_stamp').reset_index(drop=True)\n",
    "\n",
    "        # Get features and labels\n",
    "        features = trip_data[BASE_COLUMNS].fillna(0).values\n",
    "        trip_labels = trip_data['y_true'].values\n",
    "\n",
    "        # Create overlapping sequences\n",
    "        for start_idx in range(0, len(features) - sequence_length + 1, step_size):\n",
    "            end_idx = start_idx + sequence_length\n",
    "            seq_features = features[start_idx:end_idx]\n",
    "            seq_labels = trip_labels[start_idx:end_idx]\n",
    "\n",
    "            # Label sequence as anomalous if any point in sequence is anomalous\n",
    "            sequence_label = int(np.any(seq_labels == 1))\n",
    "\n",
    "            sequences.append(seq_features)\n",
    "            labels.append(sequence_label)\n",
    "            trip_ids.append(trip_id)\n",
    "\n",
    "    return np.array(sequences), np.array(labels), np.array(trip_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.553354811Z",
     "start_time": "2025-06-18T16:07:26.517035073Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-───────────────────────────── LSTM Model Definitions ──────────────────────────────"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def build_lstm_autoencoder(input_shape: Tuple[int, int], lstm_units: int = 64,\n",
    "                          dense_units: int = 32, dropout_rate: float = 0.2) -> Model:\n",
    "    \"\"\"\n",
    "    Build LSTM Autoencoder for anomaly detection.\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    encoded = LSTM(lstm_units, return_sequences=True)(input_layer)\n",
    "    encoded = Dropout(dropout_rate)(encoded)\n",
    "    encoded = LSTM(lstm_units // 2, return_sequences=False)(encoded)\n",
    "    encoded = Dropout(dropout_rate)(encoded)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = Dense(dense_units, activation='relu')(encoded)\n",
    "    bottleneck = Dropout(dropout_rate)(bottleneck)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Dense(lstm_units // 2, activation='relu')(bottleneck)\n",
    "    decoded = Dropout(dropout_rate)(decoded)\n",
    "    decoded = tf.keras.layers.RepeatVector(input_shape[0])(decoded)\n",
    "    decoded = LSTM(lstm_units // 2, return_sequences=True)(decoded)\n",
    "    decoded = Dropout(dropout_rate)(decoded)\n",
    "    decoded = LSTM(lstm_units, return_sequences=True)(decoded)\n",
    "    decoded = TimeDistributed(Dense(input_shape[1]))(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    return autoencoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.553529016Z",
     "start_time": "2025-06-18T16:07:26.545871302Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def build_lstm_classifier(input_shape: Tuple[int, int], lstm_units: int = 64,\n",
    "                         dense_units: int = 32, dropout_rate: float = 0.2) -> Model:\n",
    "    \"\"\"\n",
    "    Build LSTM Classifier for anomaly detection.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units // 2, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.553598335Z",
     "start_time": "2025-06-18T16:07:26.546031269Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train_lstm_autoencoder(X_train_normal: np.ndarray, X_val: np.ndarray,\n",
    "                          model_path: str) -> Tuple[Model, Dict]:\n",
    "    \"\"\"\n",
    "    Train LSTM Autoencoder on normal data only.\n",
    "    \"\"\"\n",
    "    input_shape = (X_train_normal.shape[1], X_train_normal.shape[2])\n",
    "    model = build_lstm_autoencoder(input_shape, LSTM_UNITS, DENSE_UNITS, DROPOUT_RATE)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=PATIENCE, restore_best_weights=True, monitor='val_loss'),\n",
    "        ModelCheckpoint(model_path.replace('.h5', '.keras'), save_best_only=True, monitor='val_loss'),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=PATIENCE//2, min_lr=1e-6, monitor='val_loss')\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_normal, X_train_normal,  # Autoencoder: input = output\n",
    "        validation_data=(X_val, X_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model, history.history"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.599051294Z",
     "start_time": "2025-06-18T16:07:26.549447549Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_lstm_classifier(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                         X_val: np.ndarray, y_val: np.ndarray,\n",
    "                         model_path: str) -> Tuple[Model, Dict]:\n",
    "    \"\"\"\n",
    "    Train LSTM Classifier for binary anomaly classification.\n",
    "    \"\"\"\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_lstm_classifier(input_shape, LSTM_UNITS, DENSE_UNITS, DROPOUT_RATE)\n",
    "\n",
    "    # Handle class imbalance\n",
    "    class_weight = {\n",
    "        0: len(y_train) / (2 * np.sum(y_train == 0)),\n",
    "        1: len(y_train) / (2 * np.sum(y_train == 1))\n",
    "    }\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=PATIENCE, restore_best_weights=True, monitor='val_loss'),\n",
    "        ModelCheckpoint(model_path.replace('.h5', '.keras'), save_best_only=True, monitor='val_loss'),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=PATIENCE//2, min_lr=1e-6, monitor='val_loss')\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model, history.history"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.599320283Z",
     "start_time": "2025-06-18T16:07:26.591495424Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def evaluate_model(model: Model, X_test: np.ndarray, y_test: np.ndarray,\n",
    "                  model_type: str = 'classifier') -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate trained model and return metrics.\n",
    "    \"\"\"\n",
    "    if model_type == 'autoencoder':\n",
    "        # For autoencoder, compute reconstruction error\n",
    "        X_pred = model.predict(X_test, verbose=0)\n",
    "        reconstruction_errors = np.mean(np.square(X_test - X_pred), axis=(1, 2))\n",
    "\n",
    "        # Use threshold to classify anomalies (e.g., 95th percentile of normal data errors)\n",
    "        # This would typically be computed on validation set\n",
    "        threshold = np.percentile(reconstruction_errors, 95)\n",
    "        y_pred = (reconstruction_errors > threshold).astype(int)\n",
    "        y_scores = reconstruction_errors\n",
    "\n",
    "    else:  # classifier\n",
    "        y_pred_prob = model.predict(X_test, verbose=0).flatten()\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "        y_scores = y_pred_prob\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "    if len(np.unique(y_test)) > 1:  # Check if both classes are present\n",
    "        auc_score = roc_auc_score(y_test, y_scores)\n",
    "        class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    else:\n",
    "        auc_score = 0.0\n",
    "        class_report = {}\n",
    "        conf_matrix = np.array([[]])\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'auc_score': auc_score,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'y_pred': y_pred,\n",
    "        'y_scores': y_scores\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.599388866Z",
     "start_time": "2025-06-18T16:07:26.591666897Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "───────────────────────────── Main Training Pipeline ──────────────────────────────\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train_lstm_pipeline(data_path: str, output_dir: str, model_type: str = 'classifier'):\n",
    "    \"\"\"\n",
    "    Main training pipeline for LSTM-based vessel anomaly detection.\n",
    "\n",
    "    Args:\n",
    "        data_path: Path to the parquet file\n",
    "        output_dir: Directory to save models and results\n",
    "        model_type: 'classifier' or 'autoencoder'\n",
    "    \"\"\"\n",
    "    print(f\"Starting LSTM {model_type} training pipeline...\")\n",
    "\n",
    "    # Load and prepare data\n",
    "    df = pd.read_parquet(data_path)\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for route in df.route_id.unique():\n",
    "        print(f\"\\n=== Training LSTM {model_type} for route: {route} ===\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare route data\n",
    "        fr =  df[df.route_id == route].copy()\n",
    "\n",
    "        # Create sequences\n",
    "        sequences, labels, trip_ids = create_sequences_from_trips(fr, SEQUENCE_LENGTH, OVERLAP_RATIO)\n",
    "\n",
    "        if len(sequences) == 0:\n",
    "            print(f\"  * No sequences created for route {route}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  * Created {len(sequences)} sequences\")\n",
    "        print(f\"  * Normal sequences: {np.sum(labels == 0)}, Anomalous sequences: {np.sum(labels == 1)}\")\n",
    "\n",
    "        # Scale features\n",
    "        scaler = MinMaxScaler()\n",
    "        n_samples, n_timesteps, n_features = sequences.shape\n",
    "        sequences_scaled = scaler.fit_transform(sequences.reshape(-1, n_features)).reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "        # Split data\n",
    "        unique_trips = np.unique(trip_ids)\n",
    "        n_train_trips = int(0.7 * len(unique_trips))\n",
    "        n_val_trips = int(0.15 * len(unique_trips))\n",
    "\n",
    "        np.random.shuffle(unique_trips)\n",
    "        train_trips = unique_trips[:n_train_trips]\n",
    "        val_trips = unique_trips[n_train_trips:n_train_trips + n_val_trips]\n",
    "        test_trips = unique_trips[n_train_trips + n_val_trips:]\n",
    "\n",
    "        train_mask = np.isin(trip_ids, train_trips)\n",
    "        val_mask = np.isin(trip_ids, val_trips)\n",
    "        test_mask = np.isin(trip_ids, test_trips)\n",
    "\n",
    "        X_train, y_train = sequences_scaled[train_mask], labels[train_mask]\n",
    "        X_val, y_val = sequences_scaled[val_mask], labels[val_mask]\n",
    "        X_test, y_test = sequences_scaled[test_mask], labels[test_mask]\n",
    "\n",
    "        # Train model\n",
    "        model_path = f\"{output_dir}/lstm_{model_type}_{route}.h5\"\n",
    "\n",
    "        if model_type == 'autoencoder':\n",
    "            # Train only on normal sequences for autoencoder\n",
    "            X_train_normal = X_train[y_train == 0]\n",
    "            X_val_normal = X_val[y_val == 0]\n",
    "\n",
    "            if len(X_train_normal) == 0:\n",
    "                print(f\"  * No normal training sequences for route {route}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            model, history = train_lstm_autoencoder(X_train_normal, X_val_normal, model_path)\n",
    "        else:\n",
    "            if len(np.unique(y_train)) < 2:\n",
    "                print(f\"  * Insufficient class diversity for route {route}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            model, history = train_lstm_classifier(X_train, y_train, X_val, y_val, model_path)\n",
    "\n",
    "        # Evaluate model\n",
    "        test_results = evaluate_model(model, X_test, y_test, model_type)\n",
    "\n",
    "        # Save results\n",
    "        route_results = {\n",
    "            'route': route,\n",
    "            'model_type': model_type,\n",
    "            'training_time': time.time() - t0,\n",
    "            'n_sequences': len(sequences),\n",
    "            'n_train': len(X_train),\n",
    "            'n_val': len(X_val),\n",
    "            'n_test': len(X_test),\n",
    "            'test_metrics': test_results,\n",
    "            'training_history': history\n",
    "        }\n",
    "\n",
    "        results[route] = route_results\n",
    "\n",
    "        # Save scaler\n",
    "        joblib.dump(scaler, f\"{output_dir}/scaler_{route}.pkl\")\n",
    "\n",
    "        print(f\"  * Training completed in {route_results['training_time']:.1f}s\")\n",
    "        print(f\"  * Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "        print(f\"  * Test AUC: {test_results['auc_score']:.4f}\")\n",
    "\n",
    "    # Save overall results\n",
    "    joblib.dump(results, f\"{output_dir}/training_results.pkl\")\n",
    "    print(f\"\\nTraining pipeline completed. Results saved to {output_dir}\")\n",
    "\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.599463253Z",
     "start_time": "2025-06-18T16:07:26.591766123Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-───────────────────────────── LSTM Training Start ──────────────────────────────"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# LSTM-specific parameters\n",
    "SEQUENCE_LENGTH = 50  # Number of time steps to look back\n",
    "OVERLAP_RATIO = 0.5   # Overlap between sequences (0.5 = 50% overlap)\n",
    "LSTM_UNITS = 64       # Number of LSTM units\n",
    "DENSE_UNITS = 32      # Dense layer units\n",
    "DROPOUT_RATE = 0.2    # Dropout rate\n",
    "BATCH_SIZE = 32       # Training batch size\n",
    "PATIENCE = 15         # Early stopping patience\n",
    "LEARNING_RATE = 0.001 # Learning rate\n",
    "\n",
    "data_path = \"data/LSTM_preprocessed.parquet\"\n",
    "output_dir = \"lstm_models\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:07:26.599521947Z",
     "start_time": "2025-06-18T16:07:26.591949202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSTM classifier_10 training pipeline...\n",
      "\n",
      "=== Training LSTM classifier_10 for route: KIEL ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "Creating sequences:   0%|          | 0/423 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d5f725bd1784e3d83d536f2dbcb3ec4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Created 20789 sequences\n",
      "  * Normal sequences: 20498, Anomalous sequences: 291\n",
      "Epoch 1/10\n",
      "\u001B[1m451/451\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 22ms/step - accuracy: 0.7851 - loss: 0.6495 - precision: 0.0184 - recall: 0.3457 - val_accuracy: 0.8332 - val_loss: 0.6622 - val_precision: 0.0430 - val_recall: 0.4074 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001B[1m451/451\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 21ms/step - accuracy: 0.7794 - loss: 0.6227 - precision: 0.0271 - recall: 0.4645 - val_accuracy: 0.9275 - val_loss: 0.5602 - val_precision: 0.0564 - val_recall: 0.2037 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001B[1m451/451\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 21ms/step - accuracy: 0.8188 - loss: 0.5991 - precision: 0.0351 - recall: 0.4788 - val_accuracy: 0.9300 - val_loss: 0.5380 - val_precision: 0.0588 - val_recall: 0.2037 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001B[1m451/451\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 21ms/step - accuracy: 0.8290 - loss: 0.5876 - precision: 0.0341 - recall: 0.4497 - val_accuracy: 0.9821 - val_loss: 0.5654 - val_precision: 0.2500 - val_recall: 0.0185 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001B[1m451/451\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 21ms/step - accuracy: 0.5541 - loss: 0.6862 - precision: 0.0102 - recall: 0.3900 - val_accuracy: 0.0288 - val_loss: 0.7662 - val_precision: 0.0175 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001B[1m451/451\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 21ms/step - accuracy: 0.5035 - loss: 0.6690 - precision: 0.0111 - recall: 0.4694 - val_accuracy: 0.0288 - val_loss: 0.7438 - val_precision: 0.0175 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001B[1m451/451\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 21ms/step - accuracy: 0.4524 - loss: 0.6618 - precision: 0.0112 - recall: 0.5226 - val_accuracy: 0.4519 - val_loss: 0.7201 - val_precision: 0.0279 - val_recall: 0.9074 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001B[1m451/451\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 21ms/step - accuracy: 0.6737 - loss: 0.6353 - precision: 0.0225 - recall: 0.5883 - val_accuracy: 0.0173 - val_loss: 0.8086 - val_precision: 0.0173 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001B[1m451/451\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 21ms/step - accuracy: 0.1346 - loss: 0.6661 - precision: 0.0106 - recall: 0.7912 - val_accuracy: 0.0173 - val_loss: 0.7458 - val_precision: 0.0173 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001B[1m451/451\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 21ms/step - accuracy: 0.2810 - loss: 0.6645 - precision: 0.0109 - recall: 0.6828 - val_accuracy: 0.0173 - val_loss: 0.7166 - val_precision: 0.0173 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "  * Training completed in 99.7s\n",
      "  * Test Accuracy: 0.9324\n",
      "  * Test AUC: 0.7551\n",
      "\n",
      "=== Training LSTM classifier_10 for route: BREMERHAVEN ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "Creating sequences:   0%|          | 0/702 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26b7e23f7647452db55fe78eded66a0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Created 14065 sequences\n",
      "  * Normal sequences: 14003, Anomalous sequences: 62\n",
      "Epoch 1/10\n",
      "\u001B[1m307/307\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 23ms/step - accuracy: 0.8473 - loss: 0.7015 - precision: 0.0083 - recall: 0.3328 - val_accuracy: 0.9981 - val_loss: 0.1700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001B[1m307/307\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 21ms/step - accuracy: 0.8150 - loss: 0.7351 - precision: 0.0086 - recall: 0.3812 - val_accuracy: 0.6036 - val_loss: 0.5936 - val_precision: 0.0048 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001B[1m307/307\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 21ms/step - accuracy: 0.7865 - loss: 0.5130 - precision: 0.0161 - recall: 0.7633 - val_accuracy: 0.6737 - val_loss: 0.5172 - val_precision: 0.0058 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001B[1m307/307\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 21ms/step - accuracy: 0.7076 - loss: 0.4643 - precision: 0.0138 - recall: 0.9277 - val_accuracy: 0.2457 - val_loss: 0.6187 - val_precision: 0.0025 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001B[1m307/307\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 21ms/step - accuracy: 0.5102 - loss: 0.6908 - precision: 0.0067 - recall: 0.7485 - val_accuracy: 0.9904 - val_loss: 0.2232 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001B[1m307/307\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 22ms/step - accuracy: 0.7060 - loss: 0.5236 - precision: 0.0179 - recall: 0.8881 - val_accuracy: 0.3071 - val_loss: 0.7924 - val_precision: 0.0028 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001B[1m307/307\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 22ms/step - accuracy: 0.3973 - loss: 0.5628 - precision: 0.0069 - recall: 0.9467 - val_accuracy: 0.5758 - val_loss: 0.4208 - val_precision: 0.0045 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001B[1m307/307\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 21ms/step - accuracy: 0.6697 - loss: 0.5112 - precision: 0.0114 - recall: 0.8624 - val_accuracy: 0.5998 - val_loss: 0.5677 - val_precision: 0.0048 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001B[1m307/307\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 21ms/step - accuracy: 0.6621 - loss: 0.4987 - precision: 0.0119 - recall: 0.9197 - val_accuracy: 0.7764 - val_loss: 0.4203 - val_precision: 0.0085 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001B[1m307/307\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 21ms/step - accuracy: 0.7044 - loss: 0.4859 - precision: 0.0137 - recall: 0.8958 - val_accuracy: 0.5821 - val_loss: 0.5772 - val_precision: 0.0046 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "  * Training completed in 70.3s\n",
      "  * Test Accuracy: 0.9926\n",
      "  * Test AUC: 0.8409\n",
      "\n",
      "Training pipeline completed. Results saved to lstm_models_classifier\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10          # Maximum training epochs\n",
    "\n",
    "# Train classifier\n",
    "results_classifier = train_lstm_pipeline(data_path, f\"{output_dir}_classifier\", f\"classifier_{EPOCHS}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:10:17.073073903Z",
     "start_time": "2025-06-18T16:07:26.592038083Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSTM autoencoder_10 training pipeline...\n",
      "\n",
      "=== Training LSTM autoencoder_10 for route: KIEL ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "Creating sequences:   0%|          | 0/423 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e5d77f185174a94a654cfbfa2930af7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Created 20789 sequences\n",
      "  * Normal sequences: 20498, Anomalous sequences: 291\n",
      "Epoch 1/10\n",
      "\u001B[1m455/455\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 22ms/step - accuracy: 0.7367 - loss: 0.6865 - precision: 0.0215 - recall: 0.3909 - val_accuracy: 0.3652 - val_loss: 0.7651 - val_precision: 0.0152 - val_recall: 0.8108 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001B[1m455/455\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 21ms/step - accuracy: 0.6685 - loss: 0.6352 - precision: 0.0273 - recall: 0.5690 - val_accuracy: 0.4899 - val_loss: 0.7507 - val_precision: 0.0165 - val_recall: 0.7027 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001B[1m455/455\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 21ms/step - accuracy: 0.7257 - loss: 0.6167 - precision: 0.0323 - recall: 0.5625 - val_accuracy: 0.5330 - val_loss: 0.7371 - val_precision: 0.0193 - val_recall: 0.7568 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001B[1m455/455\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 22ms/step - accuracy: 0.7664 - loss: 0.6067 - precision: 0.0370 - recall: 0.5518 - val_accuracy: 0.6978 - val_loss: 0.6726 - val_precision: 0.0296 - val_recall: 0.7568 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001B[1m455/455\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 21ms/step - accuracy: 0.7265 - loss: 0.5995 - precision: 0.0361 - recall: 0.6386 - val_accuracy: 0.8763 - val_loss: 0.5732 - val_precision: 0.0547 - val_recall: 0.5676 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001B[1m455/455\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 21ms/step - accuracy: 0.7708 - loss: 0.6171 - precision: 0.0379 - recall: 0.5669 - val_accuracy: 0.0137 - val_loss: 0.9968 - val_precision: 0.0121 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001B[1m455/455\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 22ms/step - accuracy: 0.5373 - loss: 0.7037 - precision: 0.0119 - recall: 0.3504 - val_accuracy: 0.9879 - val_loss: 0.6530 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001B[1m455/455\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 21ms/step - accuracy: 0.5338 - loss: 0.6859 - precision: 0.0167 - recall: 0.5070 - val_accuracy: 0.0121 - val_loss: 0.6949 - val_precision: 0.0121 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001B[1m455/455\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 21ms/step - accuracy: 0.6172 - loss: 0.6864 - precision: 0.0134 - recall: 0.3327 - val_accuracy: 0.9879 - val_loss: 0.6864 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001B[1m455/455\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 21ms/step - accuracy: 0.5328 - loss: 0.6833 - precision: 0.0145 - recall: 0.4415 - val_accuracy: 0.9879 - val_loss: 0.6884 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "  * Training completed in 102.9s\n",
      "  * Test Accuracy: 0.8763\n",
      "  * Test AUC: 0.7226\n",
      "\n",
      "=== Training LSTM autoencoder_10 for route: BREMERHAVEN ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "Creating sequences:   0%|          | 0/702 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fb89a186af246198d003150d4d04eb3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Created 14065 sequences\n",
      "  * Normal sequences: 14003, Anomalous sequences: 62\n",
      "Epoch 1/10\n",
      "\u001B[1m304/304\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 23ms/step - accuracy: 0.8718 - loss: 0.7639 - precision: 0.0048 - recall: 0.1937 - val_accuracy: 0.6685 - val_loss: 0.5549 - val_precision: 0.0152 - val_recall: 0.7857 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001B[1m304/304\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 21ms/step - accuracy: 0.7341 - loss: 0.6350 - precision: 0.0082 - recall: 0.6234 - val_accuracy: 0.5933 - val_loss: 0.7345 - val_precision: 0.0135 - val_recall: 0.8571 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001B[1m304/304\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 28ms/step - accuracy: 0.6837 - loss: 0.4931 - precision: 0.0094 - recall: 0.8836 - val_accuracy: 0.6717 - val_loss: 0.5488 - val_precision: 0.0166 - val_recall: 0.8571 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001B[1m304/304\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 22ms/step - accuracy: 0.7169 - loss: 0.4594 - precision: 0.0103 - recall: 0.8728 - val_accuracy: 0.5642 - val_loss: 0.8953 - val_precision: 0.0126 - val_recall: 0.8571 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001B[1m304/304\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 22ms/step - accuracy: 0.6508 - loss: 0.4978 - precision: 0.0089 - recall: 0.9194 - val_accuracy: 0.6681 - val_loss: 0.6987 - val_precision: 0.0165 - val_recall: 0.8571 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001B[1m304/304\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 22ms/step - accuracy: 0.7031 - loss: 0.4238 - precision: 0.0107 - recall: 0.9472 - val_accuracy: 0.8296 - val_loss: 0.4231 - val_precision: 0.0267 - val_recall: 0.7143 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001B[1m304/304\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 21ms/step - accuracy: 0.8269 - loss: 0.4439 - precision: 0.0138 - recall: 0.7129 - val_accuracy: 0.7336 - val_loss: 0.5246 - val_precision: 0.0204 - val_recall: 0.8571 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001B[1m304/304\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 22ms/step - accuracy: 0.7295 - loss: 0.4917 - precision: 0.0093 - recall: 0.7401 - val_accuracy: 0.7553 - val_loss: 0.4925 - val_precision: 0.0222 - val_recall: 0.8571 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001B[1m304/304\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 22ms/step - accuracy: 0.7182 - loss: 0.4790 - precision: 0.0090 - recall: 0.7441 - val_accuracy: 0.7313 - val_loss: 0.5092 - val_precision: 0.0203 - val_recall: 0.8571 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001B[1m304/304\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 21ms/step - accuracy: 0.7346 - loss: 0.4328 - precision: 0.0112 - recall: 0.8947 - val_accuracy: 0.7165 - val_loss: 0.5008 - val_precision: 0.0192 - val_recall: 0.8571 - learning_rate: 0.0010\n",
      "  * Training completed in 72.9s\n",
      "  * Test Accuracy: 0.8237\n",
      "  * Test AUC: 0.8448\n",
      "\n",
      "Training pipeline completed. Results saved to lstm_models_autoencoder\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10          # Maximum training epochs\n",
    "# Train autoencoder\n",
    "results_autoencoder = train_lstm_pipeline(data_path, f\"{output_dir}_autoencoder\", f\"autoencoder_{EPOCHS}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-18T16:13:13.310108062Z",
     "start_time": "2025-06-18T16:10:17.075070579Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
