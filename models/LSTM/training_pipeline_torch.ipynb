{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T01:27:26.917914442Z",
     "start_time": "2025-06-25T01:27:21.147727883Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMModel interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-06-25T01:27:26.924976628Z",
     "start_time": "2025-06-25T01:27:26.919539423Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of a Long Short-Term Memory (LSTM) model for time-series forecasting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 num_layers,\n",
    "                 output_size,\n",
    "                 dropout=0):\n",
    "\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Encode: Get the final hidden state as the compressed representation\n",
    "        _, (hidden, cell) = self.encoder(x)\n",
    "\n",
    "        # Prepare decoder input: repeat the encoded representation for each timestep\n",
    "        # Use the last hidden state as the encoded representation\n",
    "        encoded = hidden[-1].unsqueeze(1).repeat(1, seq_len, 1)  # Shape: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Decode: Reconstruct the sequence\n",
    "        decoder_output, _ = self.decoder(encoded)\n",
    "\n",
    "        # Map back to original feature space\n",
    "        reconstructed = self.output_layer(decoder_output)\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "    def get_reconstruction_error(self, x):\n",
    "        \"\"\"Calculate reconstruction error for anomaly detection\"\"\"\n",
    "        with torch.no_grad():\n",
    "            reconstructed = self.forward(x)\n",
    "            # Calculate MSE for each sequence\n",
    "            mse = torch.mean((x - reconstructed) ** 2, dim=(1, 2))\n",
    "            return mse.cpu().numpy()  # Changed from .gpu() to .cpu()\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Get the encoded representation of input sequences\"\"\"\n",
    "        with torch.no_grad():\n",
    "            _, (hidden, _) = self.encoder(x)\n",
    "            return hidden[-1]  # Return the last layer's hidden state\n",
    "    \n",
    "    def get_score_route(self, sequences, trip_length):\n",
    "        if len(sequences) == 0:\n",
    "            return np.zeros(trip_length)\n",
    "        \n",
    "        # Convert to tensor and move to same device as model\n",
    "        X_tensor = torch.from_numpy(sequences).float().to(next(self.parameters()).device)\n",
    "        \n",
    "        # Get reconstruction errors\n",
    "        reconstruction_errors = self.get_reconstruction_error(X_tensor)\n",
    "        \n",
    "        # Initialize scores array\n",
    "        scores = np.zeros(trip_length)\n",
    "        \n",
    "        # Map reconstruction errors to trajectory points\n",
    "        sequence_length = sequences.shape[1]\n",
    "        \n",
    "        for i, error in enumerate(reconstruction_errors):\n",
    "            start_idx = i\n",
    "            end_idx = min(i + sequence_length, len(scores))\n",
    "            # Use maximum error for overlapping windows\n",
    "            scores[start_idx:end_idx] = np.maximum(scores[start_idx:end_idx], error)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T01:27:26.931404574Z",
     "start_time": "2025-06-25T01:27:26.922351641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded for experiment: lstm_per_route_v1\n",
      "Training epochs: 4, Batch size: 16\n",
      "Output directories: models_per_route_lstm_ae\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION PARAMETERS - MODIFY THESE TO EXPERIMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Data parameters\n",
    "DATA_PATH = \"LSTM_preprocessed.parquet\"\n",
    "EXPERIMENT_NAME = \"lstm_per_route_v1\"\n",
    "\n",
    "# FEATURE_COLUMNS = ['latitude', 'longitude',\n",
    "#                    'speed_over_ground', 'course_over_ground'\n",
    "#                    ]\n",
    "\n",
    "FEATURE_COLUMNS = [ \"speed_over_ground\", \"course_over_ground\",\n",
    "                    \"x_km\", \"y_km\", \"dist_to_ref\", \"zone\"\n",
    "                  ]\n",
    "\n",
    "# Sequence parameters\n",
    "SEQUENCE_LENGTH = 15        # Number of time steps in each sequence\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 4                 # Maximum epochs\n",
    "BATCH_SIZE = 16            \n",
    "VALIDATION_SIZE = 0.2       # Proportion of data for validation\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 1 \n",
    "\n",
    "# Evaluation parameters\n",
    "AUTOENCODER_THRESHOLD_PERCENTILE = 95\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR_AE = \"models_per_route_lstm_ae\"    # Autoencoder models\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "print(f\"Configuration loaded for experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Training epochs: {EPOCHS}, Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Output directories: {OUTPUT_DIR_AE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-06-25T01:27:27.009502651Z",
     "start_time": "2025-06-25T01:27:26.936092969Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Creates sequences from time-series data for training recurrent neural networks.\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i : i + seq_length])\n",
    "        ys.append(data[i + seq_length])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def create_sequences_for_trip(trip_data, feature_columns, sequence_length, scaler):\n",
    "    # Extract and scale features\n",
    "    features = trip_data[feature_columns].values\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Create sequences\n",
    "    if len(features_scaled) < sequence_length:\n",
    "        return np.array([])  # Not enough points for even one sequence\n",
    "    \n",
    "    sequences = []\n",
    "    for i in range(len(features_scaled) - sequence_length + 1):\n",
    "        sequences.append(features_scaled[i:i + sequence_length])\n",
    "    \n",
    "    return np.array(sequences)\n",
    "    \n",
    "def divide_validation(X, y):\n",
    "    X_train, y_train, X_val, y_val = [], [], [], []\n",
    "    # Split the data into training and validation sets\n",
    "\n",
    "def divide_normal_anomaly(df):\n",
    "    normal_trip_ids = df.groupby('trip_id')['y_true'].all()\n",
    "    normal_trip_ids = normal_trip_ids[normal_trip_ids == True].index\n",
    "    df_completely_normal_trips = df[df['trip_id'].isin(normal_trip_ids)].copy()\n",
    "\n",
    "    anomaly_trip_ids = df.groupby('trip_id')['y_true'].any()\n",
    "    anomaly_trip_ids = anomaly_trip_ids[anomaly_trip_ids == True].index\n",
    "    df_trips_with_anomalies = df[df['trip_id'].isin(anomaly_trip_ids)].copy()\n",
    "\n",
    "\n",
    "    print(f\"Total trips: {len(df['trip_id'].unique())}\")\n",
    "    print(f\"Normal trips: {len(df_completely_normal_trips['trip_id'].unique())}\")\n",
    "    print(f\"Anomaly trips: {len(df_trips_with_anomalies['trip_id'].unique())}\")\n",
    "    \n",
    "    return df_completely_normal_trips, df_trips_with_anomalies\n",
    "    \n",
    "def get_data_loader(df):\n",
    "    data_features = df[FEATURE_COLUMNS].values\n",
    "    data_scaled = scaler.fit_transform(data_features)\n",
    "\n",
    "    X, y = create_sequences(data_scaled, SEQUENCE_LENGTH)\n",
    "\n",
    "    X_tensor = torch.from_numpy(X).float()\n",
    "    y_tensor = torch.from_numpy(y).float()\n",
    "    \n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "def load_data_route(route, datapath = DATA_PATH):\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "    df.sort_values(['trip_id', 'time_stamp'], inplace=True)\n",
    "    df_route = df[df['start_port'] == route].copy()\n",
    "\n",
    "    return df_route\n",
    "\n",
    "def get_anomaly_example(df_anomalies):\n",
    "    \n",
    "    one_anomaly = df_anomalies[df_anomalies['trip_id'] == df_anomalies['trip_id'].unique()[0]].copy()\n",
    "    one_anomaly['lstm_prediction'] = 0\n",
    "    \n",
    "    one_anomaly = one_anomaly.sort_values('time_stamp')\n",
    "    one_an_scaled = scaler.transform(one_anomaly[FEATURE_COLUMNS].values)\n",
    "    X_anomaly, y_anomaly = create_sequences(one_an_scaled, SEQUENCE_LENGTH)\n",
    "    X_anomaly_tensor = torch.from_numpy(X_anomaly).float()\n",
    "\n",
    "    return one_anomaly, X_anomaly_tensor, y_anomaly\n",
    "    \n",
    "def anomaly_check(one_anomaly):\n",
    "    print(\"LSTM Predictions Summary:\")\n",
    "    print(f\"Total points: {len(one_anomaly)}\")\n",
    "    print(f\"LSTM predicted anomalies: {(one_anomaly['lstm_prediction'] == 1).sum()}\")\n",
    "    print(f\"Ground truth anomalies: {(one_anomaly['y_true'] == 1).sum()}\")\n",
    "\n",
    "    # Calculate some basic metrics\n",
    "    true_positives = ((one_anomaly['y_true'] == 1) & (one_anomaly['lstm_prediction'] == 1)).sum()\n",
    "    false_positives = ((one_anomaly['y_true'] == 0) & (one_anomaly['lstm_prediction'] == 1)).sum()\n",
    "    false_negatives = ((one_anomaly['y_true'] == 1) & (one_anomaly['lstm_prediction'] == 0)).sum()\n",
    "\n",
    "    print(f\"True positives: {true_positives}\")\n",
    "    print(f\"False positives: {false_positives}\")\n",
    "    print(f\"False Negatives: {false_negatives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T01:27:27.062410065Z",
     "start_time": "2025-06-25T01:27:26.954835516Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_loader, \n",
    "          loss_function,\n",
    "          optimizer,\n",
    "          X_anomaly_tensor, \n",
    "          one_anomaly):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Enable cudnn benchmarking for better performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Mixed precision training for better memory usage\n",
    "    # scaler_amp = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        num_batches = 0\n",
    "    \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        train_reconstruction_errors = []\n",
    "    \n",
    "        for batch_idx, (batch_x, _) in enumerate(pbar):\n",
    "            # Move batch to GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "    \n",
    "            # Mixed precision forward pass\n",
    "            with torch.cuda.amp.autocast():\n",
    "                reconstructed = model(batch_x)\n",
    "                loss = loss_function(reconstructed, batch_x)\n",
    "    \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            # scaler_amp.unscale_(optimizer)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "            # scaler_amp.step(optimizer)\n",
    "            # scaler_amp.update()\n",
    "    \n",
    "            batch_errors = model.get_reconstruction_error(batch_x)\n",
    "            train_reconstruction_errors.extend(batch_errors)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "            # Update progress bar\n",
    "            if batch_idx % 10 == 0:\n",
    "                pbar.set_postfix({'loss': f'{train_loss/num_batches:.6f}'})\n",
    "    \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "\n",
    "        # val_loss = 0.0\n",
    "        # num_val_batches = 0\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     for batch_x, _ in val_loader:\n",
    "        #         batch_x = batch_x.to(device)\n",
    "        #         reconstructed = model(batch_x)\n",
    "        #         loss = loss_function(reconstructed, batch_x)\n",
    "        #         val_loss += loss.item()\n",
    "        #         num_val_batches += 1\n",
    "        #\n",
    "        # avg_train_loss = train_loss / num_batches\n",
    "        # avg_val_loss = val_loss / num_val_batches\n",
    "        #\n",
    "        # print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.6f}, Val Loss = {avg_val_loss:.6f}\")\n",
    "        #\n",
    "        # # Learning rate scheduling\n",
    "        # scheduler.step(avg_val_loss)\n",
    "        #\n",
    "        # # Early stopping\n",
    "        # if avg_val_loss < best_val_loss:\n",
    "        #     best_val_loss = avg_val_loss\n",
    "        #     patience_counter = 0\n",
    "        #     # Save best model\n",
    "        #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     if patience_counter >= PATIENCE:\n",
    "        #         print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        #         break\n",
    "    \n",
    "        # train_reconstruction_errors = model.get_reconstruction_error(X_train_tensor)\n",
    "        # threshold = np.percentile(train_reconstruction_errors, AUTOENCODER_THRESHOLD_PERCENTILE)\n",
    "    \n",
    "        train_reconstruction_errors = np.array(train_reconstruction_errors)\n",
    "        threshold = np.percentile(train_reconstruction_errors, AUTOENCODER_THRESHOLD_PERCENTILE)\n",
    "        print(f\"Anomaly threshold: {threshold:.6f}\")\n",
    "    \n",
    "        reconstruction_errors = model.get_reconstruction_error(X_anomaly_tensor)\n",
    "    \n",
    "        anomaly_mask = reconstruction_errors > threshold\n",
    "        anomaly_indices = np.where(anomaly_mask)[0]\n",
    "    \n",
    "        # Mark points as anomalous based on which sequences they belong to\n",
    "        for seq_idx in anomaly_indices:\n",
    "            start_point = seq_idx\n",
    "            end_point = min(seq_idx + SEQUENCE_LENGTH, len(one_anomaly))\n",
    "    \n",
    "            # Mark all points in this sequence as anomalous\n",
    "            one_anomaly.iloc[start_point:end_point, one_anomaly.columns.get_loc('lstm_prediction')] = 1\n",
    "    \n",
    "        anomaly_check(one_anomaly)\n",
    "    \n",
    "    # Load best model\n",
    "    # model.load_state_dict(torch.load('best_model.pth'))\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "def evaluate_model(model, train_loader):\n",
    "    model.eval()\n",
    "    train_reconstruction_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_errors = model.get_reconstruction_error(batch_x)\n",
    "            train_reconstruction_errors.extend(batch_errors)\n",
    "    \n",
    "    train_reconstruction_errors = np.array(train_reconstruction_errors)\n",
    "    threshold = np.percentile(train_reconstruction_errors, AUTOENCODER_THRESHOLD_PERCENTILE)\n",
    "    print(f\"Anomaly threshold: {threshold:.6f}\")\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and devide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T01:27:33.241215638Z",
     "start_time": "2025-06-25T01:27:26.958833813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BREMERHAVEN\n",
      "Total trips: 702\n",
      "Normal trips: 650\n",
      "Anomaly trips: 14\n",
      "\n",
      "KIEL\n",
      "Total trips: 420\n",
      "Normal trips: 356\n",
      "Anomaly trips: 62\n"
     ]
    }
   ],
   "source": [
    "df_KIEL = load_data_route(\"KIEL\")\n",
    "df_BREM = load_data_route(\"BREMERHAVEN\")\n",
    "\n",
    "print(\"BREMERHAVEN\")\n",
    "df_normal_B, df_anomalies_B = divide_normal_anomaly(df_BREM)\n",
    "train_loader_B = get_data_loader(df_normal_B)\n",
    "\n",
    "print(\"\\nKIEL\")\n",
    "df_normal_K, df_anomalies_K = divide_normal_anomaly(df_KIEL)\n",
    "train_loader_K = get_data_loader(df_normal_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T14:28:41.537296869Z",
     "start_time": "2025-06-24T14:28:41.515513062Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "anomaly_B, X_anomaly_tensor_B, _ = get_anomaly_example(df_anomalies_B)\n",
    "anomaly_K, X_anomaly_tensor_K, _ = get_anomaly_example(df_anomalies_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model configure and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_BREM = LSTMModel(input_size=len(FEATURE_COLUMNS), hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=len(FEATURE_COLUMNS))\n",
    "loss_function_BREM = nn.MSELoss()\n",
    "optimizer_BREM = torch.optim.Adam(model_BREM.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "\n",
    "model_KIEL = LSTMModel(input_size=len(FEATURE_COLUMNS), hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=len(FEATURE_COLUMNS))\n",
    "loss_function_KIEL = nn.MSELoss()\n",
    "optimizer_KIEL = torch.optim.Adam(model_KIEL.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Move to CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model_BREM = model_BREM.to(device)\n",
    "model_KIEL = model_KIEL.to(device)\n",
    "\n",
    "\n",
    "X_anomaly_tensor_B = X_anomaly_tensor_B.to(device)\n",
    "X_anomaly_tensor_K = X_anomaly_tensor_K.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 21999/21999 [01:31<00:00, 240.84it/s, loss=0.001679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 0.006242\n",
      "LSTM Predictions Summary:\n",
      "Total points: 659\n",
      "LSTM predicted anomalies: 296\n",
      "Ground truth anomalies: 228\n",
      "True positives: 79\n",
      "False positives: 217\n",
      "False Negatives: 149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 21999/21999 [01:39<00:00, 220.28it/s, loss=0.000423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 0.001429\n",
      "LSTM Predictions Summary:\n",
      "Total points: 659\n",
      "LSTM predicted anomalies: 614\n",
      "Ground truth anomalies: 228\n",
      "True positives: 228\n",
      "False positives: 386\n",
      "False Negatives: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 21999/21999 [01:31<00:00, 239.54it/s, loss=0.000282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 0.000725\n",
      "LSTM Predictions Summary:\n",
      "Total points: 659\n",
      "LSTM predicted anomalies: 658\n",
      "Ground truth anomalies: 228\n",
      "True positives: 228\n",
      "False positives: 430\n",
      "False Negatives: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 21999/21999 [02:03<00:00, 178.42it/s, loss=0.000226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 0.000493\n",
      "LSTM Predictions Summary:\n",
      "Total points: 659\n",
      "LSTM predicted anomalies: 658\n",
      "Ground truth anomalies: 228\n",
      "True positives: 228\n",
      "False positives: 430\n",
      "False Negatives: 0\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "train(model_BREM, train_loader_B, loss_function_BREM, optimizer_BREM, X_anomaly_tensor_B, anomaly_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 0.000699\n"
     ]
    }
   ],
   "source": [
    "trashhold_BREM = evaluate_model(model_BREM, train_loader_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 28364/28364 [01:42<00:00, 276.14it/s, loss=0.000351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 0.000878\n",
      "LSTM Predictions Summary:\n",
      "Total points: 1646\n",
      "LSTM predicted anomalies: 0\n",
      "Ground truth anomalies: 22\n",
      "True positives: 0\n",
      "False positives: 0\n",
      "False Negatives: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 28364/28364 [02:30<00:00, 188.22it/s, loss=0.000087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 0.000152\n",
      "LSTM Predictions Summary:\n",
      "Total points: 1646\n",
      "LSTM predicted anomalies: 25\n",
      "Ground truth anomalies: 22\n",
      "True positives: 19\n",
      "False positives: 6\n",
      "False Negatives: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 28364/28364 [02:38<00:00, 178.87it/s, loss=0.000063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 0.000106\n",
      "LSTM Predictions Summary:\n",
      "Total points: 1646\n",
      "LSTM predicted anomalies: 27\n",
      "Ground truth anomalies: 22\n",
      "True positives: 21\n",
      "False positives: 6\n",
      "False Negatives: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 28364/28364 [02:26<00:00, 193.39it/s, loss=0.000052]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 0.000090\n",
      "LSTM Predictions Summary:\n",
      "Total points: 1646\n",
      "LSTM predicted anomalies: 28\n",
      "Ground truth anomalies: 22\n",
      "True positives: 21\n",
      "False positives: 7\n",
      "False Negatives: 1\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model_KIEL, train_loader_K, loss_function_KIEL, optimizer_KIEL, X_anomaly_tensor_K, anomaly_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 0.000075\n"
     ]
    }
   ],
   "source": [
    "trashhold_KIEL = evaluate_model(model_KIEL, train_loader_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Prepare anomaly data\n",
    "# anomaly_data = one_anomaly[FEATURE_COLUMNS].values\n",
    "# anomaly_data_scaled = scaler.transform(anomaly_data)\n",
    "#\n",
    "# if len(anomaly_data_scaled) >= SEQUENCE_LENGTH:\n",
    "#     X_anomaly, _ = create_sequences(anomaly_data_scaled, SEQUENCE_LENGTH)\n",
    "#\n",
    "#     # Process in batches to avoid OOM\n",
    "#     batch_size_test = 64\n",
    "#     anomaly_mask = []\n",
    "#\n",
    "#     for i in range(0, len(X_anomaly), batch_size_test):\n",
    "#         batch = X_anomaly[i:i+batch_size_test]\n",
    "#         batch_tensor = torch.FloatTensor(batch).to(device)\n",
    "#\n",
    "#         with torch.no_grad():\n",
    "#             batch_errors = model.get_reconstruction_error(batch_tensor)\n",
    "#             batch_mask = batch_errors > threshold\n",
    "#             anomaly_mask.extend(batch_mask)\n",
    "#\n",
    "#         # Clear cache after each batch\n",
    "#         torch.cuda.empty_cache()\n",
    "#\n",
    "#     anomaly_mask = np.array(anomaly_mask)\n",
    "#     anomaly_indices = np.where(anomaly_mask)[0]\n",
    "#\n",
    "#     # Initialize prediction column\n",
    "#     one_anomaly['lstm_prediction'] = 0\n",
    "#\n",
    "#     # Mark anomalous sequences\n",
    "#     for seq_idx in anomaly_indices:\n",
    "#         start_point = seq_idx\n",
    "#         end_point = min(seq_idx + SEQUENCE_LENGTH, len(one_anomaly))\n",
    "#         one_anomaly.iloc[start_point:end_point, one_anomaly.columns.get_loc('lstm_prediction')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Save to file\n",
    "output_dir = Path(\"models_per_route_lstm_ae\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def save_model(model, scaler, threshold, route_name):\n",
    "    # Move model to CPU before saving\n",
    "    model.cpu()\n",
    "    \n",
    "    # Save everything needed for inference\n",
    "    lstm_artifacts = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"scaler\": scaler,\n",
    "        \"threshold\": threshold,\n",
    "        \"model_config\": {\n",
    "            \"input_size\": len(FEATURE_COLUMNS),\n",
    "            \"hidden_size\": HIDDEN_SIZE, \n",
    "            \"num_layers\": NUM_LAYERS,\n",
    "            \"sequence_length\": SEQUENCE_LENGTH,\n",
    "            \"threshold_percentile\": AUTOENCODER_THRESHOLD_PERCENTILE\n",
    "        },\n",
    "        \"features\": FEATURE_COLUMNS,\n",
    "        \"model_type\": \"lstm\"\n",
    "    }\n",
    "    \n",
    "\n",
    "    # Save for the specific route\n",
    "    model_filename = output_dir / f\"{route_name}_lstm_model.pkl\"\n",
    "    joblib.dump(lstm_artifacts, model_filename)\n",
    "\n",
    "    print(f\"LSTM model saved to {model_filename}\")\n",
    "    return model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model saved to models_per_route_lstm_ae/BREMERHAVEN_lstm_model.pkl\n",
      "LSTM model saved to models_per_route_lstm_ae/KIEL_lstm_model.pkl\n",
      "Dispatcher saved to models_per_route_lstm_ae/dispatcher.pkl\n"
     ]
    }
   ],
   "source": [
    "models_filenames = []\n",
    "dispatcher_lstm = {}\n",
    "\n",
    "# Save BREMERHAVEN model\n",
    "model_filename_BREM = save_model(model_BREM, scaler, trashhold_BREM, \"BREMERHAVEN\")\n",
    "models_filenames.append(model_filename_BREM)\n",
    "dispatcher_lstm[\"BREMERHAVEN\"] = str(model_filename_BREM)\n",
    "\n",
    "# Save KIEL model\n",
    "model_filename_KIEL = save_model(model_KIEL, scaler, trashhold_KIEL, \"KIEL\")\n",
    "models_filenames.append(model_filename_KIEL)\n",
    "dispatcher_lstm[\"KIEL\"] = str(model_filename_KIEL)\n",
    "\n",
    "# Save dispatcher\n",
    "dispatcher_file = output_dir / \"dispatcher.pkl\"\n",
    "joblib.dump(dispatcher_lstm, dispatcher_file)\n",
    "print(f\"Dispatcher saved to {dispatcher_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trip_anomaly_predictions(model, trip_data, feature_columns, sequence_length, scaler, threshold):\n",
    "    \"\"\"\n",
    "    Get anomaly predictions for a complete trip\n",
    "    \n",
    "    Args:\n",
    "        model: trained LSTM model\n",
    "        trip_data: DataFrame containing trip points\n",
    "        feature_columns: list of feature column names\n",
    "        sequence_length: length of each sequence\n",
    "        scaler: fitted MinMaxScaler\n",
    "        threshold: anomaly threshold\n",
    "    \n",
    "    Returns:\n",
    "        predictions: binary array (0=normal, 1=anomaly) for each point\n",
    "    \"\"\"\n",
    "    # Create sequences\n",
    "    sequences = create_sequences_for_trip(trip_data, feature_columns, sequence_length, scaler)\n",
    "    \n",
    "    if len(sequences) == 0:\n",
    "        return np.zeros(len(trip_data))\n",
    "    \n",
    "    # Get anomaly scores\n",
    "    scores = model.get_score_route(sequences, trip_data)\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    predictions = (scores > threshold).astype(int)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the anomaly trip\n",
    "predictions = get_trip_anomaly_predictions(\n",
    "    model=model_KIEL,\n",
    "    trip_data=anomaly_K,\n",
    "    feature_columns=FEATURE_COLUMNS,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    scaler=scaler,\n",
    "    threshold=trashhold_KIEL  # Use the threshold you calculated\n",
    ")\n",
    "\n",
    "# Update the lstm_prediction column\n",
    "anomaly_K['lstm_prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipywidgets import interact, widgets, HBox, VBox\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "\n",
    "\n",
    "def create_interactive_anomaly_visualization(df, trip_id=None, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create an interactive visualization for ship trajectory anomaly detection\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['longitude', 'latitude', 'y_true', 'lstm_prediction', 'time_stamp', etc.]\n",
    "        trip_id: Specific trip ID to visualize (if None, uses first available)\n",
    "        title_suffix: Additional text for the title\n",
    "    \"\"\"\n",
    "    \n",
    "    if trip_id is None:\n",
    "        trip_id = df['trip_id'].iloc[0] if 'trip_id' in df.columns else \"Unknown\"\n",
    "    \n",
    "    # Filter data for specific trip if trip_id column exists\n",
    "    if 'trip_id' in df.columns:\n",
    "        trip_data = df[df['trip_id'] == trip_id].copy()\n",
    "    else:\n",
    "        trip_data = df.copy()\n",
    "    \n",
    "    # Sort by timestamp if available\n",
    "    if 'time_stamp' in trip_data.columns:\n",
    "        trip_data = trip_data.sort_values('time_stamp').reset_index(drop=True)\n",
    "    \n",
    "    # Ensure we have the required columns\n",
    "    required_columns = ['longitude', 'latitude']\n",
    "    if not all(col in trip_data.columns for col in required_columns):\n",
    "        print(f\"Missing required columns. Available columns: {trip_data.columns.tolist()}\")\n",
    "        return None\n",
    "    \n",
    "    # Create y_true column if it doesn't exist (for demo purposes)\n",
    "    if 'y_true' not in trip_data.columns:\n",
    "        print(\"mock y_true\")\n",
    "        trip_data['y_true'] = np.random.choice([0, 1], size=len(trip_data), p=[0.85, 0.15])\n",
    "    \n",
    "    # Create lstm_prediction column if it doesn't exist (for demo purposes)\n",
    "    if 'lstm_prediction' not in trip_data.columns:\n",
    "        print(\"mock lstm_prediction\")\n",
    "        trip_data['lstm_prediction'] = np.random.choice([0, 1], size=len(trip_data), p=[0.8, 0.2])\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    trip_data['point_index'] = range(len(trip_data))\n",
    "    trip_data['color'] = trip_data['y_true'].map({0: 'blue', 1: 'red'})\n",
    "    trip_data['anomaly_status'] = trip_data['y_true'].map({0: 'Normal', 1: 'Anomaly'})\n",
    "    \n",
    "    # Add LSTM prediction status\n",
    "    trip_data['lstm_status'] = trip_data['lstm_prediction'].map({0: 'Normal', 1: 'Predicted Anomaly'})\n",
    "    \n",
    "    # Create hover text\n",
    "    hover_text = []\n",
    "    for idx, row in trip_data.iterrows():\n",
    "        text = f\"Point: {row['point_index']}<br>\"\n",
    "        text += f\"Lat: {row['latitude']:.4f}<br>\"\n",
    "        text += f\"Lon: {row['longitude']:.4f}<br>\"\n",
    "        text += f\"Ground Truth: {row['anomaly_status']}<br>\"\n",
    "        text += f\"LSTM Prediction: {row['lstm_status']}<br>\"\n",
    "        \n",
    "        if 'time_stamp' in row:\n",
    "            text += f\"Time: {row['time_stamp']}<br>\"\n",
    "        if 'speed_over_ground' in row:\n",
    "            text += f\"Speed: {row['speed_over_ground']:.2f}<br>\"\n",
    "        if 'course_over_ground' in row:\n",
    "            text += f\"Course: {row['course_over_ground']:.2f}<br>\"\n",
    "            \n",
    "        hover_text.append(text)\n",
    "    \n",
    "    trip_data['hover_text'] = hover_text\n",
    "    \n",
    "    # Create the main plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add normal points (blue)\n",
    "    normal_points = trip_data[trip_data['y_true'] == 0]\n",
    "    if len(normal_points) > 0:\n",
    "        fig.add_trace(go.Scattermapbox(\n",
    "            lat=normal_points['latitude'],\n",
    "            lon=normal_points['longitude'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color='#3498db',\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            text=normal_points['hover_text'],\n",
    "            hovertemplate='%{text}<extra></extra>',\n",
    "            name='Normal Points',\n",
    "            showlegend=True\n",
    "        ))\n",
    "    \n",
    "    # Add anomaly points (red)\n",
    "    anomaly_points = trip_data[trip_data['y_true'] == 1]\n",
    "    if len(anomaly_points) > 0:\n",
    "        fig.add_trace(go.Scattermapbox(\n",
    "            lat=anomaly_points['latitude'],\n",
    "            lon=anomaly_points['longitude'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color='#e74c3c',\n",
    "                opacity=0.9,\n",
    "                symbol='circle'\n",
    "            ),\n",
    "            text=anomaly_points['hover_text'],\n",
    "            hovertemplate='%{text}<extra></extra>',\n",
    "            name='Anomaly Points',\n",
    "            showlegend=True\n",
    "        ))\n",
    "    \n",
    "    # Add LSTM predictions if different from ground truth\n",
    "    lstm_only_anomalies = trip_data[\n",
    "        (trip_data['lstm_prediction'] == 1) & (trip_data['y_true'] == 0)\n",
    "    ]\n",
    "    if len(lstm_only_anomalies) > 0:\n",
    "        fig.add_trace(go.Scattermapbox(\n",
    "            lat=lstm_only_anomalies['latitude'],\n",
    "            lon=lstm_only_anomalies['longitude'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color='#FFC0CB',\n",
    "                opacity=0.8,\n",
    "                symbol='circle'\n",
    "            ),\n",
    "            text=lstm_only_anomalies['hover_text'],\n",
    "            hovertemplate='%{text}<extra></extra>',\n",
    "            name='LSTM Predicted Anomalies',\n",
    "            showlegend=True\n",
    "        ))\n",
    "    \n",
    "    # Mark start point (green)\n",
    "    start_point = trip_data.iloc[0]\n",
    "    fig.add_trace(go.Scattermapbox(\n",
    "        lat=[start_point['latitude']],\n",
    "        lon=[start_point['longitude']],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=15,\n",
    "            color='#27ae60',\n",
    "            symbol='circle',\n",
    "            opacity=1.0\n",
    "        ),\n",
    "        text=f\"START<br>{start_point['hover_text']}\",\n",
    "        hovertemplate='%{text}<extra></extra>',\n",
    "        name='Start Point',\n",
    "        showlegend=True\n",
    "    ))\n",
    "    \n",
    "    # Mark end point (yellow)\n",
    "    end_point = trip_data.iloc[-1]\n",
    "    fig.add_trace(go.Scattermapbox(\n",
    "        lat=[end_point['latitude']],\n",
    "        lon=[end_point['longitude']],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=15,\n",
    "            color='#f1c40f',\n",
    "            symbol='cros',\n",
    "            opacity=1.0\n",
    "        ),\n",
    "        text=f\"END PORT<br>{end_point['hover_text']}\",\n",
    "        hovertemplate='%{text}<extra></extra>',\n",
    "        name='End Port',\n",
    "        showlegend=True\n",
    "    ))\n",
    "    \n",
    "    # Calculate center and zoom\n",
    "    center_lat = trip_data['latitude'].mean()\n",
    "    center_lon = trip_data['longitude'].mean()\n",
    "    \n",
    "    lat_range = trip_data['latitude'].max() - trip_data['latitude'].min()\n",
    "    lon_range = trip_data['longitude'].max() - trip_data['longitude'].min()\n",
    "    zoom = max(1, min(15, 12 - np.log(max(lat_range, lon_range, 0.001))))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Ship Trajectory Anomaly Detection - Trip {trip_id} {title_suffix}',\n",
    "            x=0.5,\n",
    "            font=dict(size=16, color='#2c3e50')\n",
    "        ),\n",
    "        mapbox=dict(\n",
    "            style=\"open-street-map\",\n",
    "            center=dict(lat=center_lat, lon=center_lon),\n",
    "            zoom=zoom\n",
    "        ),\n",
    "        margin=dict(r=0, t=50, l=0, b=0),\n",
    "        height=600,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01,\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "            bordercolor=\"rgba(0, 0, 0, 0.2)\",\n",
    "            borderwidth=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1947489/1032801482.py:84: DeprecationWarning:\n",
      "\n",
      "*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n",
      "/tmp/ipykernel_1947489/1032801482.py:102: DeprecationWarning:\n",
      "\n",
      "*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n",
      "/tmp/ipykernel_1947489/1032801482.py:123: DeprecationWarning:\n",
      "\n",
      "*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n",
      "/tmp/ipykernel_1947489/1032801482.py:141: DeprecationWarning:\n",
      "\n",
      "*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n",
      "/tmp/ipykernel_1947489/1032801482.py:159: DeprecationWarning:\n",
      "\n",
      "*scattermapbox* is deprecated! Use *scattermap* instead. Learn more at: https://plotly.com/python/mapbox-to-maplibre/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_id =  anomaly_K['trip_id'].unique()[0]\n",
    "fig = create_interactive_anomaly_visualization(anomaly_K, trip_id)\n",
    "\n",
    "from pathlib import Path\n",
    "output_dir = Path(\"data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "fig.write_html(f\"data/test_trip_{trip_id}_anomalies.html\", include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
