{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Per-Route Training Pipeline with Dispatcher\n",
    "\n",
    "This notebook trains separate LSTM models for each route and creates a dispatcher system\n",
    "compatible with the visualization pipeline. Each route gets its own model optimized for\n",
    "that specific route's characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 21:11:46.555355: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-23 21:11:46.620691: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "# Import our custom classes\n",
    "from lstm_autoencoder import LSTMAutoencoder\n",
    "from lstm_classifier import LSTMClassifier\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "tqdm.pandas()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(\"Configuring GPU...\")\n",
    "#\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(f\"GPUs available: {len(gpus)}\")\n",
    "#\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         # Configure GPU\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#\n",
    "#         # Key addition: Force all operations to GPU by default\n",
    "#         tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "#\n",
    "#         # Set GPU as default device\n",
    "#         tf.config.set_soft_device_placement(True)\n",
    "#\n",
    "#         print(f\"GPU configured: {gpus[0]}\")\n",
    "#\n",
    "#         # Test that new tensors are created on GPU\n",
    "#         with tf.device('/GPU:0'):\n",
    "#             test_tensor = tf.constant([1.0, 2.0, 3.0])\n",
    "#             print(f\"Test tensor device: {test_tensor.device}\")\n",
    "#\n",
    "#     except RuntimeError as e:\n",
    "#         print(f\"GPU configuration error: {e}\")\n",
    "#\n",
    "# from lstm_autoencoder import LSTMAutoencoder\n",
    "# from lstm_classifier import LSTMClassifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters\n",
    "\n",
    "**Modify these parameters to experiment with different configurations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:11:59.123134746Z",
     "start_time": "2025-06-23T19:11:59.094424444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded for experiment: lstm_per_route_v1\n",
      "Sequence length: 50, LSTM units: 64\n",
      "Training epochs: 50, Batch size: 32\n",
      "Output directories: models_per_route_lstm_ae, models_per_route_lstm_clf\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION PARAMETERS - MODIFY THESE TO EXPERIMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Data parameters\n",
    "DATA_PATH = \"data/LSTM_preprocessed.parquet\"\n",
    "EXPERIMENT_NAME = \"lstm_per_route_v1\"\n",
    "\n",
    "# Feature columns to use (must match data preprocessing)\n",
    "FEATURE_COLUMNS = [\n",
    "    \"speed_over_ground\", \"dv\", \"dcourse\", \"ddraft\",\n",
    "    \"zone\", \"x_km\", \"y_km\", \"dist_to_ref\", \"route_dummy\"\n",
    "]\n",
    "\n",
    "# Sequence parameters\n",
    "SEQUENCE_LENGTH = 50        # Number of time steps in each sequence TODO\n",
    "OVERLAP_RATIO = 0.5         # Overlap between consecutive sequences (0.5 = 50%) TODO\n",
    "\n",
    "# Model architecture parameters\n",
    "LSTM_UNITS = 64             # Number of LSTM units\n",
    "DENSE_UNITS = 32            # Dense layer units\n",
    "DROPOUT_RATE = 0.1          # Dropout rate for regularization\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 50                 # Maximum epochs (early stopping will likely kick in)\n",
    "BATCH_SIZE = 32             # Training batch size\n",
    "PATIENCE = 5               # Early stopping patience\n",
    "VALIDATION_SIZE = 0.2       # Proportion of data for validation\n",
    "\n",
    "# Model selection\n",
    "TRAIN_AUTOENCODER = True    # Train LSTM Autoencoder per route\n",
    "TRAIN_CLASSIFIER = True     # Train LSTM Classifier per route\n",
    "\n",
    "# Evaluation parameters\n",
    "AUTOENCODER_THRESHOLD_PERCENTILE = 95  # Threshold for autoencoder (95th percentile)\n",
    "CLASSIFIER_THRESHOLD = 0.5             # Threshold for classifier\n",
    "\n",
    "# Output directories (following OC-SVM pattern)\n",
    "OUTPUT_DIR_AE = \"models_per_route_lstm_ae\"    # Autoencoder models\n",
    "OUTPUT_DIR_CLF = \"models_per_route_lstm_clf\"  # Classifier models\n",
    "\n",
    "print(f\"Configuration loaded for experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Sequence length: {SEQUENCE_LENGTH}, LSTM units: {LSTM_UNITS}\")\n",
    "print(f\"Training epochs: {EPOCHS}, Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Output directories: {OUTPUT_DIR_AE}, {OUTPUT_DIR_CLF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Route Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:12:00.696242035Z",
     "start_time": "2025-06-23T19:12:00.024346284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/LSTM_preprocessed.parquet...\n",
      "Loaded 911,989 rows with 1122 unique trips\n",
      "\n",
      "Route Analysis:\n",
      "             trips  anomalies  total_points  anomaly_rate\n",
      "route_id                                                 \n",
      "BREMERHAVEN    702      621.0         25791          2.41\n",
      "KIEL           420     3505.0         80407          4.36\n",
      "\n",
      "Will train models for 2 routes: ['KIEL', 'BREMERHAVEN']\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "print(f\"Loading data from {DATA_PATH}...\")\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Loaded {len(df):,} rows with {df['trip_id'].nunique()} unique trips\")\n",
    "\n",
    "# Check for required columns\n",
    "missing_cols = [col for col in FEATURE_COLUMNS if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns: {missing_cols}\")\n",
    "    FEATURE_COLUMNS = [col for col in FEATURE_COLUMNS if col in df.columns]\n",
    "    print(f\"Using available columns: {FEATURE_COLUMNS}\")\n",
    "\n",
    "# Analyze routes\n",
    "route_summary = df.groupby('route_id').agg({\n",
    "    'trip_id': 'nunique',\n",
    "    'y_true': ['sum', 'count']\n",
    "}).round(2)\n",
    "\n",
    "route_summary.columns = ['trips', 'anomalies', 'total_points']\n",
    "route_summary['anomaly_rate'] = (route_summary['anomalies'] / route_summary['total_points'] * 100).round(2)\n",
    "\n",
    "print(\"\\nRoute Analysis:\")\n",
    "print(route_summary)\n",
    "\n",
    "routes = df.route_id.unique()\n",
    "print(f\"\\nWill train models for {len(routes)} routes: {list(routes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sequence Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:12:01.548388359Z",
     "start_time": "2025-06-23T19:12:01.462404791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence creation functions defined.\n"
     ]
    }
   ],
   "source": [
    "def create_sequences_for_route(df_route: pd.DataFrame, sequence_length: int, \n",
    "                              overlap_ratio: float, feature_columns: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Create sequences from route data, ensuring proper temporal ordering.\n",
    "    \n",
    "    Args:\n",
    "        df_route: DataFrame containing data for a single route\n",
    "        sequence_length: Length of each sequence\n",
    "        overlap_ratio: Overlap between consecutive sequences\n",
    "        feature_columns: List of feature column names\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (sequences, labels, trip_ids)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    trip_ids = []\n",
    "    step_size = max(1, int(sequence_length * (1 - overlap_ratio)))\n",
    "    \n",
    "    for trip_id, trip_data in df_route.groupby('trip_id'):\n",
    "        trip_data = trip_data.sort_values('time_stamp').reset_index(drop=True)\n",
    "            \n",
    "        features = trip_data[feature_columns].fillna(0).values\n",
    "        trip_labels = trip_data['y_true'].values\n",
    "        \n",
    "        # Create overlapping sequences\n",
    "        for start_idx in range(0, len(features) - sequence_length + 1, step_size):\n",
    "            end_idx = start_idx + sequence_length\n",
    "            seq_features = features[start_idx:end_idx]\n",
    "            seq_labels = trip_labels[start_idx:end_idx]\n",
    "            \n",
    "            # Label sequence as anomalous if any point is anomalous\n",
    "            sequence_label = int(np.any(seq_labels == 1))\n",
    "            \n",
    "            sequences.append(seq_features)\n",
    "            labels.append(sequence_label)\n",
    "            trip_ids.append(trip_id)\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), np.array(trip_ids)\n",
    "\n",
    "def balance_sequences(X: np.ndarray, y: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Balance training data by undersampling normal sequences.\n",
    "    \n",
    "    Args:\n",
    "        X: Sequences array\n",
    "        y: Labels array\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (balanced_X, balanced_y)\n",
    "    \"\"\"\n",
    "    normal_indices = np.where(y == 0)[0]\n",
    "    anomaly_indices = np.where(y == 1)[0]\n",
    "    \n",
    "    if len(anomaly_indices) == 0:\n",
    "        return X, y  # No anomalies to balance with\n",
    "    \n",
    "    if len(normal_indices) > len(anomaly_indices):\n",
    "        # Undersample normal to match anomalous count\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        selected_normal = np.random.choice(normal_indices, len(anomaly_indices), replace=False)\n",
    "        balanced_indices = np.concatenate([selected_normal, anomaly_indices])\n",
    "        np.random.shuffle(balanced_indices)\n",
    "        \n",
    "        return X[balanced_indices], y[balanced_indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "print(\"Sequence creation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTM Autoencoder Per-Route Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:57:12.677385298Z",
     "start_time": "2025-06-23T19:12:02.539842178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING LSTM AUTOENCODER PER ROUTE\n",
      "================================================================================\n",
      "\n",
      "=== Training Autoencoder for Route: KIEL ===\n",
      "  * Created 20,752 sequences\n",
      "  * Normal: 20,350, Anomalous: 402\n",
      "  * Training on 16,133 normal sequences\n",
      "  * Validating on 4,217 normal sequences\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 21:12:16.588852: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT32 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m66s\u001B[0m 107ms/step - accuracy: 0.7653 - auc: 0.8116 - loss: 0.0098 - precision: 0.9921 - recall: 0.2473 - val_accuracy: 0.9591 - val_auc: 0.9001 - val_loss: 7.8305e-04 - val_precision: 1.0000 - val_recall: 0.2443\n",
      "Epoch 2/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 101ms/step - accuracy: 0.9260 - auc: 0.8637 - loss: 0.0012 - precision: 1.0000 - recall: 0.2571 - val_accuracy: 0.9422 - val_auc: 0.8748 - val_loss: 5.0068e-04 - val_precision: 1.0000 - val_recall: 0.2443\n",
      "Epoch 3/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 102ms/step - accuracy: 0.9348 - auc: 0.8729 - loss: 7.5510e-04 - precision: 1.0000 - recall: 0.2589 - val_accuracy: 0.9569 - val_auc: 0.8640 - val_loss: 3.4020e-04 - val_precision: 1.0000 - val_recall: 0.2387\n",
      "Epoch 4/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m57s\u001B[0m 114ms/step - accuracy: 0.9422 - auc: 0.8788 - loss: 6.3419e-04 - precision: 1.0000 - recall: 0.2586 - val_accuracy: 0.9634 - val_auc: 0.8737 - val_loss: 3.5656e-04 - val_precision: 1.0000 - val_recall: 0.2414\n",
      "Epoch 5/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 98ms/step - accuracy: 0.9458 - auc: 0.8804 - loss: 5.8337e-04 - precision: 1.0000 - recall: 0.2586 - val_accuracy: 0.9716 - val_auc: 0.8777 - val_loss: 3.3231e-04 - val_precision: 1.0000 - val_recall: 0.2293\n",
      "Epoch 6/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m57s\u001B[0m 112ms/step - accuracy: 0.9440 - auc: 0.8817 - loss: 5.5139e-04 - precision: 1.0000 - recall: 0.2587 - val_accuracy: 0.9699 - val_auc: 0.8605 - val_loss: 3.1803e-04 - val_precision: 1.0000 - val_recall: 0.2384\n",
      "Epoch 7/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m53s\u001B[0m 104ms/step - accuracy: 0.9481 - auc: 0.8817 - loss: 5.2783e-04 - precision: 1.0000 - recall: 0.2593 - val_accuracy: 0.9688 - val_auc: 0.8822 - val_loss: 2.8149e-04 - val_precision: 1.0000 - val_recall: 0.2323\n",
      "Epoch 8/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 92ms/step - accuracy: 0.9463 - auc: 0.8835 - loss: 5.0966e-04 - precision: 1.0000 - recall: 0.2585 - val_accuracy: 0.9666 - val_auc: 0.8895 - val_loss: 2.7516e-04 - val_precision: 1.0000 - val_recall: 0.2416\n",
      "Epoch 9/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 95ms/step - accuracy: 0.9480 - auc: 0.8831 - loss: 4.9480e-04 - precision: 1.0000 - recall: 0.2593 - val_accuracy: 0.9720 - val_auc: 0.8621 - val_loss: 2.9376e-04 - val_precision: 1.0000 - val_recall: 0.2388\n",
      "Epoch 10/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 93ms/step - accuracy: 0.9494 - auc: 0.8834 - loss: 4.7207e-04 - precision: 1.0000 - recall: 0.2587 - val_accuracy: 0.9699 - val_auc: 0.8816 - val_loss: 2.4470e-04 - val_precision: 1.0000 - val_recall: 0.2413\n",
      "Epoch 11/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 93ms/step - accuracy: 0.9504 - auc: 0.8822 - loss: 4.5350e-04 - precision: 1.0000 - recall: 0.2582 - val_accuracy: 0.9719 - val_auc: 0.8714 - val_loss: 2.9878e-04 - val_precision: 1.0000 - val_recall: 0.2357\n",
      "Epoch 12/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 96ms/step - accuracy: 0.9513 - auc: 0.8834 - loss: 4.6793e-04 - precision: 1.0000 - recall: 0.2584 - val_accuracy: 0.9727 - val_auc: 0.8784 - val_loss: 2.4368e-04 - val_precision: 1.0000 - val_recall: 0.2374\n",
      "Epoch 13/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 93ms/step - accuracy: 0.9504 - auc: 0.8824 - loss: 4.4516e-04 - precision: 1.0000 - recall: 0.2574 - val_accuracy: 0.9657 - val_auc: 0.8415 - val_loss: 2.9919e-04 - val_precision: 1.0000 - val_recall: 0.2557\n",
      "Epoch 14/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 87ms/step - accuracy: 0.9513 - auc: 0.8803 - loss: 4.3072e-04 - precision: 1.0000 - recall: 0.2583 - val_accuracy: 0.9610 - val_auc: 0.8535 - val_loss: 2.7548e-04 - val_precision: 1.0000 - val_recall: 0.2300\n",
      "Epoch 15/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m55s\u001B[0m 108ms/step - accuracy: 0.9532 - auc: 0.8790 - loss: 4.0434e-04 - precision: 1.0000 - recall: 0.2579 - val_accuracy: 0.9711 - val_auc: 0.8369 - val_loss: 2.4178e-04 - val_precision: 1.0000 - val_recall: 0.2478\n",
      "Epoch 16/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m68s\u001B[0m 135ms/step - accuracy: 0.9537 - auc: 0.8792 - loss: 3.8922e-04 - precision: 1.0000 - recall: 0.2578 - val_accuracy: 0.9706 - val_auc: 0.8518 - val_loss: 2.5344e-04 - val_precision: 1.0000 - val_recall: 0.2308\n",
      "Epoch 17/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 101ms/step - accuracy: 0.9539 - auc: 0.8815 - loss: 3.8993e-04 - precision: 1.0000 - recall: 0.2566 - val_accuracy: 0.9713 - val_auc: 0.8944 - val_loss: 2.1816e-04 - val_precision: 1.0000 - val_recall: 0.2328\n",
      "Epoch 18/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 97ms/step - accuracy: 0.9534 - auc: 0.8819 - loss: 3.8969e-04 - precision: 1.0000 - recall: 0.2578 - val_accuracy: 0.9703 - val_auc: 0.8740 - val_loss: 2.2382e-04 - val_precision: 1.0000 - val_recall: 0.2383\n",
      "Epoch 19/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 97ms/step - accuracy: 0.9553 - auc: 0.8790 - loss: 3.6857e-04 - precision: 1.0000 - recall: 0.2576 - val_accuracy: 0.9668 - val_auc: 0.8561 - val_loss: 2.1809e-04 - val_precision: 1.0000 - val_recall: 0.2350\n",
      "Epoch 20/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 98ms/step - accuracy: 0.9549 - auc: 0.8800 - loss: 3.6227e-04 - precision: 1.0000 - recall: 0.2576 - val_accuracy: 0.9654 - val_auc: 0.8923 - val_loss: 2.3913e-04 - val_precision: 1.0000 - val_recall: 0.2396\n",
      "Epoch 21/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 91ms/step - accuracy: 0.9520 - auc: 0.8822 - loss: 3.6542e-04 - precision: 1.0000 - recall: 0.2576 - val_accuracy: 0.9737 - val_auc: 0.9048 - val_loss: 2.1846e-04 - val_precision: 1.0000 - val_recall: 0.2406\n",
      "Epoch 22/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 94ms/step - accuracy: 0.9561 - auc: 0.8786 - loss: 3.5337e-04 - precision: 1.0000 - recall: 0.2579 - val_accuracy: 0.9714 - val_auc: 0.8824 - val_loss: 1.9297e-04 - val_precision: 1.0000 - val_recall: 0.2411\n",
      "Epoch 23/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 92ms/step - accuracy: 0.9576 - auc: 0.8810 - loss: 3.4001e-04 - precision: 1.0000 - recall: 0.2578 - val_accuracy: 0.9697 - val_auc: 0.9087 - val_loss: 2.0972e-04 - val_precision: 1.0000 - val_recall: 0.2359\n",
      "Epoch 24/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 96ms/step - accuracy: 0.9557 - auc: 0.8798 - loss: 3.5983e-04 - precision: 1.0000 - recall: 0.2578 - val_accuracy: 0.9720 - val_auc: 0.8781 - val_loss: 2.0439e-04 - val_precision: 1.0000 - val_recall: 0.2414\n",
      "Epoch 25/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 88ms/step - accuracy: 0.9575 - auc: 0.8817 - loss: 3.3309e-04 - precision: 1.0000 - recall: 0.2576 - val_accuracy: 0.9750 - val_auc: 0.8800 - val_loss: 1.9052e-04 - val_precision: 1.0000 - val_recall: 0.2371\n",
      "Epoch 26/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 93ms/step - accuracy: 0.9559 - auc: 0.8800 - loss: 3.4312e-04 - precision: 1.0000 - recall: 0.2576 - val_accuracy: 0.9720 - val_auc: 0.8794 - val_loss: 2.1781e-04 - val_precision: 1.0000 - val_recall: 0.2396\n",
      "Epoch 27/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 86ms/step - accuracy: 0.9563 - auc: 0.8791 - loss: 3.3577e-04 - precision: 1.0000 - recall: 0.2575 - val_accuracy: 0.9674 - val_auc: 0.8899 - val_loss: 2.2163e-04 - val_precision: 1.0000 - val_recall: 0.2426\n",
      "Epoch 28/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 87ms/step - accuracy: 0.9554 - auc: 0.8803 - loss: 3.3327e-04 - precision: 1.0000 - recall: 0.2574 - val_accuracy: 0.9691 - val_auc: 0.8576 - val_loss: 2.1116e-04 - val_precision: 1.0000 - val_recall: 0.2435\n",
      "Epoch 29/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 93ms/step - accuracy: 0.9581 - auc: 0.8820 - loss: 3.3930e-04 - precision: 1.0000 - recall: 0.2577 - val_accuracy: 0.9743 - val_auc: 0.8870 - val_loss: 1.8947e-04 - val_precision: 1.0000 - val_recall: 0.2373\n",
      "Epoch 30/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 89ms/step - accuracy: 0.9586 - auc: 0.8798 - loss: 3.3347e-04 - precision: 1.0000 - recall: 0.2577 - val_accuracy: 0.9637 - val_auc: 0.8518 - val_loss: 1.9473e-04 - val_precision: 1.0000 - val_recall: 0.2345\n",
      "Epoch 31/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 87ms/step - accuracy: 0.9562 - auc: 0.8788 - loss: 3.2667e-04 - precision: 1.0000 - recall: 0.2579 - val_accuracy: 0.9724 - val_auc: 0.8415 - val_loss: 1.9870e-04 - val_precision: 1.0000 - val_recall: 0.2364\n",
      "Epoch 32/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m43s\u001B[0m 85ms/step - accuracy: 0.9571 - auc: 0.8803 - loss: 3.2295e-04 - precision: 1.0000 - recall: 0.2582 - val_accuracy: 0.9671 - val_auc: 0.8734 - val_loss: 2.4858e-04 - val_precision: 1.0000 - val_recall: 0.2417\n",
      "Epoch 33/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 91ms/step - accuracy: 0.9539 - auc: 0.8796 - loss: 3.2663e-04 - precision: 1.0000 - recall: 0.2577 - val_accuracy: 0.9710 - val_auc: 0.8606 - val_loss: 2.1445e-04 - val_precision: 1.0000 - val_recall: 0.2425\n",
      "Epoch 34/50\n",
      "\u001B[1m505/505\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 89ms/step - accuracy: 0.9562 - auc: 0.8793 - loss: 3.1641e-04 - precision: 1.0000 - recall: 0.2573 - val_accuracy: 0.9644 - val_auc: 0.8556 - val_loss: 2.2306e-04 - val_precision: 1.0000 - val_recall: 0.2368\n",
      "  * Threshold: 0.0010\n",
      "  * Precision: 0.089\n",
      "  * Recall: 0.306\n",
      "  * F1 Score: 0.138\n",
      "  * Route KIEL completed in 1684.8s\n",
      "\n",
      "=== Training Autoencoder for Route: BREMERHAVEN ===\n",
      "  * Created 14,065 sequences\n",
      "  * Normal: 14,003, Anomalous: 62\n",
      "  * Training on 11,248 normal sequences\n",
      "  * Validating on 2,755 normal sequences\n",
      "Epoch 1/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 98ms/step - accuracy: 0.7549 - auc: 0.8375 - loss: 0.0381 - precision: 0.9636 - recall: 0.3612 - val_accuracy: 0.9074 - val_auc: 0.9490 - val_loss: 0.0088 - val_precision: 0.9934 - val_recall: 0.4358\n",
      "Epoch 2/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 86ms/step - accuracy: 0.9145 - auc: 0.9142 - loss: 0.0087 - precision: 0.9944 - recall: 0.4408 - val_accuracy: 0.9356 - val_auc: 0.9555 - val_loss: 0.0063 - val_precision: 0.9983 - val_recall: 0.4332\n",
      "Epoch 3/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 98ms/step - accuracy: 0.9362 - auc: 0.9247 - loss: 0.0068 - precision: 0.9969 - recall: 0.4427 - val_accuracy: 0.9409 - val_auc: 0.9490 - val_loss: 0.0055 - val_precision: 0.9988 - val_recall: 0.4340\n",
      "Epoch 4/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 101ms/step - accuracy: 0.9412 - auc: 0.9320 - loss: 0.0058 - precision: 0.9975 - recall: 0.4466 - val_accuracy: 0.9359 - val_auc: 0.9373 - val_loss: 0.0051 - val_precision: 0.9996 - val_recall: 0.4424\n",
      "Epoch 5/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 107ms/step - accuracy: 0.9426 - auc: 0.9350 - loss: 0.0054 - precision: 0.9977 - recall: 0.4488 - val_accuracy: 0.9403 - val_auc: 0.9391 - val_loss: 0.0049 - val_precision: 0.9984 - val_recall: 0.4538\n",
      "Epoch 6/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 100ms/step - accuracy: 0.9442 - auc: 0.9377 - loss: 0.0051 - precision: 0.9979 - recall: 0.4455 - val_accuracy: 0.9351 - val_auc: 0.9447 - val_loss: 0.0049 - val_precision: 0.9996 - val_recall: 0.4274\n",
      "Epoch 7/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 132ms/step - accuracy: 0.9468 - auc: 0.9390 - loss: 0.0046 - precision: 0.9981 - recall: 0.4376 - val_accuracy: 0.9431 - val_auc: 0.9419 - val_loss: 0.0038 - val_precision: 0.9991 - val_recall: 0.4388\n",
      "Epoch 8/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 93ms/step - accuracy: 0.9486 - auc: 0.9415 - loss: 0.0043 - precision: 0.9980 - recall: 0.4317 - val_accuracy: 0.9411 - val_auc: 0.9418 - val_loss: 0.0036 - val_precision: 0.9994 - val_recall: 0.4228\n",
      "Epoch 9/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 95ms/step - accuracy: 0.9502 - auc: 0.9422 - loss: 0.0041 - precision: 0.9981 - recall: 0.4289 - val_accuracy: 0.9480 - val_auc: 0.9503 - val_loss: 0.0033 - val_precision: 0.9995 - val_recall: 0.4326\n",
      "Epoch 10/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 100ms/step - accuracy: 0.9530 - auc: 0.9442 - loss: 0.0039 - precision: 0.9982 - recall: 0.4275 - val_accuracy: 0.9548 - val_auc: 0.9486 - val_loss: 0.0032 - val_precision: 0.9993 - val_recall: 0.4256\n",
      "Epoch 11/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 99ms/step - accuracy: 0.9540 - auc: 0.9456 - loss: 0.0037 - precision: 0.9983 - recall: 0.4273 - val_accuracy: 0.9544 - val_auc: 0.9237 - val_loss: 0.0031 - val_precision: 0.9990 - val_recall: 0.4197\n",
      "Epoch 12/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 94ms/step - accuracy: 0.9569 - auc: 0.9450 - loss: 0.0036 - precision: 0.9983 - recall: 0.4266 - val_accuracy: 0.9537 - val_auc: 0.9347 - val_loss: 0.0029 - val_precision: 0.9995 - val_recall: 0.4211\n",
      "Epoch 13/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 93ms/step - accuracy: 0.9573 - auc: 0.9456 - loss: 0.0035 - precision: 0.9984 - recall: 0.4269 - val_accuracy: 0.9559 - val_auc: 0.9281 - val_loss: 0.0027 - val_precision: 0.9995 - val_recall: 0.4112\n",
      "Epoch 14/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 104ms/step - accuracy: 0.9573 - auc: 0.9457 - loss: 0.0034 - precision: 0.9983 - recall: 0.4261 - val_accuracy: 0.9522 - val_auc: 0.9427 - val_loss: 0.0028 - val_precision: 0.9995 - val_recall: 0.4266\n",
      "Epoch 15/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 101ms/step - accuracy: 0.9586 - auc: 0.9465 - loss: 0.0033 - precision: 0.9984 - recall: 0.4266 - val_accuracy: 0.9562 - val_auc: 0.9299 - val_loss: 0.0030 - val_precision: 0.9995 - val_recall: 0.4133\n",
      "Epoch 16/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 100ms/step - accuracy: 0.9598 - auc: 0.9457 - loss: 0.0032 - precision: 0.9985 - recall: 0.4266 - val_accuracy: 0.9628 - val_auc: 0.9292 - val_loss: 0.0026 - val_precision: 0.9996 - val_recall: 0.4187\n",
      "Epoch 17/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 102ms/step - accuracy: 0.9618 - auc: 0.9462 - loss: 0.0031 - precision: 0.9985 - recall: 0.4265 - val_accuracy: 0.9581 - val_auc: 0.9443 - val_loss: 0.0026 - val_precision: 0.9996 - val_recall: 0.4174\n",
      "Epoch 18/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 104ms/step - accuracy: 0.9600 - auc: 0.9474 - loss: 0.0031 - precision: 0.9985 - recall: 0.4265 - val_accuracy: 0.9620 - val_auc: 0.9361 - val_loss: 0.0027 - val_precision: 0.9995 - val_recall: 0.4194\n",
      "Epoch 19/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 100ms/step - accuracy: 0.9615 - auc: 0.9471 - loss: 0.0030 - precision: 0.9985 - recall: 0.4251 - val_accuracy: 0.9675 - val_auc: 0.9389 - val_loss: 0.0024 - val_precision: 0.9998 - val_recall: 0.4223\n",
      "Epoch 20/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 100ms/step - accuracy: 0.9643 - auc: 0.9481 - loss: 0.0030 - precision: 0.9986 - recall: 0.4277 - val_accuracy: 0.9714 - val_auc: 0.9383 - val_loss: 0.0023 - val_precision: 0.9993 - val_recall: 0.4151\n",
      "Epoch 21/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 94ms/step - accuracy: 0.9635 - auc: 0.9485 - loss: 0.0029 - precision: 0.9985 - recall: 0.4263 - val_accuracy: 0.9680 - val_auc: 0.9657 - val_loss: 0.0024 - val_precision: 0.9994 - val_recall: 0.4165\n",
      "Epoch 22/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 93ms/step - accuracy: 0.9646 - auc: 0.9496 - loss: 0.0028 - precision: 0.9987 - recall: 0.4259 - val_accuracy: 0.9677 - val_auc: 0.9307 - val_loss: 0.0024 - val_precision: 0.9997 - val_recall: 0.4192\n",
      "Epoch 23/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 94ms/step - accuracy: 0.9648 - auc: 0.9493 - loss: 0.0029 - precision: 0.9986 - recall: 0.4256 - val_accuracy: 0.9706 - val_auc: 0.9321 - val_loss: 0.0023 - val_precision: 0.9996 - val_recall: 0.4250\n",
      "Epoch 24/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 106ms/step - accuracy: 0.9652 - auc: 0.9497 - loss: 0.0028 - precision: 0.9986 - recall: 0.4261 - val_accuracy: 0.9675 - val_auc: 0.9468 - val_loss: 0.0022 - val_precision: 0.9995 - val_recall: 0.4161\n",
      "Epoch 25/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 90ms/step - accuracy: 0.9661 - auc: 0.9500 - loss: 0.0028 - precision: 0.9987 - recall: 0.4268 - val_accuracy: 0.9696 - val_auc: 0.9356 - val_loss: 0.0024 - val_precision: 0.9992 - val_recall: 0.4190\n",
      "Epoch 26/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 96ms/step - accuracy: 0.9651 - auc: 0.9499 - loss: 0.0027 - precision: 0.9986 - recall: 0.4256 - val_accuracy: 0.9642 - val_auc: 0.9322 - val_loss: 0.0023 - val_precision: 0.9999 - val_recall: 0.4240\n",
      "Epoch 27/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 95ms/step - accuracy: 0.9676 - auc: 0.9495 - loss: 0.0027 - precision: 0.9987 - recall: 0.4262 - val_accuracy: 0.9661 - val_auc: 0.9328 - val_loss: 0.0023 - val_precision: 0.9994 - val_recall: 0.4261\n",
      "Epoch 28/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 92ms/step - accuracy: 0.9665 - auc: 0.9503 - loss: 0.0027 - precision: 0.9986 - recall: 0.4259 - val_accuracy: 0.9643 - val_auc: 0.9362 - val_loss: 0.0023 - val_precision: 0.9995 - val_recall: 0.4129\n",
      "Epoch 29/50\n",
      "\u001B[1m352/352\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 91ms/step - accuracy: 0.9646 - auc: 0.9494 - loss: 0.0028 - precision: 0.9986 - recall: 0.4252 - val_accuracy: 0.9648 - val_auc: 0.9332 - val_loss: 0.0024 - val_precision: 0.9998 - val_recall: 0.4168\n",
      "  * Threshold: 0.0061\n",
      "  * Precision: 0.022\n",
      "  * Recall: 0.188\n",
      "  * F1 Score: 0.039\n",
      "  * Route BREMERHAVEN completed in 1025.4s\n",
      "\n",
      "Autoencoder training complete!\n",
      " Models saved to: models_per_route_lstm_ae\n",
      "  Dispatcher saved: models_per_route_lstm_ae/dispatcher.pkl\n",
      " Routes trained: ['KIEL', 'BREMERHAVEN']\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_AUTOENCODER:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TRAINING LSTM AUTOENCODER PER ROUTE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(OUTPUT_DIR_AE).mkdir(exist_ok=True)\n",
    "    dispatcher_ae: Dict[str, str] = {}\n",
    "    \n",
    "    for route in routes:\n",
    "        t0 = time.time()\n",
    "        print(f\"\\n=== Training Autoencoder for Route: {route} ===\")\n",
    "        \n",
    "        # Filter data for this route\n",
    "        df_route = df[df.route_id == route].copy()\n",
    "        \n",
    "        # Create sequences for this route\n",
    "        X, y, trip_ids = create_sequences_for_route(\n",
    "            df_route, SEQUENCE_LENGTH, OVERLAP_RATIO, FEATURE_COLUMNS\n",
    "        )\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(f\"  * No sequences created for route {route}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  * Created {len(X):,} sequences\")\n",
    "        print(f\"  * Normal: {np.sum(y == 0):,}, Anomalous: {np.sum(y == 1):,}\")\n",
    "        \n",
    "        # Split data for this route (preventing trip leakage)\n",
    "        unique_trips = np.unique(trip_ids)\n",
    "        train_trips, val_trips = train_test_split(\n",
    "            unique_trips, test_size=VALIDATION_SIZE, random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        train_mask = np.isin(trip_ids, train_trips)\n",
    "        val_mask = np.isin(trip_ids, val_trips)\n",
    "        \n",
    "        X_train, y_train = X[train_mask], y[train_mask]\n",
    "        X_val, y_val = X[val_mask], y[val_mask]\n",
    "        \n",
    "        # For autoencoder, use only normal data for training\n",
    "        X_train_normal = X_train[y_train == 0]\n",
    "        X_val_normal = X_val[y_val == 0]\n",
    "        \n",
    "        if len(X_train_normal) < 10:  # Need minimum data\n",
    "            print(f\"  * Insufficient normal training data for route {route}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  * Training on {len(X_train_normal):,} normal sequences\")\n",
    "        print(f\"  * Validating on {len(X_val_normal):,} normal sequences\")\n",
    "        \n",
    "        # Initialize autoencoder\n",
    "        autoencoder = LSTMAutoencoder(\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            n_features=len(FEATURE_COLUMNS),\n",
    "            lstm_units=LSTM_UNITS,\n",
    "            dense_units=DENSE_UNITS,\n",
    "            dropout_rate=DROPOUT_RATE\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        try:\n",
    "            history = autoencoder.fit(\n",
    "                X_train_normal,\n",
    "                validation_data=(X_val_normal, y_val[y_val == 0]),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                patience=PATIENCE\n",
    "            )\n",
    "            \n",
    "            # Evaluate and set threshold\n",
    "            reconstruction_errors, _ = autoencoder.predict(X_val)\n",
    "            threshold = np.percentile(reconstruction_errors, AUTOENCODER_THRESHOLD_PERCENTILE)\n",
    "            \n",
    "            # Evaluate model\n",
    "            metrics = autoencoder.evaluate(X_val, y_val, threshold=threshold)\n",
    "            \n",
    "            print(f\"  * Threshold: {threshold:.4f}\")\n",
    "            print(f\"  * Precision: {metrics['precision']:.3f}\")\n",
    "            print(f\"  * Recall: {metrics['recall']:.3f}\")\n",
    "            print(f\"  * F1 Score: {metrics['f1_score']:.3f}\")\n",
    "            \n",
    "            # Save model artifacts (compatible with visualizer)\n",
    "            model_filename = f\"lstm_autoencoder_{route}.pkl\"\n",
    "            model_path = Path(OUTPUT_DIR_AE) / model_filename\n",
    "            \n",
    "            model_artifacts = {\n",
    "                \"model\": autoencoder,           # The trained model\n",
    "                \"features\": FEATURE_COLUMNS,   # Feature names\n",
    "                \"threshold\": threshold,        # Anomaly threshold (tau)\n",
    "                \"sequence_length\": SEQUENCE_LENGTH,\n",
    "                \"model_type\": \"autoencoder\",\n",
    "                \"metrics\": metrics,\n",
    "                \"route\": route\n",
    "            }\n",
    "            \n",
    "            joblib.dump(model_artifacts, model_path)\n",
    "            dispatcher_ae[route] = str(model_path)\n",
    "            \n",
    "            print(f\"  * Route {route} completed in {time.time() - t0:.1f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  * Error training route {route}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save dispatcher\n",
    "    dispatcher_path_ae = Path(OUTPUT_DIR_AE) / \"dispatcher.pkl\"\n",
    "    joblib.dump(dispatcher_ae, dispatcher_path_ae)\n",
    "    \n",
    "    print(f\"\\nAutoencoder training complete!\")\n",
    "    print(f\" Models saved to: {OUTPUT_DIR_AE}\")\n",
    "    print(f\"  Dispatcher saved: {dispatcher_path_ae}\")\n",
    "    print(f\" Routes trained: {list(dispatcher_ae.keys())}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping autoencoder training (TRAIN_AUTOENCODER=False)\")\n",
    "    dispatcher_ae = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LSTM Classifier Per-Route Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRAIN_CLASSIFIER:\n",
    "#     print(\"=\" * 80)\n",
    "#     print(\"TRAINING LSTM CLASSIFIER PER ROUTE\")\n",
    "#     print(\"=\" * 80)\n",
    "#\n",
    "#     # Create output directory\n",
    "#     Path(OUTPUT_DIR_CLF).mkdir(exist_ok=True)\n",
    "#     dispatcher_clf: Dict[str, str] = {}\n",
    "#\n",
    "#     for route in routes:\n",
    "#         t0 = time.time()\n",
    "#         print(f\"\\n=== Training Classifier for Route: {route} ===\")\n",
    "#\n",
    "#         # Filter data for this route\n",
    "#         df_route = df[df.route_id == route].copy()\n",
    "#\n",
    "#         # Create sequences for this route\n",
    "#         X, y, trip_ids = create_sequences_for_route(\n",
    "#             df_route, SEQUENCE_LENGTH, OVERLAP_RATIO, FEATURE_COLUMNS\n",
    "#         )\n",
    "#\n",
    "#         if len(X) == 0:\n",
    "#             print(f\"  * No sequences created for route {route}, skipping.\")\n",
    "#             continue\n",
    "#\n",
    "#         print(f\"  * Created {len(X):,} sequences\")\n",
    "#         print(f\"  * Normal: {np.sum(y == 0):,}, Anomalous: {np.sum(y == 1):,}\")\n",
    "#\n",
    "#         # Check for class diversity\n",
    "#         if len(np.unique(y)) < 2:\n",
    "#             print(f\"  * Insufficient class diversity for route {route}, skipping.\")\n",
    "#             continue\n",
    "#\n",
    "#         # Split data for this route (preventing trip leakage)\n",
    "#         unique_trips = np.unique(trip_ids)\n",
    "#\n",
    "#         # Get trip-level labels for stratification\n",
    "#         trip_labels = []\n",
    "#         for trip in unique_trips:\n",
    "#             trip_mask = trip_ids == trip\n",
    "#             trip_has_anomaly = np.any(y[trip_mask] == 1)\n",
    "#             trip_labels.append(int(trip_has_anomaly))\n",
    "#\n",
    "#         trip_labels = np.array(trip_labels)\n",
    "#\n",
    "#         # Stratified split by trip\n",
    "#         if len(np.unique(trip_labels)) > 1:\n",
    "#             train_trips, val_trips = train_test_split(\n",
    "#                 unique_trips, test_size=VALIDATION_SIZE,\n",
    "#                 stratify=trip_labels, random_state=RANDOM_STATE\n",
    "#             )\n",
    "#         else:\n",
    "#             train_trips, val_trips = train_test_split(\n",
    "#                 unique_trips, test_size=VALIDATION_SIZE, random_state=RANDOM_STATE\n",
    "#             )\n",
    "#\n",
    "#         train_mask = np.isin(trip_ids, train_trips)\n",
    "#         val_mask = np.isin(trip_ids, val_trips)\n",
    "#\n",
    "#         X_train, y_train = X[train_mask], y[train_mask]\n",
    "#         X_val, y_val = X[val_mask], y[val_mask]\n",
    "#\n",
    "#         # Balance training data\n",
    "#         X_train, y_train = balance_sequences(X_train, y_train)\n",
    "#\n",
    "#         if len(X_train) < 10:  # Need minimum data\n",
    "#             print(f\"  * Insufficient training data for route {route}, skipping.\")\n",
    "#             continue\n",
    "#\n",
    "#         print(f\"  * Training on {len(X_train):,} sequences (balanced)\")\n",
    "#         print(f\"  * Validating on {len(X_val):,} sequences\")\n",
    "#         print(f\"  * Train anomaly rate: {np.mean(y_train == 1):.1%}\")\n",
    "#\n",
    "#         # Initialize classifier\n",
    "#         classifier = LSTMClassifier(\n",
    "#             sequence_length=SEQUENCE_LENGTH,\n",
    "#             n_features=len(FEATURE_COLUMNS),\n",
    "#             lstm_units=LSTM_UNITS,\n",
    "#             dense_units=DENSE_UNITS,\n",
    "#             dropout_rate=DROPOUT_RATE\n",
    "#         )\n",
    "#\n",
    "#         # Train the model\n",
    "#         try:\n",
    "#             history = classifier.fit(\n",
    "#                 X_train, y_train,\n",
    "#                 validation_data=(X_val, y_val),\n",
    "#                 epochs=EPOCHS,\n",
    "#                 batch_size=BATCH_SIZE,\n",
    "#                 patience=PATIENCE\n",
    "#             )\n",
    "#\n",
    "#             # Evaluate model\n",
    "#             metrics = classifier.evaluate(X_val, y_val, threshold=CLASSIFIER_THRESHOLD)\n",
    "#\n",
    "#             print(f\"  * Threshold: {CLASSIFIER_THRESHOLD:.3f}\")\n",
    "#             print(f\"  * Accuracy: {metrics['accuracy']:.3f}\")\n",
    "#             print(f\"  * Precision: {metrics['precision']:.3f}\")\n",
    "#             print(f\"  * Recall: {metrics['recall']:.3f}\")\n",
    "#             print(f\"  * F1 Score: {metrics['f1_score']:.3f}\")\n",
    "#\n",
    "#             # Save model artifacts (compatible with visualizer)\n",
    "#             model_filename = f\"lstm_classifier_{route}.pkl\"\n",
    "#             model_path = Path(OUTPUT_DIR_CLF) / model_filename\n",
    "#\n",
    "#             model_artifacts = {\n",
    "#                 \"model\": classifier,           # The trained model\n",
    "#                 \"features\": FEATURE_COLUMNS,   # Feature names\n",
    "#                 \"threshold\": CLASSIFIER_THRESHOLD,  # Classification threshold (tau)\n",
    "#                 \"sequence_length\": SEQUENCE_LENGTH,\n",
    "#                 \"model_type\": \"classifier\",\n",
    "#                 \"metrics\": metrics,\n",
    "#                 \"route\": route\n",
    "#             }\n",
    "#\n",
    "#             joblib.dump(model_artifacts, model_path)\n",
    "#             dispatcher_clf[route] = str(model_path)\n",
    "#\n",
    "#             print(f\"  * Route {route} completed in {time.time() - t0:.1f}s\")\n",
    "#\n",
    "#         except Exception as e:\n",
    "#             print(f\"  * Error training route {route}: {e}\")\n",
    "#             continue\n",
    "#\n",
    "#     # Save dispatcher\n",
    "#     dispatcher_path_clf = Path(OUTPUT_DIR_CLF) / \"dispatcher.pkl\"\n",
    "#     joblib.dump(dispatcher_clf, dispatcher_path_clf)\n",
    "#\n",
    "#     print(f\"\\n Classifier training complete!\")\n",
    "#     print(f\" Models saved to: {OUTPUT_DIR_CLF}\")\n",
    "#     print(f\"️  Dispatcher saved: {dispatcher_path_clf}\")\n",
    "#     print(f\" Routes trained: {list(dispatcher_clf.keys())}\")\n",
    "#\n",
    "# else:\n",
    "#     print(\"Skipping classifier training (TRAIN_CLASSIFIER=False)\")\n",
    "#     dispatcher_clf = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Summary and Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nExperiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Total routes: {len(routes)}\")\n",
    "\n",
    "if dispatcher_ae:\n",
    "    print(f\"\\nAutoencoder models trained: {len(dispatcher_ae)}\")\n",
    "    print(f\"Routes: {list(dispatcher_ae.keys())}\")\n",
    "    print(f\"Dispatcher: {OUTPUT_DIR_AE}/dispatcher.pkl\")\n",
    "\n",
    "# if dispatcher_clf:\n",
    "#     print(f\"\\nClassifier models trained: {len(dispatcher_clf)}\")\n",
    "#     print(f\"Routes: {list(dispatcher_clf.keys())}\")\n",
    "#     print(f\"Dispatcher: {OUTPUT_DIR_CLF}/dispatcher.pkl\")\n",
    "\n",
    "# Test loading a model (to verify compatibility)\n",
    "if dispatcher_ae:\n",
    "    test_route = list(dispatcher_ae.keys())[0]\n",
    "    print(f\"\\n=== Testing Model Loading for Route: {test_route} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Load model artifacts\n",
    "        artifacts = joblib.load(dispatcher_ae[test_route])\n",
    "        \n",
    "        print(f\"== Successfully loaded {artifacts['model_type']} for route {test_route}\")\n",
    "        print(f\"   Features: {len(artifacts['features'])}\")\n",
    "        print(f\"   Threshold: {artifacts['threshold']:.4f}\")\n",
    "        print(f\"   Sequence length: {artifacts['sequence_length']}\")\n",
    "        \n",
    "        # Test prediction (with dummy data)\n",
    "        dummy_X = np.random.random((1, SEQUENCE_LENGTH, len(FEATURE_COLUMNS)))\n",
    "        model = artifacts['model']\n",
    "        scores, preds = model.predict(dummy_X)\n",
    "        \n",
    "        print(f\"== Prediction test successful: score={scores[0]:.4f}, pred={preds[0]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing model: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
